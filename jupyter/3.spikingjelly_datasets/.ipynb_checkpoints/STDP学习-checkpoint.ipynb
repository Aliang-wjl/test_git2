{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a92a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4000]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from spikingjelly.activation_based import neuron, layer, learning\n",
    "from matplotlib import pyplot as plt\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def f_weight(x):\n",
    "    return torch.clamp(x, -1, 1.)\n",
    "\n",
    "tau_pre = 2.\n",
    "tau_post = 2.\n",
    "T = 128\n",
    "N = 1\n",
    "lr = 0.01\n",
    "net = nn.Sequential(\n",
    "    layer.Linear(1, 1, bias=False),\n",
    "    neuron.IFNode()\n",
    ")\n",
    "nn.init.constant_(net[0].weight.data, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fba5ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.)\n",
    "\n",
    "in_spike = (torch.rand([T, N, 1]) > 0.7).float()\n",
    "stdp_learner = learning.STDPLearner(step_mode='s', synapse=net[0], sn=net[1], tau_pre=tau_pre, tau_post=tau_post,\n",
    "                                    f_pre=f_weight, f_post=f_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88a90f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_spike = []\n",
    "trace_pre = []\n",
    "trace_post = []\n",
    "weight = []\n",
    "with torch.no_grad():\n",
    "    for t in range(T):\n",
    "        optimizer.zero_grad()\n",
    "        out_spike.append(net(in_spike[t]).squeeze())\n",
    "        stdp_learner.step(on_grad=True)  # 将STDP学习得到的参数更新量叠加到参数的梯度上\n",
    "        optimizer.step()\n",
    "        weight.append(net[0].weight.data.clone().squeeze())\n",
    "        trace_pre.append(stdp_learner.trace_pre.squeeze())\n",
    "        trace_post.append(stdp_learner.trace_post.squeeze())\n",
    "\n",
    "in_spike = in_spike.squeeze()\n",
    "out_spike = torch.stack(out_spike)\n",
    "trace_pre = torch.stack(trace_pre)\n",
    "trace_post = torch.stack(trace_post)\n",
    "weight = torch.stack(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c500afbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43dd9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083661e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df43f662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e41ac57",
   "metadata": {},
   "source": [
    "# 官方完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e369554e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from spikingjelly.activation_based import neuron, layer, learning\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def f_weight(x):\n",
    "    return torch.clamp(x, -1, 1.)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# plt.style.use(['science'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    def f_pre(x, w_min, alpha=0.):\n",
    "        return (x - w_min) ** alpha\n",
    "\n",
    "    def f_post(x, w_max, alpha=0.):\n",
    "        return (w_max - x) ** alpha\n",
    "\n",
    "    w_min, w_max = -1., 1.\n",
    "    tau_pre, tau_post = 2., 2.\n",
    "    N_in, N_out = 4, 3\n",
    "    T = 128\n",
    "    batch_size = 2\n",
    "    lr = 0.01\n",
    "    net = nn.Sequential(\n",
    "        layer.Linear(N_in, N_out, bias=False),\n",
    "        neuron.IFNode()\n",
    "    )\n",
    "    nn.init.constant_(net[0].weight.data, 0.4)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.)\n",
    "\n",
    "    in_spike = (torch.rand([T, batch_size, N_in]) > 0.7).float()\n",
    "    learner = learning.STDPLearner(step_mode='s', synapse=net[0], sn=net[1], \n",
    "                                   tau_pre=tau_pre, tau_post=tau_post,\n",
    "                                   f_pre=f_weight, f_post=f_weight)\n",
    "\n",
    "    out_spike = []\n",
    "    trace_pre = []\n",
    "    trace_post = []\n",
    "    weight = []\n",
    "    with torch.no_grad():\n",
    "        for t in range(T):\n",
    "            optimizer.zero_grad()\n",
    "            out_spike.append(net(in_spike[t]))\n",
    "            learner.step(on_grad=True)\n",
    "            optimizer.step()\n",
    "            net[0].weight.data.clamp_(w_min, w_max)\n",
    "            weight.append(net[0].weight.data.clone())\n",
    "            trace_pre.append(learner.trace_pre)\n",
    "            trace_post.append(learner.trace_post)\n",
    "\n",
    "    out_spike = torch.stack(out_spike)   # [T, batch_size, N_out]\n",
    "    trace_pre = torch.stack(trace_pre)   # [T, batch_size, N_in]\n",
    "    trace_post = torch.stack(trace_post) # [T, batch_size, N_out]\n",
    "    weight = torch.stack(weight)         # [T, N_out, N_in]\n",
    "\n",
    "    t = torch.arange(0, T).float()\n",
    "    \n",
    "    in_spike = in_spike[:, 0, 0]\n",
    "    out_spike = out_spike[:, 0, 0]\n",
    "    trace_pre = trace_pre[:, 0, 0]\n",
    "    trace_post = trace_post[:, 0, 0]\n",
    "    weight = weight[:, 0, 0]\n",
    "\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    plt.subplot(5, 1, 1)\n",
    "    plt.eventplot((in_spike * t)[in_spike == 1], lineoffsets=0, colors=cmap(0))\n",
    "    plt.xlim(-0.5, T + 0.5)\n",
    "    plt.ylabel('$s[i]$', rotation=0, labelpad=10)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.subplot(5, 1, 2)\n",
    "    plt.plot(t, trace_pre, c=cmap(1))\n",
    "    plt.xlim(-0.5, T + 0.5)\n",
    "    plt.ylabel('$tr_{pre}$', rotation=0)\n",
    "    plt.yticks([trace_pre.min().item(), trace_pre.max().item()])\n",
    "    plt.xticks([])\n",
    "\n",
    "    plt.subplot(5, 1, 3)\n",
    "    plt.eventplot((out_spike * t)[out_spike == 1], lineoffsets=0, colors=cmap(2))\n",
    "    plt.xlim(-0.5, T + 0.5)\n",
    "    plt.ylabel('$s[j]$', rotation=0, labelpad=10)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.subplot(5, 1, 4)\n",
    "    plt.plot(t, trace_post, c=cmap(3))\n",
    "    plt.ylabel('$tr_{post}$', rotation=0)\n",
    "    plt.yticks([trace_post.min().item(), trace_post.max().item()])\n",
    "    plt.xlim(-0.5, T + 0.5)\n",
    "    plt.xticks([])\n",
    "\n",
    "    plt.subplot(5, 1, 5)\n",
    "    plt.plot(t, weight, c=cmap(4))\n",
    "    plt.xlim(-0.5, T + 0.5)\n",
    "    plt.ylabel('$w[i][j]$', rotation=0)\n",
    "    plt.yticks([weight.min().item(), weight.max().item()])\n",
    "    plt.xlabel('time-step')\n",
    "    \n",
    "    plt.gcf().subplots_adjust(left=0.18)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.savefig('./docs/source/_static/tutorials/activation_based/stdp/stdp_trace.png')\n",
    "    plt.savefig('./docs/source/_static/tutorials/activation_based/stdp/stdp_trace.svg')\n",
    "    plt.savefig('./docs/source/_static/tutorials/activation_based/stdp/stdp_trace.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2400a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cf96056",
   "metadata": {},
   "source": [
    "# STDP（训练卷积层）与梯度下降（训练全连接层）结合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1fcaf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from spikingjelly.activation_based import learning, layer, neuron, functional\n",
    "\n",
    "T = 8\n",
    "N = 2\n",
    "C = 3\n",
    "H = 32\n",
    "W = 32\n",
    "lr = 0.1\n",
    "tau_pre = 2.\n",
    "tau_post = 100.\n",
    "step_mode = 'm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a248afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_weight(x):\n",
    "    return torch.clamp(x, -1, 1.)\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    layer.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    neuron.IFNode(),\n",
    "    layer.MaxPool2d(2, 2),\n",
    "    layer.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    neuron.IFNode(),\n",
    "    layer.MaxPool2d(2, 2),\n",
    "    layer.Flatten(),\n",
    "    layer.Linear(16 * 8 * 8, 64, bias=False),\n",
    "    neuron.IFNode(),\n",
    "    layer.Linear(64, 10, bias=False),\n",
    "    neuron.IFNode(),\n",
    ")\n",
    "\n",
    "functional.set_step_mode(net, step_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49b66ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_stdp = (layer.Conv2d, )\n",
    "\n",
    "stdp_learners = []\n",
    "\n",
    "for i in range(net.__len__()):\n",
    "    if isinstance(net[i], instances_stdp):\n",
    "        stdp_learners.append(\n",
    "            learning.STDPLearner(step_mode=step_mode, synapse=net[i], sn=net[i+1], tau_pre=tau_pre, tau_post=tau_post,\n",
    "                                f_pre=f_weight, f_post=f_weight)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88e85b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_stdp = []\n",
    "for m in net.modules():\n",
    "    if isinstance(m, instances_stdp):\n",
    "        for p in m.parameters():\n",
    "            params_stdp.append(p)\n",
    "\n",
    "params_stdp_set = set(params_stdp)\n",
    "params_gradient_descent = []\n",
    "for p in net.parameters():\n",
    "    if p not in params_stdp_set:\n",
    "        params_gradient_descent.append(p)\n",
    "\n",
    "optimizer_gd = Adam(params_gradient_descent, lr=lr)\n",
    "optimizer_stdp = SGD(params_stdp, lr=lr, momentum=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10fb2736",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq = (torch.rand([T, N, C, H, W]) > 0.5).float()\n",
    "target = torch.randint(low=0, high=10, size=[N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6440da4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab3d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_gd.zero_grad()\n",
    "optimizer_stdp.zero_grad()\n",
    "y = net(x_seq).mean(0)\n",
    "loss = F.cross_entropy(y, target)\n",
    "loss.backward()\n",
    "optimizer_stdp.zero_grad()\n",
    "\n",
    "for i in range(stdp_learners.__len__()):\n",
    "    stdp_learners[i].step(on_grad=True)\n",
    "\n",
    "optimizer_gd.step()\n",
    "optimizer_stdp.step()\n",
    "\n",
    "functional.reset_net(net)\n",
    "for i in range(stdp_learners.__len__()):\n",
    "    stdp_learners[i].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84784d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf5fb173",
   "metadata": {},
   "source": [
    "# 完整的示例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46b99ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from spikingjelly.activation_based import learning, layer, neuron, functional\n",
    "\n",
    "T = 8\n",
    "N = 2\n",
    "C = 3\n",
    "H = 32\n",
    "W = 32\n",
    "lr = 0.1\n",
    "tau_pre = 2.\n",
    "tau_post = 100.\n",
    "step_mode = 'm'\n",
    "\n",
    "def f_weight(x):\n",
    "    return torch.clamp(x, -1, 1.)\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    layer.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    neuron.IFNode(),\n",
    "    layer.MaxPool2d(2, 2),\n",
    "    layer.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    neuron.IFNode(),\n",
    "    layer.MaxPool2d(2, 2),\n",
    "    layer.Flatten(),\n",
    "    layer.Linear(16 * 8 * 8, 64, bias=False),\n",
    "    neuron.IFNode(),\n",
    "    layer.Linear(64, 10, bias=False),\n",
    "    neuron.IFNode(),\n",
    ")\n",
    "\n",
    "functional.set_step_mode(net, step_mode)\n",
    "\n",
    "instances_stdp = (layer.Conv2d, )\n",
    "\n",
    "stdp_learners = []\n",
    "\n",
    "for i in range(net.__len__()):\n",
    "    if isinstance(net[i], instances_stdp):\n",
    "        stdp_learners.append(\n",
    "            learning.STDPLearner(step_mode=step_mode, synapse=net[i], sn=net[i+1], tau_pre=tau_pre, tau_post=tau_post,\n",
    "                                f_pre=f_weight, f_post=f_weight)\n",
    "        )\n",
    "\n",
    "\n",
    "params_stdp = []\n",
    "for m in net.modules():\n",
    "    if isinstance(m, instances_stdp):\n",
    "        for p in m.parameters():\n",
    "            params_stdp.append(p)\n",
    "\n",
    "params_stdp_set = set(params_stdp)\n",
    "params_gradient_descent = []\n",
    "for p in net.parameters():\n",
    "    if p not in params_stdp_set:\n",
    "        params_gradient_descent.append(p)\n",
    "\n",
    "optimizer_gd = Adam(params_gradient_descent, lr=lr)\n",
    "optimizer_stdp = SGD(params_stdp, lr=lr, momentum=0.)\n",
    "\n",
    "\n",
    "\n",
    "x_seq = (torch.rand([T, N, C, H, W]) > 0.5).float()\n",
    "target = torch.randint(low=0, high=10, size=[N])\n",
    "\n",
    "optimizer_gd.zero_grad()\n",
    "optimizer_stdp.zero_grad()\n",
    "\n",
    "y = net(x_seq).mean(0)\n",
    "loss = F.cross_entropy(y, target)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "optimizer_stdp.zero_grad()\n",
    "\n",
    "for i in range(stdp_learners.__len__()):\n",
    "    stdp_learners[i].step(on_grad=True)\n",
    "\n",
    "optimizer_gd.step()\n",
    "optimizer_stdp.step()\n",
    "\n",
    "functional.reset_net(net)\n",
    "for i in range(stdp_learners.__len__()):\n",
    "    stdp_learners[i].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf8619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f5802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c68c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ab8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
