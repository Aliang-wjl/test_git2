{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bbf591",
   "metadata": {},
   "source": [
    "detach_reset 参数的作用是决定在重置神经元状态时是否切断计算图的连接。这对反向传播非常重要。\n",
    "当 detach_reset=True 时，每次神经元发放脉冲并进行重置时，会从计算图中断开。这意味着重置操作不会影响到之前的计算，这样的设计避免了梯度在每个脉冲发放时被传播回去，从而使得模型更稳定并收敛更快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58db2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "from torch.cuda import amp\n",
    "import smodels\n",
    "import argparse\n",
    "from spikingjelly.clock_driven import functional\n",
    "from spikingjelly.datasets import cifar10_dvs\n",
    "import math\n",
    "from tqdm.notebook import tqdm  # tqdm 进度条显示,这个更好看\n",
    "\n",
    "_seed_ = 2020\n",
    "import random\n",
    "random.seed(2020)\n",
    "\n",
    "torch.manual_seed(_seed_)  # use torch.manual_seed() to seed the RNG for all devices (both CPU and CUDA)\n",
    "torch.cuda.manual_seed_all(_seed_)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(_seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7adc0a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_train_test_set(train_ratio: float, origin_dataset: torch.utils.data.Dataset, num_classes: int, random_split: bool = False):\n",
    "    '''\n",
    "    :param train_ratio: split the ratio of the origin dataset as the train set\n",
    "    :type train_ratio: float\n",
    "    :param origin_dataset: the origin dataset\n",
    "    :type origin_dataset: torch.utils.data.Dataset\n",
    "    :param num_classes: total classes number, e.g., ``10`` for the MNIST dataset\n",
    "    :type num_classes: int\n",
    "    :param random_split: If ``False``, the front ratio of samples in each classes will\n",
    "            be included in train set, while the reset will be included in test set.\n",
    "            If ``True``, this function will split samples in each classes randomly. The randomness is controlled by\n",
    "            ``numpy.randon.seed``\n",
    "            使用这个函数就意味着均匀的划分了类别。 下面说的都是针对一个类别\n",
    "            这儿的意思是 如果为 False, 就按照划分比例顺序索引数据，如果为 True，就完全随机的索引数据，只确保比例正确。\n",
    "    :type random_split: int\n",
    "    :return: a tuple ``(train_set, test_set)``\n",
    "    :rtype: tuple\n",
    "    '''\n",
    "    label_idx = []\n",
    "    for i in range(num_classes):\n",
    "        label_idx.append([])\n",
    "    # 经过这个for循环，就能将每个类别归到一个数组里面，且获取到他们在数组中的位置。\n",
    "    for i, item in enumerate(origin_dataset):\n",
    "        y = item[1]\n",
    "        if isinstance(y, np.ndarray) or isinstance(y, torch.Tensor):\n",
    "            y = y.item()\n",
    "        label_idx[y].append(i)\n",
    "    # 如果为True，就随机打乱顺序\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    if random_split:\n",
    "        for i in range(num_classes):\n",
    "            np.random.shuffle(label_idx[i])\n",
    "    #\n",
    "    for i in range(num_classes):\n",
    "        pos = math.ceil(label_idx[i].__len__() * train_ratio)\n",
    "        train_idx.extend(label_idx[i][0: pos])\n",
    "        test_idx.extend(label_idx[i][pos: label_idx[i].__len__()])\n",
    "\n",
    "    return torch.utils.data.Subset(origin_dataset, train_idx), torch.utils.data.Subset(origin_dataset, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a30a4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(T=20, device='cuda:0', b=16, epochs=64, j=4, data_dir='/raid/wfang/datasets/CIFAR10DVS', out_dir='../save_models', resume=None, amp=False, opt='SGD', lr=0.1, momentum=0.9, lr_scheduler='CosALR', step_size=32, gamma=0.1, T_max=64, model=None, cnf=None, T_train=None, dts_cache='./dts_cache')\n",
      "Namespace(T=20, device='cuda:0', b=16, epochs=64, j=4, data_dir='/raid/wfang/datasets/CIFAR10DVS', out_dir='../save_models', resume=None, amp=False, opt='SGD', lr=0.1, momentum=0.9, lr_scheduler='CosALR', step_size=32, gamma=0.1, T_max=64, model='SEWResNet', cnf='ADD', T_train=None, dts_cache='./dts_cache')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Classify DVS128 Gesture')\n",
    "parser.add_argument('-T', default=20, type=int, help='simulating time-steps')\n",
    "parser.add_argument('-device', default='cuda:0', help='device')\n",
    "parser.add_argument('-b', default=16, type=int, help='batch size')\n",
    "parser.add_argument('-epochs', default=64, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-j', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('-data_dir', type=str, default='/raid/wfang/datasets/CIFAR10DVS')\n",
    "parser.add_argument('-out_dir', type=str,default = r\"../save_models\",help='root dir for saving logs and checkpoint')\n",
    "\n",
    "# parser.add_argument('-resume', default=r'E:\\mycode\\jupyter\\0.SNN\\test_git2\\sew_resnet\\save_models',\n",
    "#                     type=str, help='resume from the checkpoint path')\n",
    "\n",
    "parser.add_argument('-resume',type=str, help='resume from the checkpoint path')\n",
    "\n",
    "parser.add_argument('-amp', action='store_true', help='automatic mixed precision training')\n",
    "\n",
    "parser.add_argument('-opt', type=str, help='use which optimizer. SDG or Adam', default='SGD')\n",
    "parser.add_argument('-lr', default=0.1, type=float, help='learning rate')\n",
    "parser.add_argument('-momentum', default=0.9, type=float, help='momentum for SGD')\n",
    "parser.add_argument('-lr_scheduler', default='CosALR', type=str, help='use which schedule. StepLR or CosALR')\n",
    "parser.add_argument('-step_size', default=32, type=float, help='step_size for StepLR')\n",
    "parser.add_argument('-gamma', default=0.1, type=float, help='gamma for StepLR')\n",
    "parser.add_argument('-T_max', default=64, type=int, help='T_max for CosineAnnealingLR')\n",
    "parser.add_argument('-model', type=str)\n",
    "parser.add_argument('-cnf', type=str)\n",
    "parser.add_argument('-T_train', default=None, type=int)\n",
    "parser.add_argument('-dts_cache', type=str, default='./dts_cache')\n",
    "args = parser.parse_args([])\n",
    "print(args)\n",
    "args.model = \"SEWResNet\"\n",
    "args.cnf = 'ADD'\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54dc5223",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetN(\n",
      "  (conv): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): SeqToANNContainer(\n",
      "        (0): Conv2d(2, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): LIFNode(\n",
      "        v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "        (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "      )\n",
      "    )\n",
      "    (1): SEWBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): SeqToANNContainer(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): SEWBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): SeqToANNContainer(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (5): SEWBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): SeqToANNContainer(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (7): SEWBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): SeqToANNContainer(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): SeqToANNContainer(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): LIFNode(\n",
      "        v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "        (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "      )\n",
      "    )\n",
      "    (10): SEWBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): SeqToANNContainer(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (12): SEWBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): SeqToANNContainer(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (14): SEWBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): SeqToANNContainer(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): LIFNode(\n",
      "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
      "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): SeqToANNContainer(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (16): Flatten(start_dim=2, end_dim=-1)\n",
      "  )\n",
      "  (out): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNetN(\n",
       "  (conv): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): SeqToANNContainer(\n",
       "        (0): Conv2d(2, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): LIFNode(\n",
       "        v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "        (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "      )\n",
       "    )\n",
       "    (1): SEWBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SeqToANNContainer(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): SEWBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): SeqToANNContainer(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (5): SEWBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): SeqToANNContainer(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (7): SEWBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): SeqToANNContainer(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): SeqToANNContainer(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): LIFNode(\n",
       "        v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "        (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "      )\n",
       "    )\n",
       "    (10): SEWBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): SeqToANNContainer(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (12): SEWBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): SeqToANNContainer(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (14): SEWBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): SeqToANNContainer(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): LIFNode(\n",
       "            v_threshold=1.0, v_reset=0.0, detach_reset=True, step_mode=m, backend=torch, tau=2.0\n",
       "            (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (15): SeqToANNContainer(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (16): Flatten(start_dim=2, end_dim=-1)\n",
       "  )\n",
       "  (out): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = smodels.__dict__[args.model](args.cnf)\n",
    "print(net)\n",
    "net.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33191716",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = None\n",
    "if args.opt == 'SGD':\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "elif args.opt == 'Adam':\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "else:\n",
    "    raise NotImplementedError(args.opt)\n",
    "\n",
    "lr_scheduler = None\n",
    "if args.lr_scheduler == 'StepLR':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "elif args.lr_scheduler == 'CosALR':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.T_max)\n",
    "else:\n",
    "    raise NotImplementedError(args.lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87b14fd0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: '/raid/wfang/datasets/CIFAR10DVS\\\\download'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     test_set \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(test_set_pth)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     origin_set \u001b[38;5;241m=\u001b[39m \u001b[43mcifar10_dvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCIFAR10DVS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mframe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumber\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     train_set, test_set \u001b[38;5;241m=\u001b[39m split_to_train_test_set(\u001b[38;5;241m0.9\u001b[39m, origin_set, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(args\u001b[38;5;241m.\u001b[39mdts_cache):\n",
      "File \u001b[1;32mD:\\software\\anaconda\\envs\\my_torch\\Lib\\site-packages\\spikingjelly\\datasets\\cifar10_dvs.py:128\u001b[0m, in \u001b[0;36mCIFAR10DVS.__init__\u001b[1;34m(self, root, data_type, frames_number, split_by, duration, custom_integrate_function, custom_integrated_frames_dir_name, transform, target_transform)\u001b[0m\n\u001b[0;32m    110\u001b[0m    \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    111\u001b[0m            \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    112\u001b[0m            root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m            target_transform: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    121\u001b[0m    ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m \u001b[38;5;250m       \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m       The CIFAR10-DVS dataset, which is proposed by `CIFAR10-DVS: An Event-Stream Dataset for Object Classification\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m<https://internal-journal.frontiersin.org/articles/10.3389/fnins.2017.00309/full>`_.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m       Refer to :class:`spikingjelly.datasets.NeuromorphicDatasetFolder` for more details about params information.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m       \"\"\"\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m        \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_integrate_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_integrated_frames_dir_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\software\\anaconda\\envs\\my_torch\\Lib\\site-packages\\spikingjelly\\datasets\\__init__.py:680\u001b[0m, in \u001b[0;36mNeuromorphicDatasetFolder.__init__\u001b[1;34m(self, root, train, data_type, frames_number, split_by, duration, custom_integrate_function, custom_integrated_frames_dir_name, transform, target_transform)\u001b[0m\n\u001b[0;32m    676\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    677\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis dataset can not be downloaded by SpikingJelly, please download [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] from [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] manually and put files at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownload_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 680\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMkdir [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownload_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] to save downloaded files.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    682\u001b[0m     resource_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresource_url_md5()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: '/raid/wfang/datasets/CIFAR10DVS\\\\download'"
     ]
    }
   ],
   "source": [
    "# train_set_pth = os.path.join(args.dts_cache, f'train_set_{args.T}.pt')\n",
    "# test_set_pth = os.path.join(args.dts_cache, f'test_set_{args.T}.pt')\n",
    "# if os.path.exists(train_set_pth) and os.path.exists(test_set_pth):\n",
    "#     train_set = torch.load(train_set_pth)\n",
    "#     test_set = torch.load(test_set_pth)\n",
    "# else:\n",
    "#     origin_set = cifar10_dvs.CIFAR10DVS(root=args.data_dir, data_type='frame', frames_number=args.T, split_by='number')\n",
    "\n",
    "#     train_set, test_set = split_to_train_test_set(0.9, origin_set, 10)\n",
    "#     if not os.path.exists(args.dts_cache):\n",
    "#         os.makedirs(args.dts_cache)\n",
    "#     torch.save(train_set, train_set_pth)\n",
    "#     torch.save(test_set, test_set_pth)\n",
    "\n",
    "# train_data_loader = DataLoader(\n",
    "#     dataset=train_set,\n",
    "#     batch_size=args.b,\n",
    "#     shuffle=True,\n",
    "#     num_workers=args.j,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=True)\n",
    "\n",
    "# test_data_loader = DataLoader(\n",
    "#     dataset=test_set,\n",
    "#     batch_size=args.b,\n",
    "#     shuffle=False,\n",
    "#     num_workers=args.j,\n",
    "#     drop_last=False,\n",
    "#     pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cccec0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集获取  CIFAR10DVS  10个类别\n",
    "def get_cifar10dvs(batch_size = 16,num_workers = 4,T = 20,\n",
    "                   train_path = r'E:\\mycode\\jupyter\\0.SNN\\data\\CIFAR10DVS\\split\\t20\\train0.8.pth', \n",
    "                   test_path = r'E:\\mycode\\jupyter\\0.SNN\\data\\CIFAR10DVS\\split\\t20\\test0.2.pth'):\n",
    "    train_set = torch.load(train_path, weights_only=False)\n",
    "    test_set = torch.load(test_path, weights_only=False)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True,num_workers=num_workers,pin_memory=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=False,num_workers=num_workers,pin_memory=True)\n",
    "    return train_loader,test_loader\n",
    "\n",
    "train_loader,test_loader = get_cifar10dvs(args.b, args.j)\n",
    "\n",
    "scaler = None\n",
    "if args.amp:\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "start_epoch = 0\n",
    "max_test_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53bb4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.resume:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    max_test_acc = checkpoint['max_test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c980bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../save_models\\\\SEWResNet_ADD_T_20_T_train_None_SGD_lr_0.1_'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = os.path.join(args.out_dir, f'{args.model}_{args.cnf}_T_{args.T}_T_train_{args.T_train}_{args.opt}_lr_{args.lr}_')\n",
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "710d1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.lr_scheduler == 'CosALR':\n",
    "    out_dir += f'CosALR_{args.T_max}'\n",
    "elif args.lr_scheduler == 'StepLR':\n",
    "    out_dir += f'StepLR_{args.step_size}_{args.gamma}'\n",
    "else:\n",
    "    raise NotImplementedError(args.lr_scheduler)\n",
    "\n",
    "if args.amp:\n",
    "    out_dir += '_amp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7607cc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../save_models\\\\SEWResNet_ADD_T_20_T_train_None_SGD_lr_0.1_CosALR_64'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5bafbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mkdir ../save_models\\SEWResNet_ADD_T_20_T_train_None_SGD_lr_0.1_CosALR_64.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    print(f'Mkdir {out_dir}.')\n",
    "else:\n",
    "    print(out_dir)\n",
    "    assert args.resume is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99b1b363",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mkdir ../save_models\\SEWResNet_ADD_T_20_T_train_None_SGD_lr_0.1_CosALR_64_pt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../save_models\\\\SEWResNet_ADD_T_20_T_train_None_SGD_lr_0.1_CosALR_64_pt'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_dir = out_dir + '_pt'\n",
    "if not os.path.exists(pt_dir):\n",
    "    os.makedirs(pt_dir)\n",
    "    print(f'Mkdir {pt_dir}.')\n",
    "pt_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6deb199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "    args_txt.write(str(args))\n",
    "\n",
    "writer = SummaryWriter(os.path.join(out_dir, 'logs'), purge_step=start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f9dd197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2bc53b0b68445839a1c0788595ec6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000278FEE6CA40>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\software\\anaconda\\envs\\my_torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"D:\\software\\anaconda\\envs\\my_torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1562, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(out_fr, label)\n\u001b[0;32m     28\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 29\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m train_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[0;32m     32\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m label\u001b[38;5;241m.\u001b[39mnumel()\n",
      "File \u001b[1;32mD:\\software\\anaconda\\envs\\my_torch\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\software\\anaconda\\envs\\my_torch\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mD:\\software\\anaconda\\envs\\my_torch\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mD:\\software\\anaconda\\envs\\my_torch\\Lib\\site-packages\\torch\\optim\\sgd.py:123\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    117\u001b[0m momentum_buffer_list: List[Optional[Tensor]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    119\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    120\u001b[0m     group, params, grads, momentum_buffer_list\n\u001b[0;32m    121\u001b[0m )\n\u001b[1;32m--> 123\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params, momentum_buffer_list):\n",
      "File \u001b[1;32mD:\\software\\anaconda\\envs\\my_torch\\Lib\\site-packages\\torch\\optim\\sgd.py:298\u001b[0m, in \u001b[0;36msgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, fused, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    296\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[1;32m--> 298\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\software\\anaconda\\envs\\my_torch\\Lib\\site-packages\\torch\\optim\\sgd.py:414\u001b[0m, in \u001b[0;36m_multi_tensor_sgd\u001b[1;34m(params, grads, momentum_buffer_list, grad_scale, found_inf, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[0;32m    411\u001b[0m         bufs\u001b[38;5;241m.\u001b[39mappend(cast(Tensor, device_momentum_buffer_list[i]))\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_states_with_momentum_buffer:\n\u001b[1;32m--> 414\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(bufs, device_grads, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dampening)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, args.epochs):\n",
    "    start_time = time.time()\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    datas = tqdm(iter(train_loader),file=sys.stdout)\n",
    "    for frame, label in datas:\n",
    "        optimizer.zero_grad()\n",
    "        frame = frame.float().to(args.device)\n",
    "\n",
    "        if args.T_train:\n",
    "            sec_list = np.random.choice(frame.shape[1], args.T_train, replace=False)\n",
    "            sec_list.sort()\n",
    "            frame = frame[:, sec_list]\n",
    "\n",
    "        label = label.to(args.device)\n",
    "        if args.amp:\n",
    "            with amp.autocast():\n",
    "                out_fr = net(frame)\n",
    "                loss = F.cross_entropy(out_fr, label)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out_fr = net(frame)\n",
    "            loss = F.cross_entropy(out_fr, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "        functional.reset_net(net)\n",
    "    train_loss /= train_samples\n",
    "    train_acc /= train_samples\n",
    "\n",
    "    writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    writer.add_scalar('train_acc', train_acc, epoch)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    test_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for frame, label in test_loader:\n",
    "            frame = frame.float().to(args.device)\n",
    "            label = label.to(args.device)\n",
    "            out_fr = net(frame)\n",
    "            loss = F.cross_entropy(out_fr, label)\n",
    "\n",
    "            test_samples += label.numel()\n",
    "            test_loss += loss.item() * label.numel()\n",
    "            test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "            functional.reset_net(net)\n",
    "\n",
    "    test_loss /= test_samples\n",
    "    test_acc /= test_samples\n",
    "    writer.add_scalar('test_loss', test_loss, epoch)\n",
    "    writer.add_scalar('test_acc', test_acc, epoch)\n",
    "\n",
    "    save_max = False\n",
    "    if test_acc > max_test_acc:\n",
    "        max_test_acc = test_acc\n",
    "        save_max = True\n",
    "\n",
    "    checkpoint = {\n",
    "        'net': net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'max_test_acc': max_test_acc\n",
    "    }\n",
    "\n",
    "    if save_max:\n",
    "        torch.save(checkpoint, os.path.join(pt_dir, 'checkpoint_max.pth'))\n",
    "\n",
    "    torch.save(checkpoint, os.path.join(pt_dir, 'checkpoint_latest.pth'))\n",
    "    for item in sys.argv:\n",
    "        print(item, end=' ')\n",
    "    print('')\n",
    "    print(args)\n",
    "    print(out_dir)\n",
    "    total_time = time.time() - start_time\n",
    "    print(f'epoch={epoch}, train_loss={train_loss}, train_acc={train_acc}, test_loss={test_loss}, test_acc={test_acc}, max_test_acc={max_test_acc}, total_time={total_time}, escape_time={(datetime.datetime.now()+datetime.timedelta(seconds=total_time * (args.epochs - epoch))).strftime(\"%Y-%m-%d %H:%M:%S\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_torch)",
   "language": "python",
   "name": "my_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
