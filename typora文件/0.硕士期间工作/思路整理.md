## 李沐视频关于科研思路的总结

[参考链接： **大模型时代下做科研的四个思路**](https://www.bilibili.com/read/cv22624519?spm_id_from=333.999.0.0&jump_opus=1)

1、Efficient(PEFT) 
提升训练效率，这里以PEFT(parameter efficient fine tuning)为例

2、Existing stuff(pretrained model)、New directions
使用别人的预训练模型，带入到新的研究方向

3、plug-and-play
做一些即插即用的模块，例如模型的模块、目标函数、新损失函数、数据增强方法等等。

4、Dataset,evaluation and survey
构建数据集、发表分析为主的文章或者综述论文（==未来可以尝试==）

---

---

## 读各类图像分类模型的思路整理

1. * 通过阅读发现大部分都是用了多少特征等等，在深度学习模型方面进行适用性改进的论文并没有，有使用深度学习模型的文章也仅仅是为了对比。发了高区的文章都倾向于特征提取算法的解释乃至数学公式的推导。
   * 从自己思路出发的话，肯定重点在模型的使用，进一步可以考虑为各个模型效果的融合。例如对四个轻量化模型跑完，保留概率，最终选择概率最高的模型，这个过程是否要保证超参数的一致性？显然不太符合常理，因为对某个网络适用的参数并不一定可以对其余网络适用。那么改进方式可以是：==首先进行每个网络的调优，确保每个网络对当前数据集分类的总体准确率是最高的，接着使用多个依据此方式训练完的模型，对测试集进行识别时，就依据概率的成分来进行选择，选择相似度最高的几个答案并求均值作为最终的结果。==
     **查查是否有此类的参考文献**
   * 保证数据在流经网络的过程中 通道数与长宽的变换比例是一致的，是否可以最大限度保证网络的性能，又或者对于多个网络，存在一个较为适用的比例，可以保证用这些网络时，准确率总是能达到很高的水平。
     **查查是否有此类的参考文献，这一点是否属于引入了先验知识**
   * 同时可以尝试使用Transformer这种大模型用于训练，尽管我们的数据并不多。但是需要对该模型进行一个变化，结合自己的数据，首先构建一个直观上容易理解且让人可以接受的模型，接着在实验中去具体验证该想法。
     **查查是否有此类的参考文献，另外关于改进在下面会提到**

2. 关于CNN与Transformer的论述：
   ```python
   CNN模型结果先行           {轻量级模型：MobilenetV2,重量级模型：EfficientnetV2}
   Transformer模型结果先行    {SwinTransformer}
   ```

3. [DropPath 解析](https://www.cnblogs.com/dan-baishucaizi/p/14703263.html)
   [迁移学习冻结某些层参数](https://www.cnblogs.com/shanqiu24/p/15844457.html)
   [unsqueeze函数解释](https://blog.csdn.net/ljwwjl/article/details/115342632)
   [transformer中QKV的通俗理解(剩女与备胎的故事)](https://blog.csdn.net/qq_42672770/article/details/128231573)
   掩码尺寸的得到过程：假设输入的数据维度是(2,4)， 经过嵌入层变为(2,4,512) 紧接着设置head = 8， 则维度变为（2,4,8,64）, 变换位置 （2,8,4,64）, 将其与（2,8,64,4）相乘变为(2,8,4,4)  则掩码的尺寸为（8,4,4） 所以其实只需要知道初始数据一个batch_size的数量及head就可得到掩码的尺寸
   [Transformer老教程](https://www.bilibili.com/video/BV18b411o7Yw/?p=43&spm_id_from=pageDriver&vd_source=3be4a0eefc99c3ec4109b6e4f90586d1)
   [对应的代码](https://blog.csdn.net/weixin_50973728/article/details/125236187)
   [对应的代码2](https://blog.csdn.net/jiaowoshouzi/article/details/89641775)
   [对应的代码3](https://www.cnblogs.com/shouhuxianjian/p/16165451.html)
   [对应的代码4](https://zhuanlan.zhihu.com/p/399512734)
   [PyTorch笔记 - IMDB数据集文本分类项目模型与训练](https://blog.csdn.net/caroline_wendy/article/details/126412391)
   [语音 知识](https://zhuanlan.zhihu.com/p/616148064)
   [了解梅尔频谱知识](https://it.cha138.com/javascript/show-93485.html#spectrogram)
   [Matplotlib 折线图 二维数据绘制](https://blog.csdn.net/qq_40491534/article/details/121343014)
   [语音特征小结](https://zhuanlan.zhihu.com/p/281947972?utm_source=wechat_session)
   [MFCC特征一区过程详解](https://blog.csdn.net/jojozhangju/article/details/18678861)
   [心音pcg识别处理流程](https://zhuanlan.zhihu.com/p/334586587?utm_id=0)
   [分析 transformer参数量：——知乎链接](https://zhuanlan.zhihu.com/p/624740065)
   [LLaMA, ChatGLM, BLOOM的参数高效微调实践：——知乎链接](https://zhuanlan.zhihu.com/p/635710004)

4. [使用pydub库读取mp3文件，参考配置环境](https://blog.csdn.net/SoYouTry/article/details/121004561)
   [安装ffmpeg，这一步完了必须重启电脑](https://blog.csdn.net/m0_53574178/article/details/122565831)
   [Python读取mp3数据,参考代码](https://blog.csdn.net/sinat_35821976/article/details/115875969)
   [Python | 语音处理 | 用 librosa / AudioSegment / soundfile 读取音频文件的对比](https://blog.csdn.net/qq_37851620/article/details/127149729)
   ==**安装libsora出现超时问题，一般是由于镜像源没有中科大，需要添加，再确认网络无问题即可顺利安装。**==
   [二阶谱的网站](https://zhuanlan.zhihu.com/p/334586587)
   [二阶谱的另外一个网站](https://aistudio.baidu.com/aistudio/projectdetail/2192884)
   [科研干货：如何用MestRenova和Origin软件绘制核磁二维谱](https://mp.weixin.qq.com/s?__biz=MzUxMDMzODg2Ng==&mid=2247566163&idx=2&sn=12f53d8e28e99c11ffd94f98ba69ff96&chksm=f907f10ece7078187e25fc248a40f1d6e02d40b202073f20d040e5dd7567cc98387c6ed89512&scene=27)
   [频谱分析-FFT之后的那些事情](https://blog.csdn.net/czyt1988/article/details/84995295)
   [librosa | 梅尔谱图最通俗的解释](https://blog.csdn.net/qq_44250700/article/details/125372510)
   [librosa 语音库（二）STFT 的实现](https://blog.csdn.net/chumingqian/article/details/124843635)
   [零基础入门语音识别: 一文详解MFCC特征（附python代码）](https://zhuanlan.zhihu.com/p/365714663)
   [如何用python画出语谱图（spectrogram）和mel谱图（mel spectrogram）](https://blog.csdn.net/pk296256948/article/details/118752128)
   [论文笔记：语音情感识别（四）语音特征之声谱图，log梅尔谱，MFCC，deltas](https://www.cnblogs.com/liaohuiqiang/p/10159429.html)
   [HHT变换](https://blog.csdn.net/qq_34769201/article/details/88399044)
   [希尔伯特-黄变换（HHT）的前世今生——一个从瞬时频率讲起的故事](https://zhuanlan.zhihu.com/p/124257081?from=groupmessage)
   [HHT算法实现（基于Python）](https://zhuanlan.zhihu.com/p/149712247?from_voters_page=true)
   [Python中HHT(希尔伯特-黄变换)以及其在EEG数据处理中的应用](https://blog.csdn.net/zhoudapeng01/article/details/108448218)
   [希尔伯特-黄变换（HHT）的前世今生——一个从瞬时频率讲起的故事](https://blog.csdn.net/fengzhuqiaoqiu/article/details/105390876)
   [语音合成基础(3)——关于梅尔频谱你想知道的都在这里](https://zhuanlan.zhihu.com/p/421460202)
   [语谱图](https://blog.csdn.net/sinat_19628145/article/details/90107913)
   [python实现时间序列信号的频谱、倒频谱以及功率谱](https://blog.csdn.net/qq_41281244/article/details/108686022)
   [Python 信号处理——包络分析](https://blog.csdn.net/m0_37262671/article/details/125303404)
   [语音信号处理(四)：时域音频特征及Python实现](https://zhuanlan.zhihu.com/p/492267506)
   [基于python的包络谱信号分析](https://zhuanlan.zhihu.com/p/566167954)
   [语音信号预处理——数字滤波器](https://blog.csdn.net/qq_34218078/article/details/90901602/)
   [Anaconda常用指令，更新查看添加下载源等](https://blog.csdn.net/duiwangxiaomi/article/details/109593234)
   [torchtext,torchdata与torch对应的版本关系](https://zhuanlan.zhihu.com/p/598841083)
   ==**torch的版本号为1.12.1  那么torchtext就必须为0.13.1   torchdata**==
   [Torchtext Field已经在最新版本中被移除](https://zhuanlan.zhihu.com/p/485686510?utm_id=0)
   [小波变换的类型,连续，离散](https://blog.csdn.net/weixin_46713695/article/details/127147800)
   [如何查看时频图](https://zhuanlan.zhihu.com/p/437209917)
   [RNN](https://zhuanlan.zhihu.com/p/30844905?ivk_sa=1024320u)
   [超算，高性能计算，并行计算，分布式计算，云计算](https://zhuanlan.zhihu.com/p/31142609)
   [云计算与并行计算、分布式计算等的区别与联系](https://weibo.com/ttarticle/p/show?id=2309404652349267509462)

5. 

---

---

# 当下自己准备的思路

==情感分析：对自然语言文本中表达的情绪、态度等信息进行分析。==
当前想法的构建应该更适合使用情感分析类的模型

==在做心电心音同步特征寻找特征过程中发现：==
正常人心电信号的**P波和R波间距呈现稳定的状态**，所以是否可以凭借这一点来进行其他深入的调研。

==知识：==CNN的平移不变性：每次用同一个kernel扫描图像，这个kernel在这一次迭代中里面的参数是不会发生变化的。 无论图像进行翻转还是平移，等卷积核移动到那块的时候，出来的数据总是一样的。————依据这个延伸出来的一个说法就是 局部连接，也就是卷积核每次只处理一小部分区域，接着通过滑动处理另外的区域。————依据这个出来的第二个特性就是权值共享，因为滑动过程中卷积核本身并没有进行迭代变化。

​           局部相关性：在一个kernel内，我们使用的是其内部的参数，这些参数就是一堆数字，不同的数字可以理解为代表		   了当前位置的权重，因为这些数字可以互成比例，故我们说cnn具有局部相关性。

多头自注意力中现有的一个想法：让每一个单词用一个单独的头，==后续可以试试这个想法==
**缺点：**预计会复杂很多，这也就导致每次加载的数据 batch_size 不能太大， 所以可能会使得非常过拟合，进而在测试集上效果变差。

==新思路：==用头数来做交叉，类似于下面这种方式，那这样的话，我们的启发也就来了，就说在划分数据的过程中想到了这种头数划分的方法，可以解释为 这是一种信息交叉的手段，但是又考虑到会过拟合，所以可以对比两种效果，文中的其余对比可以是经典残差，convnet网络，transformer的encoder，Decoder，接着是我们这种重叠头与注意力的简化(flash attention)， 对应到病症上，这种重叠也能够解释为增加对信号每一小段前后的理解，进而强化对疾病及非疾病的差异程度。（如果这个验证有效，那是否可以将这种方法用于特征提取?）
<img src="../../../AppData/Roaming/Typora/typora-user-images/image-20231127091638751.png" alt="image-20231127091638751" style="zoom: 67%;" />

==仍需要继续学习的东西：== 学习当前的大模型并尝试进行微调，归纳一下思想，是否有新的想法可以添加。
模型在多项生理信号好都能取得很好的性能甚至是SOTA。
之前发现的最大池化对心电心音有效，是否可能进行扩展？是不是对于所有的生理信号，最大池化相比卷积来说更能提取到有用的特征？

==思路扩展：== 上面的50%交叉是否可以是随机选择的，保证1 3 5 是顺序的， 2的一半是1中的随机的一部分，2的另一半是3中的随机一部分。 达到通道数的完美交叉利用。是否可以扩大模型的泛化能力？

==网络初期所用的结构==：可以在输入数据刚进入网络的时候先使用几个3x3卷积+池化+非线性激活函数来控制一下长和宽，减缓信息的损失，增加可学习的内容

==**训练一个完全dense的网络，然后在上面剪枝才是最好的方法，unet++如是说。**==



==网络构建思路：==  尝试对多路数据进行融合后使用残差接着在分离。。。。。。



![image-20240426105130995](../../../AppData/Roaming/Typora/typora-user-images/image-20240426105130995.png)

# 关于SNN未来思路的几种借鉴观点

[参考链接](https://www.zhihu.com/question/554950172/answer/3403194785)

1. 边缘神经网络硬件加速器，处理器
2. 深度学习，deep learning仍然有很多可以做的点。
3. 当前最火的还是大模型，那就需要将模型放在云端，但是在很多场景下，无法支持将本地数据送往云端来进行模型推理，因为涉及到了更多的隐私问题。-==解决方案为：将数据进行加密上传 or 压缩网络模型在本地运行。==
   * 对于第一种，对应的技术为**完全**同态加密(Fully Homomorphic Encryption (FHE)）,在联邦学习中。
     ==该方法存在的问题为：==在安全性要求极高的环境下数据传输仍然具有较高的风险。且因为该方法仍然是将数据上传到云端，当云端突然宕机的条件下，本地设备可能完全处于瘫痪状态，那么安全性又怎么保障？
   * 对于第二种，将模型放到本地，此时模型规模就是一个大问题了。此时可以尝试选择压缩模型参数来减小本地运算的压力。 目前常见的神经网络都是人工设计的，难以比较出何种参数为最好，且超参数和网络的剪枝也会影响模型性能====神经网络被说成是炼丹的原因之一。
     * 使用 Network Architecture Search (NAS) 进行网络搜索，最终获得一个最优子网络，这是一个方案。
     * 如果获得不了一个更小的网络，那么压缩参数数据量也是一种可行的方案。当前神经网络的权重一般为32位甚至64位的浮点数，但根据先前研究，在训练过程中甚至训练后，把神经网络参数量化到低精度的16位浮点数，8位整数，神经网络依然可以正常工作，只是精度会小一些。且当量化为 4bit，2bit甚至1bit的时候，神经网络也可以工作，精度仅损失百分之几。
     * 1bit的整数神经网络相对于32bit，消耗的硬件资源会小很多，那带来的推理速度就会快很多。
     * 目前工业界最多量化到8bit，更低的精度会变得不安全。但是==把32bit浮点数压缩成1bit数据时，网络仍能够正确运行的原因是啥？==  超低精度量化后，神经网络仍然可以正常工作的现象本身就已经说明当前我们所采用的网络训练方法并不完善， **这属于神经网络的基础研究。**
     * 可以尝试做==混合精度神经网络==，不同的层使用不同的精度（理想化来说，低层的数据低精度，高层的数据高精度）。    **数据压缩率和精度之间的权衡**    
       还有一个新出现的数据表示法：Posit低精度浮点数格式。
4. 除了常规神经网络，脉冲神经网络SNN也是一个研究的点。
   * 特点：结构计算简单，数据吞吐量高，ANN转SNN仍然较为普遍， 且目前SNN的精度比之前的低精度量化神经网络还要低。
   * 抛弃BP，使用完全生物神经的STDP算法来训练SNN，但是到目前为止，STDP的效果很差。
   * 1bit量化神经网络的激活函数不是常见的sigmoid或者ReLU之类的，而是Sign（阈值比较，大于阈值，输出1，小于阈值，输出0），**问题来了， 1bit二值化量化神经网络与SNN之间的关系是啥？** 
   * 32位浮点数神经网络，8bit整数量化神经网络，1bit二值化量化神经网络，还有IF模型，又或者LIF模型的SNN神经网络之间是否有什么潜在的联系呢？为什么这些看起来差别巨大，甚至训练方法都不一样的神经网络都可以工作呢？
     低精度量化神经网络QNN。 （Low precision quantization neural network）
     ==目前的大模型：==  没有使用低精度量化或者SNN技术的。
     ==AI大模型的崛起==： 使用的结构仍然和十几年前的技术无任何区别。
5. 关于该回答的评论
   * 问题：上述所说的==硬件加速==这个方向还有东西做吗，似乎我本科时候硬件加速AI就已经很火了。就像你说的，本质上都是乘加或者类似的基础计算，数字电路设计也没有什么新的结构吧？
   * 回答：肯定有的做啊，cpu的结构出来那么多年了不还是年年出新型号。要说彻底改变架构的大工作那是不好做，也不是我个博士生搞得定的。但是**针对低精度，针对数据流瓶颈，针对混合神经网络架构，还有安全性，比如完全同态加密的特殊计算硬件，还有鲁棒性，某些抗干扰的编码，还有SNN的硬件，做一些设计上的优化，对新出现的神经网络技术做硬件适配**。这都有的做，比如Transformer里有个位置编码，比如VAE里的随机数，这些不属于基础的神经网络计算操作，做一下硬件上的实施改进工作，优化电路提高吞吐率还有降低能耗，还是很多可以做的。
   * 问题：作者当前研究的方向是啥？新的计算架构吗, 还是各种数据流或者是稀疏?
   * 回答：我做的有点类似数据流，也修改了一部分架构，在设计一个边缘上的可变精度通用加速器，利用网络数据流的方式进行传输，尝试做成自组织集群的形式，
   * 问题：无论是定点数还是浮点数乘法，在硬件实现上是类似的，都有一个乘法部件y=axb，是否有可能用一个近似计算y=f(a,b)，f关于a和b都是单调递增的，通过简化进位链，实现硬件规模、处理速度上的优化。并且，这也相当于把非线性的激活耦合到里面。
   * 回答：你说的有人做过类似的，针对的1-2bit量化神经网络，利用的是FPGA中LUT的特性，把从输入到激活输出压缩到一个LUT中，叫LogicNets。这个有一个问题，如果所有输入并行，然后一个输出，fan in太大，硬件消耗下不来，critical path delay太高，频率上不去。所以必须配合很高比例的剪枝使用，这又影响了精度。如果分batch串行输入，然后累积处理，这个累积的模块其实就是个类似近似计算Approximate Computing 的东西。但是问题出在如果这是个近似计算组件，误差也会累积，有可能影响最终输出，在训练上可能也需要配合的优化算法。
   * 问题：您觉得模型蒸馏，压缩 在算法端更有效还是硬件上优化更有潜力呢？现在车载orin算力也能到250TFLOPs 左右了吧，目前这种算力能够负担多大规模的模型呢？
   * 回答： 知识蒸馏，剪枝，量化，NAS都是很适合边缘硬件的模型压缩方案，我见过的还有权重共享，字典编码之类的。但是大部分都是在服务器或者主机上训练好了模型的参数，然后再传输到边缘硬件上，==直接在边缘硬件上做训练的工作很少==。==因为边缘硬件更多的定位是在产品上提供神经网络推理服务，而不是开发新的神经网络，所以对训练的需求比推理少很多==。某些需要在线微调学习的工作关注这些。
     谷歌TPU理论上也可以在边缘部署，但是这个更多专注于数据中心。250TFLOPS的计算速度是很强的，但是实际运用中计算速度很多情况下，甚至大部分情况下，都吃不满。==因为参数和输入都是存在外部的内存/显存上，片上的缓存量很少，不可能加载全部模型和全部中间结果，导致外部存储的带宽是个大瓶颈。==再加上计算结果也需要往外部存储器上存，数据会在外存和GPU之间不断搬入搬出。这时候电路设计，还有驱动和算法的优化就很重要了，影响会很大。
     但是我估计，一些几年前的LLM经过量化压缩应该是可以上的，但是新一些的估计有难度。这也是最近几年内存计算的加速架构在学界比较火热的原因，为的就是想办法解决这个外部存储瓶颈的问题。
   * 问题：AI是有大量方向可以做，但感觉是不是在国内做不了？
   * 回答：目前AI硬件相关的，特别是EDA（*Electronic Design Automation*）方向的论文，少见要求数据规模特别恐怖的。搞AI大模型的话，主要就还是算力限制。有卡有服务器集群几天训练出来的东西，没有好显卡可能得几个月。但是硬件设计上，基本一个类型的几种基础模型可以跑，那这一类的就差不多都可以跑，剩下的就是推理延迟，功耗，还有会不会爆DDR的问题了!但是这些也可以大概估算出来极限是多少。——DDR（Double Data Rate）是一种双倍速率同步动态随机存储器，广泛应用于计算机和电子设备中。
     大公司的目光目前主要还是在云端，边缘计算要落地，新的神经网络结构，配套的神经网络训练，驱动和运行环境支持，硬件设计优化，高时钟频率下的时序问题，各种工作一大把，足够先让学术界搞个几年试试方向了。
   * 问题：量化8it->4bit，这趋势显然是冲着1bit过去的，那么那时候网络跟SNN有什么区别呢？
   * 回答：低比特的量化在某些情况下也有推理精度问题，但是从我给硬件准备验证网络数据的一点点经验上看，感觉比大比例的剪枝要稍微好一点点？
     1bit的二值化神经网络BNN的输入和IF模型下的SNN最大的差别在没有时间信息。因为SNN是一串编码，信息不止是编码进01，还存在于脉冲时间序列中。按理来说SNN的脉冲编码蕴含比BNN更多的信息，准确度会高一些，但是实际上并没有什么优势，感觉SNN的训练还是有点问题。
   * 问题：我是学通信的，我感觉那1bit不就是纯编码了么，0101数字电路硬做推理？
   * 回答：其实不完全是，二值化神经网络不是简单把输入量化到0/1，而是量化为-1/+1，只不过用0表示-1，用1表示+1。然后算一下+1×+1，+1×-1，-1×+1，-1×-1，结果再换成对应的0/1表示。然后就可以发现输入的01计算和输出的01表示其实可以就是个1bit的XNOR运算。
6. 















