





# ==文献阅读   ANN  to SNN文章==

1. ## ==Spiking deep convolutional neural networks for energy-efficient object recognition==

   脉冲卷积神经网络做高效的目标识别
   [博客链接](https://blog.csdn.net/h__ang/article/details/91345702?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522170592752916800227461663%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=170592752916800227461663&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-3-91345702-null-null.nonecase&utm_term=snn&spm=1018.2226.3001.4450)

* **期刊**：International Journal of Computer Vision  2区 计算机科学 IF=19.5 2014年 
  
* **摘要：** 对CNN进行裁剪，以训练未裁剪CNN的方式继续训练已经裁剪的CNN，接着将学习到的网络权重直接应用到SNN上， 实现了ANN到SNN的转换，并对比了硬件能耗。

* **1 介绍：** 介绍了一些历史性的东西

* **2 Method**

  * 2.1 传统的CNN结构

  * 2.2 实现脉冲型CNN的挑战

    * 将CNN转换为SNN有两种方法，第一种就是直接训练类似CNN结构的SNN，尽管存在一些像STDP的脉冲学习规则以一种自组织、无监督的方式训练SNN，但在这方面的研究目前还处于起步阶段，并且还不清楚如何高效训练SNN来实现更高层次的功能；

      第二个方法就是训练原始的CNN，然后将学习到的权重应用到和训练的CNN结构类似的SNN中。主要的挑战就是在将CNN转换为SNN时分类任务有不可接受的精度损失。

    * CNN转换到SNN有精度损失存在于以下几个方面：

      1. CNN层的负输出值在SNN中很难精确的表示（==例如tanh激活函数，幸运的是近几年的模型大多使用Relu激活函数，所以也就不存在负值了，这篇文章写的比较早，当时ReLU还不火。==）
      2. 不像在CNN中，在脉冲网络中没有什么好办法表示偏置，每个卷积层的偏置可以是正也可以是负，这在SNN中不容易表示；
      3. Max-pooling需要两层脉冲网络，在CNN中，空间上的最大池化就是在一个区域内求最大值，在SNN中，我们需要两层网络来实现它，横向抑制后紧跟着小区域的池化操作，这个方法需要更多的神经元，而且由于额外的复杂度造成了精度损失。   ==没有看到如何转化的，横向抑制说的应该是在时间维度上的操作，紧接着在去掉时间维度后进行最大值的筛选或者求平均值来实现最大池化或者平均池化==

  * 2.3 本文提出的脉冲型卷积网络 

    * 裁剪CNN的方式：

      1. 将所有层的输出置为正——在预处理之后加上abs()函数来使得第一层卷积的输入为非负的，当预处理本身就可以得到非负值的时候这个步骤可以省略（对于CNN来说）；将tanh()函数变为 ReLU()激活函数。

         另外文章中说到使用ReLU激活函数的优点在于：其在大于0时的激活值的线性的，这在将CNN转换为SNN时最大限度的减少了准确性的损失。==直观来看的话是IF神经元模型模拟出的激活值可以和RELU激活函数相匹配==

      2. 将卷积层和全连接层的偏置都置零；
      3. 使用空间线性子采样代替空间最大池化。空间线性子采样使用总和为1.0的均匀权重的内核将小图像邻域上的所有像素相加。空间线性子采样函数可以很容易地转换到尖峰域。==这里的意思感觉就是在做平均池化==

    * ==裁剪后的CNN结构变为SNN的方式：==  新增脉冲产生和脉冲计数模块，其余模块均是剪枝后的CNN中继承过来， HalfRect激活函数就是ReLU。 

    * ![image-20240125094734190](../../../AppData/Roaming/Typora/typora-user-images/image-20240125094734190.png)

    * SNN结构中脉冲神经元的膜电位$V(t)$根据 integrate-and-fire 神经元模型导出的下面的等式在每个time step更新：![image-20240125095335669](../../../AppData/Roaming/Typora/typora-user-images/image-20240125095335669.png)

      

      ![image-20240125100021269](../../../AppData/Roaming/Typora/typora-user-images/image-20240125100021269.png)

      脉冲产生层经常定义为：$I_{ijk}(k=1,2,3)$ 可以看作是脉冲产生层的输入,参照上图中的$X_{ij}(t)$，在t时刻，在第k个 image map (通道)的神经元（i，j）, 如果满足 随机数(在0和1之间)  $rand() < cI_{ijk}$ ，则发射一个脉冲，否则不发射，这里c是一个对脉冲产生频率的缩放量。举个例子，我们一般取c=1/3，RGB图片的输入先被正则化到 [0.0, 1.0] 的范围，脉冲计数对所有类别神经元发射的脉冲进行计数（时间是从输入图片被送入网络开始的整个时间周期），然后发射脉冲最多的神经元对应的就是最终的输出类别。

      ==问题：需要去看原始的代码，因为整体看下来还是感觉说的不够清楚，没法自己去复现。==

* 3 ==Experiments 实验==

  * Tower Dataset1
    ![image-20240123091101700](../../../AppData/Roaming/Typora/typora-user-images/image-20240123091101700.png)

    综合结果来看，仍然是原始的CNN具有很好的结果，接着是裁剪后的CNN，以及SNN。且SNN在最后两个类别的识别效果相比前两个下降的比较多。整体来看，ANN-SNN的转化还是很成功的，不过从CNN结果来看，该数据集还是太简单才导致性能看起来没有下降很多。

  * Tower Dataset2
    ![image-20240123091353418](../../../AppData/Roaming/Typora/typora-user-images/image-20240123091353418.png)

    在数据集2中，SNN反而效果更好了，初步验证SNN或许真的可以替代ANN，==仍然可以认为数据集过于简单，考虑到这个原因，作者在后续也对其他数据集进行了实验==

  * CIFAR-10数据集
    这里作者用的CNN架构并不是性能最好的，因为复杂的CNN架构中有些组件不能转换成SNN，所以就用了一个简单的CNN，并将其转换为SNN去对比结果。这里他们在转SNN架构时，直接先对原始图像进行了归一化的操作。

    ![image-20240123091913868](../../../AppData/Roaming/Typora/typora-user-images/image-20240123091913868.png)
    可以看到，结果还是很好的，但是问题依旧是：在简单的CNN架构上进行转换，没有高性能可言，且通用性不够高

* ==4 Energy Analysis of Mapping to Neuromorphic Hardware==    映射到神经形态硬件上的能效分析

  * ![image-20240123093005295](../../../AppData/Roaming/Typora/typora-user-images/image-20240123093005295.png)

  * 这个表是基于两个已经发布的基于脉冲的神经形态电路的功率特性的总功耗。最后一列中的比例是 CNN功耗/SNN功耗，可以看出第一行方法实现的SNN结构非常节能。

    

* ==5 讨论部分==

  * 1.在输入特征上使用绝对值函数 abs()
  * 2.与有限分辨率CNN实现的比较

* ==6 总结部分==

  CNN转换为SNN需要考虑一下几点：

  * 负值的变换
  * CNN所有的激活函数需要使用 ReLU
  * 将所有的偏差置为0

2. ## ==Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing==

   权重、阈值权衡实现快速分类且高精度的脉冲神经网络
   [参考博客](https://blog.csdn.net/h__ang/article/details/91128509)
   
   * **期刊**：2015 International Joint Conference on Neural Networks IJCNN    CCF-C会议
   
   * **摘要： ** 由于ANN没有时间信息，所以转换为稀疏的脉冲发射型、事件驱动的SNN会有精度损失。作者分析了脉冲神经元的发射率和阈值这些参数的选择对ANNs转换为SNNs的影响，展示了一组优化技术来最小化转换过程中的性能损失。这些技术产生的网络优于之前在MNIST数据集上的表现最好的网络，包括在训练期间使用修正后的零偏置的线性单元（如ReLUs）、使用一种新的权重正则化方法帮助正则化发射率。
   
   * **1. 介绍：**
   
     * 在本篇论文中，我们将介绍对于深度脉冲神经网络的新的优化方法，它可以使得脉冲神经网络获得比之前的脉冲方法更高的性能，同时实现了低延迟、更少的操作。
   
       与在传统CPUs或GPUs上跑神经网络相比，神经形态平台的功耗可能要低好几个数量级，因为它允许分布式和异步基于事件驱动的计算，因此提高了可扩展性和减少了延迟。
   
       提到了上一篇文献，最近，“Spiking deep convolutional neural networks for energy-efficient object recognition”提出了一种转换的方法，它的表现比以前的方法都好，因为它将脉冲和非脉冲网络之间的特征差异考虑了。主要挑战就是对于脉冲神经元中的负值和偏置的表示，这个通过使用修正的线性单元（ReLUs）和将偏置设为零解决了。同时，卷积网络的最大池化操作被空间线性子采样替代，==同样地转换结果也有很小的损失。==
   
       在这篇工作中，我们展示了这种微小损失的来源，并且展示了优化的几种工具。我们发现如果SNNs以正确的方式驱动，接近无损的转换是可能的，并且还可以进行非常快速的分类仅基于少量输出峰值。
   
   * **2. NEURAL NETWORK ARCHITECTURES**  神经网络结构
     ==简单介绍了一下，这里不扩展了==
   
     * 2.1 基于ReLU的前馈神经网络
     * 2.2 卷积神经网络
     * 2.3 Dropout
   
   * **3. Spiking Neural Networks** 脉冲神经网络
   
     * 3.1 背景
   
     * 在传统的ANN中，整个输入向量同时被送入网络，然后逐层处理，最终产生输出值。在SNN中，输入通常以事件流的形式传入网络，然后神经元在这个时间段内整合数据，产生脉冲用于将信息传递给后续层，最终驱动输出神经元发射脉冲。这种方法有非常重要的优势：输入和输出的“假象同步”可以实现，并且时变输入可以更高效地处理，同时在特殊硬件上的更加高效的计算可以实现。
   
     * 3.2 脉冲网络转换
       基于上一篇文章(Spiking deep convolutional neural networks for energy-efficient object recognition)提出的转换方法，他们添加了新颖的正则化方法、发射率和阈值的分析。
   
       给出了ANN TO SNN 转换的几点建议：
   
       1. 对于网络中的所有单元使用ReLU；
       2. 在训练过程中将偏置设置为0，并利用反向传播更新网络参数；
       3. 直接将ReLU网络的权重映射为IF单元网络； ==仿真结果显示这两者在0-1之间的函数曲线较为一致==
       4. 使用权重正则化来获得接近无损的精度和更快的收敛；==其实就是引入了缩放比例==
   
       上面的这些建议适用于全连接层和卷积层，一旦人工神经网络中的ReLU在训练后被IF神经元取代，那么在固定仿真期间的性能损失主要来源于三个因素：
   
       1. 单个时间步内==没有接受充分的输入==使得膜电位超过阈值，即发射率低于它原来应该达到的发射率；
       2. 一个时间步内，单个时间步内接受的输入过多而导致输出脉冲过多。
       3. 结合前两点：脉冲输入的随机性与不均匀性，导致脉冲过激活或者欠激活。
       
       然而，我们要在脉冲阈值、输入权重和输入发射率之间找到一个权衡。具体来讲，高的输入阈值（或低的输入权重）会减小过激活和非理想脉冲序列的错误，然而同样增加了欠激活的风险，反之亦然。请注意脉冲阈值和输入权重的比率决定了脉冲的数量，不用手动调整参数，在这里我们提出了一个更加严格的方法来调整网络权重——通过计算权重的缩放因子对权重归一化从而减少了上面说的三个因素带来的错误。
       
     * 3.3 权重正则化
     
       这里提出了两种方法来对网络的权重归一化，并且确保激活值足够小，能够防止ReLU过高估计输出激活值。
     
       * (1) 最安全、最保守的方法就是考虑所有可能的正的激活值，并且通过可能的最大激活值（同时也是下一层的输入）对权重缩放。如果最大的正输入仅仅对应单个脉冲，那么网络同一个神经元在一个timestep内至多发射一个脉冲。通过这样做，脉冲神经网络变得具有鲁棒性——可以接受任意高的输入发射率并且完全消除由于太多输入而导致的转换损失。不幸的是，这意味着为了产生一个脉冲整合数据可能要花费很长时间，如果一个分类任务对性能要求比较高且可以接受更长的采样时间，这种方法就很合适。
     
         这种方法可能是一种寻找合适的权重缩放因子的方法。我们常将这种方法称为"model-based normalization"
     
       * (2) 训练集也可以用来评估网络中的激活值，而不是假设最大正激活值的最坏情况。在我们的实验中，观察到使用第二种方法创建的这个缩放因子相对保守，它的精度几乎没有损失，且极大地缩短了整合时间。对于这个方法，在训练完网络后，训练集前向传播，我们将ReLU过后的激活值保存下来，接着，权重根据最大的可能激活值缩放，所以这种情况下也只会同时发射一个脉冲。另外，这种方法也需要将最大的输入权重考虑进去（即它的缩放因子是激活值和权重值平衡之后的最大值），因为如果不考虑权重的话还是有可能出现在一个timestep内需要发射多次脉冲——此处的意思是可能存在多个最大值。
     
         然而这不是一个强有力的保证——保证在测试集上也可以维持这种性能表现，训练集应该代表测试集，同时结果显示了这种方法是很有效的。我们将这种方法称为"data-based normalization"。
     
       * **总结上面两个方法，第一个是只考虑最大值，第二个是考虑输入值与最大值**
     
       * ==和上一篇文章一样，依旧没有具体的实现，只有类似的伪代码==
     
     * **4 实验设置**
     
       * 4.1 数据集
         本文选的是MNIST数据集，之前脉冲神经网络实现手写数字分类的最高精度为 98.30%。
     
       * 4.2 结构
         两种网络结构，一个是四层的全连接网络，一个是卷积网络，测试集精度相差不大。
     
       * 4.3 脉冲输入
     
         先将MNIST图片的像素点都归一化到0-1，然后基于这些像素点，对于每个像素点利用泊松分布产生与像素值成正比的脉冲序列。
     
     * **5 结果**
     
       * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240123120357940.png" alt="image-20240123120357940" style="zoom: 50%;" />
     
         
       
       * ![image-20240123121219661](../../../AppData/Roaming/Typora/typora-user-images/image-20240123121219661.png)
         作为输入速率和发射阈值的函数，不同架构的分类性能和产生的脉冲数量。上图显示了ConvNets的结果，下图显示了FCN的结果。每个圆的颜色代表对MNIST测试集的平均准确性（5次试验的平均值），对每个输入示例使用0.5秒（500个时间步）的积分时间。圆的大小对应于整个网络每个示例呈现时产生的平均脉冲数量。右侧面板显示了标准化网络的相同数据，其中阈值在所有实验中都固定为1。导致测试错误率大于1.15（ConvNet）或2.2（FCN)的参数设置均未显示。
     
         通常，在增加阈值以在传播特征的检测之前整合更多尖峰与减小阈值以减少产生足够数量的尖峰所需的采样时间以最小化由于所传输消息的离散化而导致的误差之间存在权衡
     
         令人惊讶的是，全连接网络和卷积网络中生成的尖峰数量相当，尽管全连接网络使用的突触比卷积网络多60%。
     
       * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240123121513009.png" alt="image-20240123121513009" style="zoom:50%;" />
     
         
       
       * 使用SNN的原因之一是它们的可配置性。如果需要高精度，则高尖峰阈值有助于提高精度;如果短延迟很重要，则低发射阈值确保仅在几个输入尖峰之后响应。
     
   * 6 结论：作者研究了SNN中性能损失的典型来源，并提出了如何最好地解决这些问题的方法。
   
3. ## ==Conversion of continuous-valued deep networks to efficient event-driven networks for image classification==

   将连续值深度网络转换为高效的事件驱动网络进行图像分类     即 ann-snn 去做图像分类
   [博客链接](https://blog.csdn.net/h__ang/article/details/90609793)

* **期刊**：Frontiers in Neuroscience   3区 医学  IF = 4.3

* **摘要：** 17年的文章，当时卷积神经网络流行的cnn架构是Vgg-16,Inception-V3等，作者将这些网络模型转换成了Snn,并在MNIST、CIFAR-10和ImageNet数据集上取得了较好的结果。实验结果表明，在错误率仅增加几个百分点的情况下，SNN实现了计算次数2倍以上的减少，凸显出SNN的节能作用。

* **介绍：** 一个重点就是深度SNN可以在产生第一个输出脉冲之后就去查询结果，而ANN必须在所有层都处理完之后，最后一层产生结果。SNN适合处理来自基于事件的传感器的输入。多层脉冲神经网络已经在FPGA上实现了(2014年)。  为了减小深度ANN和SNN的差距，有必要开发深度SNN。成功的方法包括使用反向传播训练SNN，使用随机梯度下降来训练SNN分类器层，或者在训练期间修改ANN的传递函数，以便更好地将网络参数映射到SNN。此前有学者在AlexNet使用了最后一个方法，结果很不错，但是这种方法还没有被应用到VGG-16等大型网络上。 还有一种更直接的方式是使用预训练的ANN，并将其映射到SNN。  关于ANN-SNN的转换早在2013年就有人开始研究， 2015年有学者提出SNN输入输出关系与激活函数RELU之间的关系。 之后陆续有学者提出各种方法。==说到了前面的两篇文章==

  ==作者的见解：== 前面的这些方法在MNIST数据集上取得了很好的结果，但是当扩展到CIFAR-10数据集上时，SNN的准确率就低了。一个原因是ANN中的许多操作(最大池化层，softmax函数，batchnorm归一化)在SNN中是不存在的，因此作者说以前各类学者提出的方法都不通用。

  ==作者工作的贡献：== 解决了一些ANN-SNN转换的缺点。1 通过对脉冲神经元输出发射速率对等模拟激活值的近似的数学分析，我们能够推导出先前转换过程引入的误差的理论度量。基于这一新理论，我们提出了对脉冲神经元模型的修改，显著改善了深度SNN的性能。2 通过开发最大池化层、softmax激活、神经元偏置和批量归一化（Ioffe和Szegedy，2015）的脉冲实现，我们扩展了可以进行转换的CNN套件。特别地，我们首次演示了GoogLeNet Inception-V3可以被转换为等效准确的SNN。此外，我们还表明转换为脉冲网络与ANN网络压缩技术（如部分连接神经网络压缩）是协同的。  

* ==METHODS==  

  * Theory for Conversion of ANNs into  SNNs

    * ANN转换为SNN的基本原理是：脉冲神经元的发射速率应该与模拟神经元的激活值相匹配。==**这句话的意思：**脉冲神经元通常根据其发放的脉冲速率来进行建模，也就是单位时间内发放脉冲的次数。而模拟神经元（比如多数深度学习模型中的神经元）则使用一个连续的激活值（比如介于0到1之间的一个实数）来表示神经元的活跃程度。因此，这句话的含义是，当我们将模拟神经元替换为脉冲神经元时，脉冲神经元的发射速率应当能够与模拟神经元的激活值相匹配，以便在神经网络模型中能够正确地传递和处理信息。这也反映了神经元模型之间的转换和对应关系。==

    * ![image-20240126153624566](../../../AppData/Roaming/Typora/typora-user-images/image-20240126153624566.png)
      ![image-20240126153836111](../../../AppData/Roaming/Typora/typora-user-images/image-20240126153836111.png)

      

    * **2.1 Membrane Equation  膜方程**

      膜电位的重置有两种，一种是重置为0，另外一种就是减去阈值。不断对输入的z进行加和，当超过阈值时，就重置![image-20240122155824557](../../../AppData/Roaming/Typora/typora-user-images/image-20240122155824557.png)

      ![image-20240122170039585](../../../AppData/Roaming/Typora/typora-user-images/image-20240122170039585.png)

      从推导出的结果来看，第二种重置机制更有利于深层网络。（第一种引入了一个乘法误差项，且没有考虑膜电位超过阈值的部分，会导致信息的损失）

    * Firing Rates in Higher Layers   更高层的发射率
      ![image-20240122171117798](../../../AppData/Roaming/Typora/typora-user-images/image-20240122171117798.png)

      ![image-20240122171418395](../../../AppData/Roaming/Typora/typora-user-images/image-20240122171418395.png)

      
      
      上面两个公式就解释了误差项的累加问题，这也解释了为什么实现与人工神经网络激活值高相关性的发射率需要更长的时间，和为什么SNN发射率在高层会恶化的原因。

      ==细致的解释-gpt：==实现与人工神经网络激活值高相关性的发射率需要更长的时间是因为要在脉冲神经网络中逼真地模拟人工神经网络中的激活值，需要对膜电位的动态变化进行建模，并且需要考虑时间本身是离散的，而人工神经网络中的激活值是连续的。这就需要通过脉冲神经元的膜电位和随时间变化的发放脉冲来实现类似的激活值。因此，为了确保发射率与激活值高度相关，脉冲神经元需要更多的时间来处理和传递信息。
      关于SNN发射率在高层会恶化的问题，这可能与在网络的高层中信息的抽象和稀疏性有关。随着神经网络的层次结构越来越高，神经元的响应变得更加抽象和稀疏。这意味着在脉冲传播过程中信息的丢失可能会增加，这可能会导致在高层中发射率的退化。在这种情况下，神经元的发放脉冲可能无法准确地反映网络中的高级特征，从而导致与激活值的高相关性出现问题
      
    * **2.2 ANN操作的脉冲实现**
      在本节中，我们介绍一些新方法改善深度SNN的分类错误率，这些方法将会允许一个更宽范围ANN的转换，也会减少SNN的近似误差。

      * 2.2.1 Converting Biases   转换偏置

        * 以前的ANN到SNN的转换是没有考虑偏置的。在SNN中，偏置其实可以使用一个与偏置符号相同的常量输入电位来实现。

      * 2.2.2 Parameter Normalization 参数归一化
        近似误差的来源之一就是在模拟SNN时的时间步长，神经元的发射率被限制在一个区间 [0,$r_{max}$]  ，然而ANN通常不会有这样的约束。权重归一化是被等人引入作为一种避免由于太高或太低的发射率而造成的近似误差的方法。通过使用一种“data-based weight normalization mechanism”的方法展示了将ANN转换为SNN时性能上的提升，我们将这个方法扩展到带有偏置的神经元上，提出了一种方法使得归一化过程对异常值更加稳健。==这里提到的基于数据的归一化方法就是上一篇文章中说的==

        * Normalization with biases 带偏置的归一化
          定义$λ^q = max[a^l]$, $a^l$就是ANN通过RELU激活函数的值
          ![image-20240122173155195](../../../AppData/Roaming/Typora/typora-user-images/image-20240122173155195.png)

          data-based权重归一化机制是使用ANN中的ReLU单元的线性度，它可以简单地扩展到偏置，通过线性的缩放所有的偏置和权重以使得ANN的激活值a对于所有的训练样本都小于1。为了保留一个层内编码的信息，一个层的参数需要联合缩放。定义$λ^l = max[a^l]$, 如上图所示。

          这种权重归一化很好的控制了SNN中发射率饱和的问题，但可能会导致非常低的发射率，当原始的权重值很小的时候，这种做法的缺点明显在：如果存在很大的离群值，经会导致大部分归一化之后的权重值都很小。
          
        * Robust normalization  鲁棒归一化
          因为有上述问题，所以作者团队提出了一个更加稳健的替代方案——选择激活值中比例为p处对应的激活值，显然这样做会导致一些很大的激活值饱和——**所以选择归一化因子就是在饱和和不充分的发射率之间权衡**。

          通常来讲，一小部分神经元饱和导致的分类错误率比要比发射率过低这种情况小很多，这种方法也可以在训练期间和BN层相结合，对每一层进行归一化，因此会使得异常值少很多。
          
          ![image-20240126164441954](../../../AppData/Roaming/Typora/typora-user-images/image-20240126164441954.png)
          
          如图1A所示，图1A绘制了16666个CIFAR10样本的第一卷积层中所有非零激活的对数尺度分布。观察到的最大激活是第99.9百分位数的三倍以上。图1B显示了同一层中所有ANN单元的16666个样本中最高激活的分布，揭示了整个数据集的大方差，以及远离绝对最大值的峰值。这种分布解释了为什么通过最大值进行归一化可能导致SNN的分类性能可能较差。对于绝大多数输入样本，即使层内单元的最大激活也将远低于所选择的归一化尺度，导致层内的激发不足以驱动更高的层，并随后导致更差的分类结果。

      * 2.2.3 Conversion of Batch-Normalization Layers  BN层的转换
        ![image-20240122190346024](../../../AppData/Roaming/Typora/typora-user-images/image-20240122190346024.png)

      * 2.2.4 Analog Input to First Hidden Layer  第一个隐藏层的模拟输入
        ![image-20240126165044982](../../../AppData/Roaming/Typora/typora-user-images/image-20240126165044982.png)
        
      * 2.2.5 Spiking Softmax
        softmax通常用在深度ANN的输出，它会有一个归一化的效果，并且使得输出像是类别概率。之前的ANN-to-SNN转换的方法不会转换softmax层，而是通过在仿真期间神经元的==脉冲发射率==来确定预测的输出类别的，但是这个方法当最后一层接受的都是负输入时会失败，因为没有脉冲发射。

        * 第一种输出脉冲是由固定发射率的额外的泊松生成器触发的，脉冲神经元自己不发射，只是简单的累加它们的输入，脉冲是否发射由额外的生成器产生。
        * 第二种脉冲softmax函数是类似的，但是不依赖额外的时钟，为了确定神经元是否发射脉冲，我们需要计算脉冲电位的softmax输出，使得输出值在[0,1]之间，这两种方法的最终分类结果都是根据仿真时间内发射率最高神经元索引决定的。我们倾向于选择第二种方法，因为这种方法仅依赖于一个超参数。
        * 第三种方法是基于我们只需要根据softmax层的==输入膜电位==就可以确定最终的分类结果，这个简化可以加速推断时间，也可以通过减少随机性来提高精度。

      * 2.2.6. Spiking Max-Pooling Layers   脉冲最大池化层
        在这里，我们提出了一个简单的转换max-pooling机制，输出单元包含一个门控功能，它只会让==最大值发射率==神经元通过，而丢弃其他神经元，门控功能通过计算前突触发射率的估计值来控制。

    * 2.3 **Counting Operations**  计算量的统计

      * ![image-20240126165846393](../../../AppData/Roaming/Typora/typora-user-images/image-20240126165846393.png)
        
      
      ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190530171136419.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hfX2FuZw==,size_16,color_FFFFFF,t_70)
      
      这里的时间步T其实还是需要再实际中测试的，不一定说T越大越好。

  * ==3 Results==
    两种方法来改善转换为SNN后的分类错误率：

    1. 在转换之前训练一个更好的ANN
    2. 通过消除SNN的近似误差改善转换。

  * ==4 Discussion==
    这项工作提出了两个新的发展。第一个是一个新的理论，描述了近似的SNN发射率，其等效的人工神经网络激活ReLU。第二个是将几乎任意连续值的CNN转换为脉冲等价物的技术。

    

* ## 4 ==Spiking deep residual network==  

   脉冲深度残差网络  这篇文章提到了前面3篇文章

   * **期刊**：IEEE Transactions on Neural Networks and Learning Systems 1区top 计算机科学 IF=10.4 2021年11月1号见刊，但是2018年4月28日就已经在arXiv上发布。  Hu Yangfan  浙江大学

   * **摘要：**对于SNN来说，训练深层SNN仍然是一个巨大的挑战，而ResNet是一个先进的卷积神经网络可以训练深层的神经网络。所以就产生了将卷积的深度残差网络转换到脉冲版本的念头。将一个被训练的ResNet变成一个脉冲神经元网络——S-ResNet。作者提出了一个捷径转换模型，适当地缩放连续值激活以匹配SNN中的发射速率 ；和一个补偿的机制，去减少因为离散化造成的误差。

   * **1 介绍：** 目前，如何训练SNN仍然是一个开放性挑战，原因是其脉冲机制的不连续性。已经付出了许多努力来解决这个问题。这些方法可以分为四类。

     1. 第一类旨在通过一些近似方法使SNN可导，并应用梯度下降。
     2. 第二类从生物神经元汲取灵感，利用突触可塑性规则。例如stdp学习规则。
     3. 第三类将SNN视为随机过程，并使用概率推理进行学习。

        ==然而，这三种方法尚未得到充分发展，无法处理深层架构。==

     4. 最后一种方法采用转换思想，缩小SNN和ANN之间性能差距。它训练常规的ANN并构建转换算法将权重映射到等效的SNN。尽管这种方法可以构建比前三种方法更深的SNN，但构建大规模SNN仍然是一个巨大挑战，因为在网络架构变得更深（例如超过30层）后，转换后的性能将迅速下降。

     ​     在本文中，我们研究了通过转换深度残差网络[8]来学习深度SNN的方法。残差网络是一种尖端的CNN架构，允许网络变得极其深层，并且在许多应用中取得了巨大成功。先前的转换方法并不适用于残差网络，因为残差结构和传统线性结构之间存在结构差异。我们设计了一种快捷转换模型，以共同规范脉冲残差网络中的突触权重。此外，我们发现积累的传播误差对于阻碍大规模网络的无损转换是至关重要的，并开发了一种有效的方法来通过==轻微饱和化神经元的发射速率和迫使更深层的神经元更快地作出响应==来补偿传播误差。我们的方法在MNIST、CIFAR-10、CIFAR-100和ImageNet上取得了最先进的性能。

   * **2 相关工作**
     本文的相关工作总结了四个方向上各个学者所作出的努力，所以其实了解每一个方向，跟着看对应的参考文献就够了。

     * **第一个方向：**基于梯度下降的算法。这种方法旨在克服SNN中的非线性问题，并应用梯度下降优化算法。早期的工作可以追溯到Spikeprop[10]，它在SNN中实现了反向传播，假设势函数在激发时间周围的一个小区域内是可微的。以类似于反向传播的方式，tempotron学习规则[11]是从由膜电位和脉冲定时定义的成本函数导出的。最近，在[12，13，14，15，16，17，18]中展示了使用随机梯度下降（SGD）的SNN直接训练。

     * **第二个方向：**突触可塑性规律的学习。这种方法利用突触可塑性规则，如基于脉冲时序依赖性的突触塑性规则（STDP）[19]。STDP以其丰富的神经生理学证据，引起了众多研究者的兴趣。在[20，21]中，STDP已被证明能够以无监督的方式选择视觉特征。在[22]中，采用STDP规则、侧抑制和稳态，显示了随时间推移的稳定学习。在[23]中，提出了事件驱动的连续STDP来识别来自时序编码脉冲序列的模式。Rathi等人[24]利用修剪的SNN和自学习STDP学习规则展示了能耗和面积的高效性。

     * **第三个方法：** 统计算法。尽管大多数算法将SNN视为确定性系统，但证据表明大脑中的神经网络类似于随机系统[25]。在[26]中，噪音和不确定性被视为促进SNN中统计学习和自组织的有益因素。在[27]中，已经显示SNN能够执行贝叶斯推断，与贝叶斯推断在认知行为中普遍存在的理论相吻合。在[28]中，提出了一个具有随机突触的网络模型，用于蒙特卡罗抽样和无监督学习。

     * **第四个方法：** 将经过训练的ANN转换为其等效的SNN的研究始于Perez-Carrasco等人[29]，他们提出了一种方法，通过按照Leaky Integrate-and-Fire（LIF）[30]脉冲神经元的参数来缩放其CNN对应权重以获得SNN的权重。

     * 第二种  转换算法建立了ANN和SNN之间的映射，通过调整激活函数或人工神经来逼近脉冲神经元的平均发射频率。这一类方法包括使用Siegert神经元进行训练和映射[31]、使用SoftLIF函数[32]、使用噪声软加函数[33]、以及使用高斯分布的互补累积分布函数[34]。这些方法需要使用较不常见的激活函数进行训练，并且与最先进的ANN架构不兼容。

     * 第三种转换算法利用ReLU激活的非负性来逼近平均发射频率。此方法最早由Cao等人在[35]中提出。随后的工作引入了基于数据的归一化[36]和动态阈值平衡[37]等方法，以提高转换后的性能。
     
     * 在[38, 39]中，介绍了常见ANN操作的脉冲版本，用于转换现代ANN架构和预训练的ANN模型。Esser等人[40]利用TrueNorth芯片进行了硬件实现演示。
     
     ==当前困难：== 构建更深层次的SNN以获得更高的精度仍然是具有挑战性的。
     
   * **3 Building Spiking ResNet**   构建脉冲残差网络
   
     * 3.1 Spiking ResNet: An Overview   S-Resnet概述
       在ANN中，神经元接收来自前一层神经元的实值输入，而SNN中的神经元接收到来自前一层神经元的脉冲序列（一系列脉冲）作为输入。此外，在ANN中，神经元通过对加和后的输入应用激活函数来处理信息，而在SNN中，神经元通过整合传入的脉冲来处理信息。每个传入的脉冲都会引起神经元的突触后电位 (PSP) 发生变化。当PSP达到一定阈值时，神经元会发出脉冲并将信息传递给其下一层神经元。在将ANN中的权重映射到SNN之前，我们必须注意，由于我们在训练中使用了ReLU函数，因此ANN的激活可以是任何正实数。同时，脉冲神经元的发射速率是固定在[0，rmax]区域内的，其中rmax是生物神经元的最大发射速率，因为生物神经元在短暂的时间间隔（单个时间步）内不会发出多个脉冲。为了简化，我们假设神经元可以发射尽可能多的脉冲，即每个时间步发出一个脉冲。为了使ANN中的激活值匹配SNN中的发射速率（都在[0，1]区间内），需要共同对ANN中的权重和偏置进行归一化处理。
       ![image-20240123165612893](../../../AppData/Roaming/Typora/typora-user-images/image-20240123165612893.png)
   
       上图为S-Resnet网络形成图。ReLU激活函数由IF神经元替代。
       
       转换过程中去掉sum是因为IF神经元已经默认实现了求和操作。
       
       卷积层被类似卷积运算的突触连接层取代。卷积层的权重映射到相应的突触层，而偏置则转换为注入到脉冲神经元的恒定电流 [39]。同样，池化层和全连接层也被类似这些操作的突触所取代。对于平均池化，突触权重固定为(1/PoolSize)的平方。对于最大池化，我们使用逻辑比较来仅选择来自具有最高发射速率的神经元的脉冲，并通过将突触权重设为零来抑制其他输入脉冲。在池化层或全连接层之后，增加了一个额外的IF神经元层来集成来自这些类型突触的脉冲。由于在转换之前训练已经完成，因此批归一化直接应用于在批归一化层之前的卷积层。
       
     * 3.2 Conversion Model of Residual Network  残差网络的转换模型
       在这个小结中，作者主要说明了残差连接的过程中，短连接也必须进行归一化，否则会出现于原始ANN特征图不匹配的情况，从而导致精度下降。
       ![image-20240123170714002](../../../AppData/Roaming/Typora/typora-user-images/image-20240123170714002.png)
   
       ![image-20240123173941888](../../../AppData/Roaming/Typora/typora-user-images/image-20240123173941888.png)
   
       ![image-20240123174020238](../../../AppData/Roaming/Typora/typora-user-images/image-20240123174020238.png)
   
       短连接层应该使用这个归一化因子去对权重进行变化。
   
     * 3.3 Compensation of Propagation Error  传播误差的补偿
       这篇论文讨论的转换方法是基于人工神经元与脉冲神经元之间的一对一对应关系：ReLU的归一化激活被脉冲神经元的发射速率近似。
   
     * ![image-20240126210658303](../../../AppData/Roaming/Typora/typora-user-images/image-20240126210658303.png)
       
     * ![image-20240126210842773](../../../AppData/Roaming/Typora/typora-user-images/image-20240126210842773.png)
       
     * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240126211037184.png" alt="image-20240126211037184" style="zoom: 67%;" />
       
     * ![image-20240126211110202](../../../AppData/Roaming/Typora/typora-user-images/image-20240126211110202.png)
       
     * 最终作者通过上述模拟误差的操作，进行了拟合，给出了误差的计算公式，但显然，这是受限制的，仅对当前对比的ANN-SNN成立。
       
     * ![image-20240126211318261](../../../AppData/Roaming/Typora/typora-user-images/image-20240126211318261.png)
       
       然后考虑到误差传播公式可能对于其他转换来说都是一个指数模型，这在实际应用中过于复杂，==最终为避免指数操作，使用了一个权重更新规则来减少参数量，而不是调整每层的权重。==
   
   * **4.实验**
   
     * 作者在四个数据集上进行了实验：MNIST，CIFAR10，CIFAR100，ImageNet 2012。
     * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240126212126011.png" alt="image-20240126212126011" style="zoom:67%;" />
     * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240126212334505.png" alt="image-20240126212334505" style="zoom:67%;" />
     * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240126212442979.png" alt="image-20240126212442979" style="zoom:67%;" />
     * 作者还进行了能耗分析。参考每个平台的能源效率，ResNet的功耗是所有深度的S-ResNet的9倍以上。这一结果向我们展示了基于SNN的神经形态系统的光明前景，因为用神经形态硬件实现的SNN的估计功耗在一定程度上击败了用顶级节能FPGA平台实现的ANN。==这个对比== 就是说Resnet使用更节能，更高级的硬件设备， 而S-ResNet使用低端设备，但是两者准确率相差不大。
   
     ==值得注意的是：== 在实际应用过程中，比如医学信号的真实分类场景，使用这两个网络进行分类，是否Resnet准确率非常好，而S-ResNet却表现不行，因为毕竟现在都是在用训练集和测试集，没有考虑到真正实际应用中的准确率。但S-ResNet确实非常节能，这一点毋庸置疑。
     
     ==代码实现：== 目前github上应该有好多的代码，后续尝试复现并改成自己的。
   
   
   
* ## 5  ==Training spiking deep networks for neuromorphic hardware==  
  
   训练神经形态硬件的脉冲深度网络
   
   * **期刊**：aixiv 预印本   2016年11月16提交的手稿   
   
   * **作者：** Eric Hunsberger 滑铁卢大学
   
   * **摘要：** 本文提出一种训练脉冲深度网络的方法，该方法可以使用LIF神经元进行操作，在五个数据集上取得了脉冲LIF网络的最新成果，包括ImageNet 2012数据集。我们将深度ANN转换为SNN的方法具有可伸缩性，并适用于广泛的神经非线性--==大脑内部神经元的关系肯定是非线性的==。我们通过软化神经响应函数来实现这些结果，以使其导数保持有界，并通过在训练网络时添加噪声，增强对脉冲引入的变异性的鲁棒性。我们的分析显示，这些网络在神经形态硬件上的实现效率将比传统硬件上等效的非脉冲网络高出数倍。
   
   * **方法：** 我们首先使用传统深度学习技术在静态图像上训练一个网络；我们称之为ANN。然后，我们获取ANN的参数（权重和偏置），并将它们用于连接脉冲神经元，形成脉冲神经网络（SNN）。一个核心挑战是==以什么样的方式训练ANN==，使其可以转换为脉冲网络，并且使得生成的SNN的分类误差最小化。
   
     * CNN
   
     * LIF神经元模型
   
     * 带有噪音的训练
       图2：LIF神经元（τRC=0.02，τref=0.004）的滤波脉冲序列与输入电流的变化。实线显示了滤波脉冲序列的平均值，'x'点显示了中位数，实线误差线显示了第25和第75百分位数，虚线误差线显示了最小值和最大值。脉冲序列经过带有τs=0.003秒的α-滤波器的滤波处理。
   
       <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240126214823395.png" alt="image-20240126214823395" style="zoom:67%;" />
   
     * 转换为脉冲网络
       脉冲网络中的参数（即权重和偏置）均与ANN中的相同。卷积运算也保持不变，因为卷积可以重写为前突神经元 $i$ 和后突神经元 $j$ 之间的简单连接权重 $w_{ij}$。 同样地，平均池化操作可以被写成一个简单的连接权重矩阵，而且这个矩阵可以与下一层的卷积权重矩阵相乘，从而获得神经元之间的直接连接权重。当从ANN迁移到SNN时，网络中唯一发生改变的组件是==神经元本身==。==最显著的变化是我们用LIF脉冲模型替换了 `soft LIF rate model`==———这里作者是否说错了？因为下面说的都是软LIF好呀，然后作者还改进了，同时在结果中作者呈现的是soft LIF———。我们去掉了训练中使用的加性高斯噪声。我们还为神经元添加了后突触滤波器，这取消了脉冲产生的高频变化的重要部分。
   
     * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240127100020294.png" alt="image-20240127100020294" style="zoom:67%;" /><img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240127100235175.png" alt="image-20240127100235175" style="zoom:67%;" />
   
       第一个公式是原始的硬阈值，第二个是改进后的软阈值，第三个是为了控制平滑量新更改的软阈值函数。
   
       ==显然，这里作者并没有详细阐述自己的过程，仅仅简单说了一下==
   
   * **结果：** 展示了本文网络与其他学者的对比，其他神经网络形态芯片的错误率对比，同时作者在CIFAR-10数据集上对比了增加不同的修改带来的准确率
     ![image-20240127095418494](../../../AppData/Roaming/Typora/typora-user-images/image-20240127095418494.png)
   
     * **能耗问题：**  
   
       * 首先考虑原始网络的计算效率：==公式给了，但是还是不太清楚具体如何计算，后续需要查看相关资料==
   
       * 关于SNN计算效率阐述的较为清楚。
   
       * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240127103125488.png" alt="image-20240127103125488" style="zoom:67%;" />
   
         **表4示出了呈现时间和突触时间常数的各种替代方案对许多数据集的网络的准确性和效率的影响。**
   
         与传统硬件相比，我们的网络在神经形态硬件上的估计效率。对于所有数据集，准确性和效率之间存在权衡，但我们发现许多配置在准确性方面牺牲很少的同时效率显著提高。τs是突触时间常数，c0是分类的开始时间，c1是分类的结束时间（即每个图像的总呈现时间）。
   
         表4显示，对于某些数据集（例如CIFAR-10和ILSVRC-2012），可以完全去除突触（τs = 0 ms），而不会牺牲太多准确性。有趣的是，MNIST网络的情况并非如此，它至少需要一些突触才能准确地发挥作用。我们怀疑这是因为MNIST网络的发射率比其他网络低得多（MNIST的平均发射率为9.67 Hz，CIFAR-10为148 Hz，ILSVRC-2012为93.3 Hz）。这种平均发射率的差异也是MNIST网络比其他网络效率高得多的原因。重要的是要调整分类时间，无论是在每个示例的总时间长度方面（c1），还是在分类开始时（c 0）。这些参数的最佳值非常依赖于网络，包括层数、放电率和突触时间常数。
   
         ![image-20240127105845478](../../../AppData/Roaming/Typora/typora-user-images/image-20240127105845478.png)
   
         分类时间对准确度的影响。各个迹线示出不同的开始分类时间（c0），并且x轴示出结束分类时间（c1）
   
         图3显示了分类时间如何影响各种网络的准确性。考虑到CIFAR-10网络在没有突触的情况下和有突触的情况下表现得几乎一样好，人们可能会质疑在训练过程中是否需要噪声。我们在没有噪声的情况下重新训练了CIFAR-10网络，并且在没有突触的情况下运行，但准确率无法达到18.06%。这表明噪音在训练中仍然是有益的。
   
         ==关于这个图的描述不是很清楚==，
   
     * **结论：**  概述了上面的工作并提出了未来可行的方向。
       本文的第一个主要贡献是证明最先进的脉冲深度网络可以使用LIF神经元进行训练，同时保持高水平的分类准确性。例如，我们描述了第一个能够在ImageNet上获得良好结果的大规模SNN。值得注意的是，所有其他最先进的方法都使用了整流并发放（IF）神经元[11,10,12]，这些方法对于深度卷积网络中常用的修正线性单元来说很容易适应。我们的研究表明，在从ANN转换为SNN时，准确性几乎没有下降。我们还研究了分类时间对准确性和能效的影响，并发现网络可以在准确性几乎不下降的情况下实现高效节能。通过使LIF响应函数变得平滑，使得其导数保持有界，我们能够使用标准的通过反向传播训练的卷积网络结合这种更复杂和非线性的神经元。我们的平滑方法适用于其他神经元类型，允许用于具有个性化神经元类型的神经形态硬件进行网络训练（例如[14]）。我们发现，对于我们使用的平滑量来说，从软响应函数转换为LIF神经元的硬响应函数引入的误差非常小。然而，对于那些具有需要更多平滑的严重不连续性的神经元来说，可能需要在训练过程中缓慢进行平滑，这样，在训练结束时，平滑响应函数可以任意接近硬响应函数。
   
     * 本文的第二个主要贡献在于证明，在神经元输出中添加噪音可以降低过渡到脉冲神经元时引入的误差。尽管没有噪音的ANN表现更好，但CIFAR-10网络的整体误差降低了0.6%。这是因为神经元输出的噪音模拟了脉冲网络在过滤脉冲序列时遇到的变异性。在使用太少噪音进行训练和使用过多噪音进行训练之间存在权衡，过少噪音会使SNN的准确性下降，而过多噪音则会使最初训练的ANN的准确性下降。 ==这种增加噪音训练的方法个人感觉行不通，因为正常来说数据中包含的噪音不可能是这么规律的噪音，所以这种方法的现实意义显然不大==
   
     
   
* ## 6. 


7. 

8. ## ==测试==

   

   * **期刊：**
   * **作者及单位：**
   * **摘要：**
   * **介绍：**
   * **方法：**
   * **试验：**
   * **讨论与总结：**

9. 



# ==SNN论文阅读==

1. ## ==Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks==
   [github 链接](https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron)

   添加可学习膜时间常数以增强脉冲神经网络的学习能力

   这篇文章和ANN to SNN 文章中的第6篇文章 作者是同一个
   [知乎链接](https://zhuanlan.zhihu.com/p/561445448)

* **期刊：**  axiv 预印本    2021.08.17发布  上一篇文章也是2021年发布在NeurIPS会议

* **作者及单位：**  Fang Wei 等人    北京大学计算机科学与技术系

* **摘要：** 
  大多数现有的学习方法仅学习权重，并且需要手动调整决定单个脉冲神经元动力学特性的膜相关参数。这些参数通常被选为所有神经元的相同值，这限制了神经元的多样性，因此限制了所得 SNN 的表现力。在本文中，我们从膜相关参数在大脑区域间不同的观察结果中汲取灵感，并提出一种不仅能够学习突触权重，还能学习 SNN 膜时间常数的训练算法。我们表明，结合可学习的膜时间常数可以使网络对初始值不那么敏感，并可以加速学习。

  讲述了本文做的工作，大概两点：

  - 让膜时间常数（ membrane time constant）成为可学习的参数，而不是预置的超参数。
  - 在SNN中使用最大池化，而不是通常认为的平均池化，最大池化具有更多优势。

* **介绍：**
  通常SNN的学习算法可以分为有监督，无监督，基于奖励的学习以及 ANN to SNN 的四个方法。
  <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129173711800.png" alt="image-20240129173711800" style="zoom:67%;" />

  * 使用PLIF（也就是带膜参数常量的LIF神经元）作为神经元，基于反向传播进行训练。这提高了模型的鲁棒性和学习速度。
  * 重新评估池化方法。证明最大池化性能不比平均池化差，还能降低计算成本（二进制位运算）、保留神经元放电的异步性（下文详细说明）。
  * 在一些数据集上检验本文的模型，效果很好。

* **相关工作：** 

  1. 无监督学习的SNN
     当前方法仅适用于浅层SNN。

  2. 基于奖励学习的SNN
     模仿人脑的学习方式，即利用多巴胺能、血清素能、胆碱能或肾上腺素给予神经元奖励或惩罚信号[15, 6, 45]。
     目前出现了诸如策略梯度[58, 30]、时序差分学习[52, 16]和 Q-learning[6]等强化学习方法，最近还提出了一些基于 STDP 的启发式现象学模型[17, 73]。
     ==方向：结合强化学习方向的进展，将其与最新的STDP学习规则相结合，是否能有更好的效果？== 

  3. ANN to SNN 的转换

     ==速率编码：==脉冲神经网络（SNN）中的速率编码是一种常用的信息编码方式，它基于神经元的发放速率（即脉冲的频率）来表示输入信息。在速率编码中，信息的数量和强度通常由脉冲的频率来编码，频率越高表示对应的信息强度越大。

     ANN2SNN 仅限于速率编码，从而失去了时序任务中的处理能力。
     据我们所知，ANN2SNN 仅适用于静态数据集，不适用于神经形态数据集。

  4. 监督学习的SNN
     SpikeProp [5] 是第一种基于反向传播的 SNN 监督学习方法，它使用线性逼近来克服 SNN 的非可微阈值触发放电机制。
     Zenke 等人 [74, 46] 系统地研究了替代梯度学习的显著鲁棒性，并表明通过替代梯度方法优化的 SNN 可以与人工神经网络实现具有竞争力的性能。与 ANN2SNN 相比，替代梯度方法对模拟时间步长没有限制，因为它不基于速率编码[64, 74]。

  5. 深层SNN的 脉冲神经元和层
     脉冲神经元和层模型在 SNN 中发挥着至关重要的作用。
     到目前为止还没有对  学习膜时间常数对 SNN 的影响进行系统研究
     Wu 等人 [64] 发现归一化层对于深层 SNN 也是至关重要的，并提出了神经元归一化 (NeuNorm)，以平衡每个神经元的放电率，避免严重的信息丢失。Ledinauskas等人 [36] 最早提出在深层 SNN 中使用批归一化 [27] 来实现更快的收敛。

* **方法：**
  在本节中，我们首先在 3.1 节中简要回顾了LIF模型，并在 3.2 节中分析了突触权重和膜时间常数的影响。然后在 3.3 – 3.5 节中介绍了参数化LIF模型和 SNN 的网络结构。最后，我们在 3.6 节和 3.7 节中描述了脉冲最大池化和 SNN 的学习算法。

  1. LIF模型

  2. Function comparison of synaptic weight and  membrane time constant    

     突触权重和膜时间常数的功能比较
     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129201158862.png" alt="image-20240129201158862" style="zoom:67%;" />

  3. Parametric Leaky Integrate-and-Fire model   参数LIF模型
     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129201457796.png" alt="image-20240129201457796" style="zoom:67%;" />

     ![image-20240129202329651](../../../AppData/Roaming/Typora/typora-user-images/image-20240129202329651.png)

     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129201844376.png" alt="image-20240129201844376" style="zoom:67%;" />

     ![image-20240129202219317](../../../AppData/Roaming/Typora/typora-user-images/image-20240129202219317.png)

     这个公式会导致数值的不稳定，因为$τ$ 是分母， 只有当公式1中的$dt$ 小于 $τ$ 时才是一个有效的离散近似，即$τ$ 大于1，为了避免不稳定问题，下面给了公式6———————— ==没有明白这里的近似是啥意思。==

     ![image-20240129202929919](../../../AppData/Roaming/Typora/typora-user-images/image-20240129202929919.png)

     ![image-20240129211648493](../../../AppData/Roaming/Typora/typora-user-images/image-20240129211648493.png)

     

  4. RNN-like Expression of LIF and PLIF    LIF 和 PLIF 的 RNN式表达  从RNN的角度解释LIF和PLIF
     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129202043885.png" alt="image-20240129202043885" style="zoom:67%;" />

     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129203420923.png" alt="image-20240129203420923" style="zoom:67%;" />

     ![image-20240129211922912](../../../AppData/Roaming/Typora/typora-user-images/image-20240129211922912.png)

     

  5. Network Formulation    网络的公式化
     我们在本文中提出了构建 SNN 的一般公式，如图 4 所示。SNN 包括脉冲编码器网络和分类器网络。脉冲编码器网络由 N个下采样模块组成，每个模块包含 N个 重复的 {Conv2dSpiking Neurons 和一个池化层。脉冲编码器可以从输入中提取特征，并将它们转换为不同时间步长的放电脉冲。分类器网络由 N个重复的 {FC-Spiking Neurons} 组成。此处 Conv2d 表示二维卷积层，FC 表示全连接层。许多以前的作品 [12, 37, 60, 75, 8, 21] 使用==泊松编码器==将图像转换为脉冲作为输入，而 [56] 表明这种编码会给网络的放电引入可变性并损害其性能。类似于 [56, 64, 53]，==输入直接馈送到我们的网络，而没有首先转换为脉冲。==在这种情况下，图像脉冲编码由第一个 {Conv2d-Spiking Neurons} 模块完成，该模块可以看作是可学习的编码器。请注意，突触连接（包括卷积层和全连接层）是无状态的，而脉冲神经元层在时间域中具有自连接，如图 4 所示的展开网络公式。所有参数在所有时间步长上都是共享的。

     ![image-20240129212055894](../../../AppData/Roaming/Typora/typora-user-images/image-20240129212055894.png)

     

  6. Spike Max-Pooling    脉冲最大池化层

     先前研究都是用平均池化，认为最大池化会丢失信息。在这篇论文证明了不会丢失信息，且可以减小计算成本。与平均池化窗口中将所有神经元等量地传输信息给下一层的情况不同，在最大池化窗口中只有发射脉冲的神经元能够将信息传输到下一层。因此，最大池化层引入了胜者通吃机制，允许发射脉冲的神经元与下一层通信，并忽略池化窗口中的其他神经元。最大池化层的另一个吸引人的特点是动态调节连接。
     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129205141254.png" alt="image-20240129205141254" style="zoom:67%;" />

     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129212846299.png" alt="image-20240129212846299" style="zoom:67%;" />

  7. Training Framework  训练框架
     损失函数 MSE ,  其余一些求导过程不太能看懂。

     

* **试验：** 
  使用PLIF神经元和脉冲最大池化进行分类任务的SNN性能，包括传统的静态MNIST、Fashion-MNIST、CIFAR-10数据集以及神经形态学N-MNIST、CIFAR10-DVS、DVS128手势数据集。

  1. Network Structure 网络框架
     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129214107639.png" alt="image-20240129214107639" style="zoom:67%;" />

     投票层是通过设置kernel大小为M、stride为M的平均池化来实现的。我们对所有数据集设置了M=10。我们使用平均池化来实现民主投票，使得少数服从多数。使用最大池化进行投票可能导致专制主义，因为少数将不参与计算图（见图5），而使用M个神经元来代表一个类将退化为使用一个神经元。

     

  2. Comparison with the State-of-the-Art     与现有技术的比较
     准确率提高，且推理速度加快了许多。
     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129215831702.png" alt="image-20240129215831702" style="zoom:67%;" />

* **讨论与总结：**
  在这项工作中，我们提出了参数泄露整合放电（PLIF）神经元，将可学习的膜时间参数纳入SNN中。我们展示了使用PLIF神经元的SNN在静态和神经形态数据集上都优于现有的比较方法。此外，我们还展示了使用PLIF神经元构建的SNN对初始值更加鲁棒，并且学习速度比由LIF神经元组成的SNN更快。我们还重新评估了SNN中最大池化和平均池化的性能，并发现先前的工作低估了最大池化的性能。我们建议在SNN中使用最大池化，因为它具有较低的计算成本、更高的时间拟合能力，并且具有接收脉冲和输出脉冲而非浮点值的特性，而前者则适用于平均池化。
  <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129220430797.png" alt="image-20240129220430797" style="zoom:67%;" />


2. # ==测试==

* **期刊：**
* **作者及单位：**
* **摘要：**
* **介绍：**
* **方法：**
* **试验：**
* **讨论与总结：**



# ==Surrogate gradient==  替代梯度

1. ## ==Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks==

   通过反向传播进行深度脉冲神经网络的时序(脉冲序列)学习
   [参考链接](https://zhuanlan.zhihu.com/p/667158856)

* **期刊：** NIPS 2020 会议论文 

* **作者及单位：** 加州大学  zhang wenrui 

* **摘要：** 脉冲神经网络适合时空学习，且能效高。现有的SNN error BP 性能低下，需要较长的时间步才可以实现较好的性能，无法将深度进行扩展。作者提出了一种新的 时序脉冲学习 反向传播 算法  TSSL-BP 

  （temporal spike sequence learning backpropagation） , 该方法是将error BP 分为神经元间和神经元内两类依赖关系。  考虑脉冲发放活动的全有或全无特性，通过突触前的发放时间来捕获神经元间的依赖性； 通过处理每个神经元状态在时间上的内部演化，捕获神经元内的依赖性。    该方法大大缩短了时间窗口，有效的训练了深度SNN，同时提高了图像分类数据集的准确性。

* **1.介绍：**  该部分相当于摘要的扩展版。可以不用看

* **2.反向传播：** 

  * 2.1  现有的SNN反向传播算法
    最早的算法有 SpikeProp，但是局限于每个输出神经元只有一个脉冲，在实际任务中表现不好；ANN-SNN的转换会导致近似误差，且无法利用SNN的时序学习能力；在发放率编码损失函数下使用BP训练SNN可以实现较好的性能；通过时间反向传播来捕捉时间效应，使用替代梯度来近似脉冲过程来解决脉冲时间的不可分问题；一种基于门函数和阈值触发突触模型的递归SNN的BP方法；通过捕捉聚集在脉冲序列水平的神经元效应，提出了脉冲序列水平BP方法；添加神经元归一化和群体解码等优化技术；==作者提出了== TSSL-BP方法，适用于多个脉冲编码形式，可以捕捉时间依赖性，只需5个时间步即可完成训练，同时可获得极高的精度。
  * 2.2 脉冲神经元模型
    论文使用了LIF神经元模型和突触模型

* **3.方法：**  

  * 3.1 前向传播
    $a^{l-1}[t] = (\varepsilon * s^{l-1})[t]$   无权重的突触后模电压PSC
    $u^{l}[t] = (1-\frac{1}{\tau_m})u^{l}[t-1] + W^{(l)}a^{l-1}+(v*s^{(l)})[t]$   膜电压值 
    $s^{l}[t] = H(u^{l}[t] - V_{th})$  脉冲序列   ==上面三个公式统一定义为 公式(6)==   
    $V_{th}$ 是脉冲发放阈值，$H(·)$ 是Heaviside step function，x大于0，H(X)等于1，x小于0，H(X)等于0，x等于0时，H(X)等于1即可

    其中，$(\varepsilon * s^{l-1})[t] =  \sum_{i=0}^n \varepsilon[i]s^{l}[t-i]$ , 逐元素的卷积操作，形成的是一个矩阵向量的形式

    从上面公式中看的话, $u^{l}[t]$ 和 $s^{l}[t]$ 互相需要对方当前的值才可以得到。在==代码实现==上， $u^{l}[t]$ 只需要前两项，接着根据阈值生成脉冲，有脉冲就会进行归零，这和第三项的作用是一致的。

    对应的计算图如下，其中的$m^{l}[t]$ 暂时不清楚含义，推测应该是电压的记忆值

    <img src="D:/software/wechat_data/WeChat Files/wxid_v8vzx3rzcwp622/FileStorage/Temp/bc874316d8f5a97537e7312387232a3.png" alt="bc874316d8f5a97537e7312387232a3" style="zoom:50%;" />

    

    3.2 **损失函数的定义：**  
    $Loss = \sum_{k=0}^{N_t} E[t_k] = \sum_{k=0}^{N_t} \frac{1}{2} ((\varepsilon * d )[t_k] - (\varepsilon * s)[t_k])^2 $

    $E[t]$ 是时间t处的误差，d是脉冲输出序列，s是实际脉冲序列，两者的差距即为损失

    3.3 **TSSL-BP方法：**

    ![image-20240224111420389](../../../AppData/Roaming/Typora/typora-user-images/image-20240224111420389.png)

    上述公式的推导如下：
    <img src="D:/software/wechat_data/WeChat Files/wxid_v8vzx3rzcwp622/FileStorage/Temp/39ba5df1939b6c5eb6938f778080fa7.jpg" alt="39ba5df1939b6c5eb6938f778080fa7" style="zoom: 25%;" />

    ![c1a5f810f96ecc5dbda7fd613c7c335](D:/software/wechat_data/WeChat Files/wxid_v8vzx3rzcwp622/FileStorage/Temp/c1a5f810f96ecc5dbda7fd613c7c335.jpg)

    当 $l$ 是输出层时，公式的第一部分可以直接从损失函数中获得，第二部分需要计算
    ![image-20240225151509391](../../../AppData/Roaming/Typora/typora-user-images/image-20240225151509391.png)

    当 $l$ 是隐藏层时，通过反向链式法则进行构造，最后依然剩下这一项需要单独求解。

    $\frac{\partial \kern 2pt a^{l}[t_k]}{\partial \kern 2pt u^{l}[t_m]}$  

    

    * 3.3.1 SNN BP的核心挑战
      ![image-20240225093042153](../../../AppData/Roaming/Typora/typora-user-images/image-20240225093042153.png)

      将脉冲的累计过程近似成一条平滑的曲线，实际上是将一个实际的脉冲转换为多个虚构的脉冲(一个离散的值被扩展成多个连续值)，对突触后膜电位的产生造成影响，进而对脉冲发放时间产生影响。
      
    * 3.3.2 TSSL-BP 背后的思想
      先前在SNN中使用BP的两个主要局限性：缺乏对脉冲不连续的适当处理（导致时间精度的损失），以及 需要多个时间步才可以得到较高的准确率（多个时间步带来了较高的延迟）。
    
      
    
      <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240225101226685.png" alt="image-20240225101226685" style="zoom:67%;" />
    
      ![image-20240225101318009](../../../AppData/Roaming/Typora/typora-user-images/image-20240225101318009.png)
    
    * 3.3.3  神经元间的反向传播
      ![image-20240225103358510](../../../AppData/Roaming/Typora/typora-user-images/image-20240225103358510.png)
    
      突触前膜电位的变化引起了突触后波形的变化。
    
      <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240225104212278.png" alt="image-20240225104212278" style="zoom: 67%;" />
    
      当$t_k < t_m$时，或者在$t_m$处没有产生脉冲，那此时inter-neuron dependencies  $\phi_i^{(l)<1>}(t_k,t_m) = 0 $ ，否则，他们的关系就可以写为（11）
      ![image-20240225104726503](../../../AppData/Roaming/Typora/typora-user-images/image-20240225104726503.png)
    
      ![image-20240225104813523](../../../AppData/Roaming/Typora/typora-user-images/image-20240225104813523.png)
    
      结合(2) ，可以将(11)中的部分变换为 (12)
      ![image-20240225104910354](../../../AppData/Roaming/Typora/typora-user-images/image-20240225104910354.png)
    
      ![image-20240225105028142](../../../AppData/Roaming/Typora/typora-user-images/image-20240225105028142.png)
    
      结合(4)，可以得出（12）的解。
    
      ![image-20240225105302890](../../../AppData/Roaming/Typora/typora-user-images/image-20240225105302890.png)
    
    * 3.3.4 Intra-Neuron Backpropagation 神经元内的反向传播
      在$t_m$ 处的膜电位不仅对突触后电流有贡献，还会影响$t_p$处的膜电位 $u_i^{(l)}[t_p]$，$PSC a_i^{(l)}[t_k]$ 对 $u_i^{(l)}[t_p]$ 存在神经元间依赖性（上面说的），而 $u_i^{(l)}[t_p]$ 由于在$t_m$处的突触前电位重置而进一步受到$t_m$的影响，进而也就形成了一个间接影响。  
    
      概括为 ： $u_i^{(l)}[t_m]$  进行重置影响了 $u_i^{(l)}[t_p]$，而$u_i^{(l)}[t_p]$  会对后续脉冲的PSC产生影响。
      ![image-20240225114045108](../../../AppData/Roaming/Typora/typora-user-images/image-20240225114045108.png)
    
      结合公式(4)可以得到(13)中的变体。 $t_p$处的膜电位是$t_m$处的脉冲序列进行重置后再$t_p$时间点的值。
      最后的一个等式结果可以分为三项，反向传播过程中，在m处计算导数时，该项已经求出；第二项和第三项依据（12）中的求解过程进行即可。
    
      ![image-20240225142819775](../../../AppData/Roaming/Typora/typora-user-images/image-20240225142819775.png)
    
      从上往下分析，第一个等于0是显然的，因为脉冲均为0
      第二项当 $t_p$ 处的脉冲为0，该项简化为（11）
      当两者均不为0，该项变为 （11） + （13）的两个误差，也就是文中一直强调的 神经元内和神经元间的误差。
      ==按照作者所说的，膜电位对突触后电位的影响在计算误差时最多使用相邻三个脉冲来进行计算，是否存在弊端？==
  
* **实验部分：**

* **讨论与总结：**



2. # ==测试==

* **期刊：**
* **作者及单位：**
* **摘要：**
* **介绍：**
* **方法：**
* **试验：**
* **讨论与总结：**







# 十二

# ==1==



* **摘要：**  
* **介绍：**
* **方法：**
* **试验：**
* **讨论与总结：**
