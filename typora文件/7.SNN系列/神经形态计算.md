## 8.12 上午

#### three levels of brain science

1. Computational theory    回答的是大脑做什么事情。   
2. Representation and Algorithm  大脑如何表征信息，使用什么算法。
3. Implementation   硬件上的实现，通过突触和环路如何加工信息。

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240821190611164.png" alt="image-20240821190611164" style="zoom: 80%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240821201715167.png" alt="image-20240821201715167" style="zoom:80%;" />

这个问题是否和神经科学相关？
这个在计算上是有趣的问题吗？
这个问题能够用现有的技术解决吗？
这个问题能够成为一篇论文吗？



#### 神经计算建模  bottom-up method   and  top-down approach

![image-20240821212647825](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240821212647825.png)

​	两个例子，表示的内容主要是结合空间和时间，可以更好的模拟大脑。



#### 工具的开发 Brainpy

已有工具不满足。之前的模型不能训练。
brainpy的特点：efficiency，integration，flexibility， extensibility

#### 神经计算领域的前景

大脑看的基本都是动态的信息

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240821223643909.png" alt="image-20240821223643909" style="zoom: 50%;" />



## 8.12下午

主要讲python编程基础，以及brainpy编程基础。


## 8.13 上午

Hodgkin-Husxley（HH） model  

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240829102410359.png" alt="image-20240829102410359" style="zoom:50%;" />

![image-20240829105634853](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240829105634853.png)

![image-20240829110140628](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240829110140628.png)

## 8.13下午

7

## 8.14上午

![image-20240830161755989](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240830161755989.png)

LIF模型的缺点和优点：
	计算简便，直观，易于构建网络，发放率与实际较为相似，较好的拟合了阈下膜电位。 
	工作电位的形式是过于简单，没有脉冲的历史记忆性，不能再现多样的发放模式。

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240830164340220.png" alt="image-20240830164340220" style="zoom:50%;" />

![image-20240830164741677](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240830164741677.png)

![image-20240830165034745](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240830165034745.png)

![image-20240902170017026](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240902170017026.png)



<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240902171253489.png" alt="image-20240902171253489" style="zoom:50%;" />



![image-20240903151718444](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240903151718444.png)

![image-20240903151917728](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240903151917728.png)

![image-20240903152339211](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240903152339211.png)

0,1,2,3就是膜电位的变化，但同时受制于w

![image-20240903152739189](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240903152739189.png)



<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240903154950660.png" alt="image-20240903154950660" style="zoom:50%;" />

分叉分析

![image-20240903160531092](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240903160531092.png)

从没有交点到有某些交点，会存在动力学性质的改变，---分叉分析

w到底是指的什么： 离子通道的性质？  
w越大，膜电压增长的越慢。

经过稳定点慢的原因是 dV/dt = 0 and dw/dt=0



## 8.14 下午

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240903161436239.png" alt="image-20240903161436239" style="zoom:50%;" />

![image-20240903161936132](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240903161936132.png)


使用brainpy做相平面分析。

![image-20240903204309223](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240903204309223.png)



# 8.15 上午

* 计算神经科学：定量研究大脑信息处理

* 如何有效提取信息，如何发现科学原理？

* brain inpsired artificial  intelligence

* 脑科学———计算神经科学———人工智能

* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240903211020963.png" alt="image-20240903211020963" style="zoom: 67%;" />

* 形态多样性和生物物理多样性  ————> 电生理行为多样性

* 树突计算：电脉冲回传激活的高原，异或逻辑门

* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240903212216382.png" alt="image-20240903212216382" style="zoom:67%;" />

* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240903212428356.png" alt="image-20240903212428356" style="zoom:67%;" />

* 关于树突整合的数学建模

* 关于树突整合法则的理解

* 突触输入强度较小

  

* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904081646258.png" alt="image-20240904081646258" style="zoom: 67%;" />

* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904083318339.png" alt="image-20240904083318339" style="zoom:67%;" />

* 传统的树突计算是简单的将兴奋性和抑制性相加，但是忽略了整合电流，然后该课题组做了这个工作，发现被忽略的整合电流也符合欧姆定律。

* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904084517626.png" alt="image-20240904084517626" style="zoom:67%;" />

* 动力学决定着大脑神经元网络状态

* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904084927159.png" alt="image-20240904084927159" style="zoom:67%;" />

* 给树突出输入，因为传递到胞体的时候有衰减，所以如何在胞体出准确推断信号输入的大小比较难。

* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904092432226.png" alt="image-20240904092432226" style="zoom:67%;" />

* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904092831047.png" alt="image-20240904092831047" style="zoom:67%;" />



# 8.15 下午

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904100019513.png" alt="image-20240904100019513" style="zoom:67%;" />

* cable equation ： 理论走在实验前边，能够引申出很有意义的参数，在实验科学中具有很重要的意义
* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904100720595.png" alt="image-20240904100720595" style="zoom: 67%;" />
* ReLU只能反映胞体上的变化，却不能反映树突上膜电位的变化
* 单个神经元的计算能力与5-8层神经网络相当
* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904164243600.png" alt="image-20240904164243600" style="zoom:67%;" />
* 基于梯度的方法在神经网络中的学习效果是最好，而突触可塑性等偏生物原理的反而表现不好。
* 越生物的神经元越不适合做静态的任务。
* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904172300448.png" alt="image-20240904172300448" style="zoom: 50%;" />
* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904173018460.png" alt="image-20240904173018460" style="zoom: 50%;" />
* <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904173148610.png" alt="image-20240904173148610" style="zoom:50%;" />
* 好用的架构----简单？  复杂不能作为一个优点，  MLP里面换成 detailed 结构（biophysically  detailed  neural network）
* 



# 8.16 上午

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904174651936.png" alt="image-20240904174651936" style="zoom: 33%;" />



<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904185603402.png" alt="image-20240904185603402" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904191748857.png" alt="image-20240904191748857" style="zoom:50%;" />

这里COBA模型中，计算时利用t-1步的电压去计算t步的电流I，接着使用第t步的电流去得到第t步的电压

AlignPre 突触变量Align到突触前，适用于所有模型，                   适用于一对多的情况。
AlignPost突触变量Align到突触后，适用与Exponential model,   适用于多对一情况。     去做event-driven计算

==AlignPre==  此处不符合事件驱动的计算

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904200927634.png" alt="image-20240904200927634" style="zoom:50%;" />

==AlignPost==， 这里是spike与weight做计算，为事件驱动的形式，使得simulation更快

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904201020941.png" alt="image-20240904201020941" style="zoom:50%;" />

## 8.16 下午

Synaptic plasticity

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904201556469.png" alt="image-20240904201556469" style="zoom:33%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904204151602.png" alt="image-20240904204151602" style="zoom:50%;" />



<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904203155459.png" alt="image-20240904203155459" style="zoom: 50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904210041797.png" alt="image-20240904210041797" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904211154415.png" alt="image-20240904211154415" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904211252530.png" alt="image-20240904211252530" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904211504215.png" alt="image-20240904211504215" style="zoom:50%;" />

==Long-term Plasticity==

也许不存在统一的模型？ 突触前神经元发放时间 与 突触后神经元发放时间  ——不同脑区可能表现的截然相反。
一个突触连接还会受到其他突触的影响。

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240904212705600.png" alt="image-20240904212705600" style="zoom: 50%;" />



SNN领域的研究还是很少有关于突触模型去提高AI performance，直接应用应该很难做
STDP的应用？ 



## 8.19 上午

E-I balanced neural network
<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905091257358.png" alt="image-20240905091257358" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905092012054.png" alt="image-20240905092012054" style="zoom:50%;" />

​			<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905095627471.png" alt="image-20240905095627471" style="zoom:50%;" />

​		EI平衡是一种快速响应外部刺激的方式，预测也是。这里面的EI-balance 没有考虑delay的情况
EI平衡会放大噪声，当产生的噪声超过阈值，去发放脉冲。

 

## 8.19 下午		

==Decision-Making Network==   标准的计算神经科学问题

1. LIP-Decision-Making
2. A Spiking Model of DM
3. Results of Spiking DM
4. A Rate model of DM
5. Phase Plane Analysis

coherent  coherence  
每个点受到的刺激一样，但是却做出了不同的运动轨迹，这就是研究决策时一个非常好的实验范式。
排除外界输入的干扰，更多的关注网络本身是如何做出决策的。
<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905103540301.png" alt="image-20240905103540301" style="zoom:50%;" />

第一个是让猴子做出及时的判断，这样存在的问题是猴子做出的判断可能只是眼动而不是真正的决策。
第二个是让motion消失后，中间的点持续停留一段时间，接着让猴子做出判断。

==Drift diffusion model==

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905105014697.png" alt="image-20240905105014697" style="zoom: 33%;" />



<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905110209706.png" alt="image-20240905110209706" style="zoom: 50%;" />

SNN of DM（决策制定的脉冲神经网络）是一个模拟神经元如何通过尖峰活动处理信息并进行决策的计算模型。在这个网络中，通常包括以下几个关键方面：

1. **输入处理**：模型通常接收来自运动敏感区域（例如MT/MST）的输入，以帮助进行决策制定。

2. **神经动态**：网络中包含慢的反馈兴奋和抑制机制，这些动态机制增强了竞争输入之间的区分，从而形成明确的选择。

3. **赢家通吃机制**：这一机制确保了一个神经群体在活动中占据主导地位，同时抑制其他群体的活动，有助于实现清晰的决策。

4. **突触机制**：模型区分快的AMPA受体和较慢的NMDA受体，这对决策过程中的时间动态有重要影响。

总体而言，这个模型旨在模拟生物决策过程，改善我们对认知功能的理解，并在人工智能应用中具有潜在价值。

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905111004517.png" alt="image-20240905111004517" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905112905590.png" alt="image-20240905112905590" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905113307353.png" alt="image-20240905113307353" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905113722594.png" alt="image-20240905113722594" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905114401411.png" alt="image-20240905114401411" style="zoom:50%;" />



##  8.20上午

==Attractor Neural Network==   吸引子网络

1. 动力系统基础知识 	Dynamical system
2. Amari-Hopfield Network
3. 连续吸引子神经网络 Continuous Attractor Neural Network （CANN）
4. CANN的计算

定义一个动力系统：状态向量和函数。
phase plane and fixed point
一组连续的鞍点被称为线吸引子。 本身该点属于鞍点，经过一个扰动以后，到达的下一个点也是鞍点，所以称为线吸引子。

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905150126157.png" alt="image-20240905150126157" style="zoom:50%;" />

为什么需要吸引子？

1. 内部神经元的表示，对于小的改变，应该是稳定和鲁棒的；
2. 能够保存记忆（对于下游认知任务）

Hopfield 网络的能量函数是用于描述网络状态的一个重要概念。它用于量化网络的状态，并帮助我们理解网络是如何在学习和记忆的过程中进行能量最小化的。

Hopfield 网络的能量函数  $E$ 定义如下：

$$E = -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} w_{ij} s_i s_j - \sum_{i=1}^{N} \theta_i s_i$$

其中：

- N是神经元的数量；
- $s_i$是第i个神经元的状态，通常取值为+1或  -1；
- $ w_{ij} $ 是连接神经元 $ i $ 和 $j $ 的权重；
- $ \theta_i$是与神经元$ i $ 相关的阈值。

这个能量函数包含两个部分：

1. **第一部分**：表示神经元之间的相互作用，抑制或促进彼此的活动。
2. **第二部分**：表示神经元的偏置，影响神经元的激发。

在网络运行中，Hopfield 网络试图通过调节神经元的状态来最小化这个能量函数，最终达到某个特定的记忆状态。通过这种方式，Hopfield 网络能够在一定条件下从噪声输入中恢复出原始存储的模式。

在 Hopfield 网络中，能量函数 \( E \) 的变化通常与神经元的状态更新相关。对于状态更新，Hopfield 网络的设计保证了每次状态更新后的能量函数是非增加的，即 \(\Delta E \leq 0\)，其中 \(\Delta E\) 是能量的变化量。

### 能量差值计算

假设第 \( i \) 个神经元的状态从 \( s_i \) 更新到 \( s_i' \)，则能量差值可以表示为：

$
\Delta E = E(s') - E(s)
$

其中 \( s' \) 是更新后的状态，\( s \) 是更新前的状态。

### 计算能量差

$s_i' = sign (\sum_{j=1}^{N} w_{ij} s_j + \theta_i)$

我们可以利用能量函数的定义来计算这个变化：

$
\Delta E = -\frac{1}{2} \sum_{j=1}^{N} (w_{ij} s_i' - w_{ij} s_i) s_j - (\theta_i s_i' - \theta_i s_i)
$

经过简化后（假设 \( s_i' \) 和 \( s_i \) 仅在第 \( i \) 个神经元状态上不同），可以得出结论：

$
\Delta E = -\left( \sum_{j=1}^{N} w_{ij} s_j + \theta_i \right) (s_i' - s_i)
$

### 状态更新准则

Hopfield 网络使用的是激活函数，当以下的条件成立时：

$
\sum_{j=1}^{N} w_{ij} s_j + \theta_i \geq 0 \quad \text{(若 } s_i' = +1\text{)}
$
$
\sum_{j=1}^{N} w_{ij} s_j + \theta_i < 0 \quad \text{(若 } s_i' = -1\text{)}
$

这意味着，状态的改变总能使能量函数的值下降或保持不变。因此，能量的差值满足：

$
\Delta E \leq 0
$

### 结论

这样，Hopfield 网络通过在状态空间的随机游走，逐渐找到能量最小值，从而使网络最终收敛到存储的记忆模式。这种设计使得网络在学习和存储信息时具有稳定性与可靠性。

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905155235601.png" alt="image-20240905155235601" style="zoom:50%;" />



<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905162259704.png" alt="image-20240905162259704" style="zoom:50%;" />



##  8.20下午

==Continuous Attractor Neural Networks with Adaptation==

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905163952546.png" alt="image-20240905163952546" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905164446047.png" alt="image-20240905164446047" style="zoom:50%;" />



<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905164951721.png" alt="image-20240905164951721" style="zoom:50%;" />

## 8.21上午

==前沿讲座  Probabilistic Computations in Continuous Attractor Neural Networks==
**Perception as Bayesian inference**

posterior belief   posterior distribution  后验分布
Vestibular 

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906084747552.png" alt="image-20240906084747552" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906090140779.png" alt="image-20240906090140779" style="zoom:50%;" />

为什么整合信息需要这么多脑区？

1. 为了增强鲁棒性？
2. reciprocal connection  脑区之间的相互连接是很重要的

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240905185603408.png" alt="image-20240905185603408" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906092616125.png" alt="image-20240906092616125" style="zoom:50%;" />

网络的结构是相似的，只是里面的参数不一样，具体参数如何选择并没有什么确定性的方法。
且网络表现不同主要是依赖于前馈输入的不同。

**Sampling Bayesian inference**
Algorithm and representation
What is the sampling algorithm adopted in the neural dynamics?
How are probability distributions represented by neural responses?

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906094456008.png" alt="image-20240906094456008" style="zoom:50%;" />

**后面的数学公式听不懂了**



## 8.21下午

==The Decision-making Model and its Applications==

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906103414229.png" alt="image-20240906103414229" style="zoom:50%;" />

将低维线性不可分的信息投射到网络的高维活动空间当中，使它们变得可分。
将库网络（==层级的库网络模型模拟视网膜的计算认知功能==）和Decision-making Module （==模拟上丘中的决策功能，每个神经元代表不同的时空模式==）进行结合，实现时空识别的任务。

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906104511310.png" alt="image-20240906104511310" style="zoom:50%;" />

层级库网络的参数设定不能颠倒，且time constance的设计也需要是层级递增的，这样能够得到较好的结果。

将呈现内容的顺序和内容本身相解离，具有什么计算特性？
得到的顺序类似于一个模版，该模版可以用于其余具有类似顺序的内容，从而实现类似任务的泛化能力。

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906110300900.png" alt="image-20240906110300900" style="zoom:50%;" />

循环神经回路是如何学习主要的时间序列的顺序结构？  顺序结构的解离表示是怎么加快时间序列的处理的？

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906111645600.png" alt="image-20240906111645600" style="zoom:50%;" />



## 8.22 上午

==Training Recurrent Neural Networks==

1. A general dynamic system
2. Fixed point representation and learning
3. Trajectory representation and learning

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906150451237.png" alt="image-20240906150451237" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906152440159.png" alt="image-20240906152440159" style="zoom: 50%;" />

这里的T指的是需要对 dv/dt 这个式子推导T步。

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906153828812.png" alt="image-20240906153828812" style="zoom:50%;" />

 

可以看出，左边根据链式法则求导出来的结果与右边推导出来的结果一致。

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906155646741.png" alt="image-20240906155646741" style="zoom:50%;" />



==第二种形式 Trajectory representation==

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906165852464.png" alt="image-20240906165852464" style="zoom:50%;" />

 

## 8.22 下午

==RNN  training and Reservoir  Computing==
RNN在神经科学中如何使用，训练

1. Classical RNN training programming (可以查看具体的代码，在brainpy里面进行计算)

2. Reservoir computing: Echo state machine

   Echo State Machine（回声状态机，ESM）是一种递归神经网络（RNN）的形式，主要用于时间序列预测和动态系统建模。它的基本思想是利用一个具有大量随机连接的"回声"状态（reservoir）来处理输入信号，具有较强的动态特性和记忆能力。

   回声状态机是一种简化的递归神经网络，通过固定的随机连接和仅训练输出层的方式，提供了一种高效且强大的方法来处理动态数据，适用于多种时间序列和信号处理的场景。

3. Echo state machine programming

   

4. Applications



Echo state machine 是不管我网络的初始状态是什么样的，只要我从0到t时刻的输入是一样的，那么网络的state是一样的。

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906181244035.png" alt="image-20240906181244035" style="zoom:50%;" />



<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906205208788.png" alt="image-20240906205208788" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906205448044.png" alt="image-20240906205448044" style="zoom:50%;" />

## 8.23 上午

==Training spiking neural network==

Artificial neural networks, but compute with spikes.
This SNN is NOT the SNN in the brain.

* Both SNNs and the brain leverage spikes for computation.
* While SNNs emphasize a feedforward perspective, the brain is characterized by rich feedback and recurrent dynamics 
* SNNs often exhibit high firing rates, in contrast to the brain's average firing rate of 1Hz.
* In SNNs, LlF neurons lacking neuronal morphology (神经元形态学) are prevalent, whereas the brain's neuronal dynamics and dendritic tree structure（树突树结构） are highly intricate.
* SNNs represent a single neuronal network, whereas the brain is a distributedand and hierarchical network that communicates with synchronization.（大脑根据任务动态的选择一些脑区参与活动）
* SNNs are trained using a single learning rule (STDP, surrogate gradient,etc.), whereas the brain's plasticity relies on multiple learning rules.
* SNN is a powerful tool we have for building biologically plausible neural networks
* 结合神经元动力学去设计snn是一个热门方向。

==神经元类型==

1. ==SNN的编码方式(neural coding)：==Rate cding, TTFS coding(Time to first spike, 反应刺激的强弱，有两种方式，Linear Latency Coding and Nonlinear Latency Coding), Phase coding( oscillation, 振动，神经元的Spike 控制在一个周期里面),  Burst coding(连续发放一段时间脉冲，然后停止) ， Delta Modulation(发放脉冲的正负,  类脑的感知器件采用的一个范式，只会对变化的刺激产生反应，变化的强度越大，产生的Spike强度越高。产生了超过阈值的变化，才会发放脉冲，否则就不发放脉冲。) 

   对图片的预编码--- 在SNN中实现多种不同的编码机制

2. ==SNN的训练算法（training algorithm）：== 起初是固定住中间层，只训练最后层**；**引入Synaptic Plasticity STDP规则**；**借鉴大脑的多巴胺系统，调节一个区域里面Plasticity的强弱，Three-Factor Plasticity**；**Feedback Alignment**；** 借鉴ANN的训练算法，怎么在SNN里面近似BP，backpropagation。

3. ==Topology Structure:== 



Dynamic Vision Sensor (DVS)
训练SNN困难，而训练ANN转为SNN，该方向并不是很友好。这种方法并不节能。

有标准的正面图片，有一些复杂场景下的侧面图片，如何进行图片的匹配？  
转换为Spike，引入噪声，导致网络认为不同侧面图片是来自不同的物体。  
在网络中以内一些Adaptation，Smooth， 如何平滑，控制是要学习的，而不是简单的在输入端进行平滑。
信号处理的噪声，无法解决不了，计算效率以及影响准确度。



==Coding==

==Surrogate gradient==

==Summary==

1. SNNs share the same spike-computing property as the brain, but are nonetheless far from mimicking the degree of complexity in the brain.
2. The drawbacks of SNNs and the promise of neuromorphic computing devices make it a great time to do research.
3. Training SNNs requires determining 3(4) components: a) neuron model b) coding scheme c) training algorithm  d) network topology
4. We can train SNNs with BPTT. At the core of its successful application is the surrogategradient method.



把EI平衡网络引入到SNN的训练当中，是一个很有意思的方向，且在实验中发现，EI平衡网络的作用和BatchNorm 作用有点类似。  （如何深度结合是一个方向）

大脑是一个 recursive network 循环递归的网络

训练SNN的过程和ANN类似，首先初始化很重要，引入EI-Balance等等。  
时间步长的选取仍然取决于自己的任务。



## 8.23 下午

==环宇翔， 广东省智能科学与技术研究院==   
**从生物大脑到人工大脑---类脑计算芯片与系统简介**

目录： 

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906112355940.png" alt="image-20240906112355940" style="zoom:33%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906113213993.png" alt="image-20240906113213993" style="zoom:50%;" />

对大脑有更深层认识后，如何依据大脑去设计我们的架构？

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906143207157.png" alt="image-20240906143207157" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906212731270.png" alt="image-20240906212731270" style="zoom:50%;" />



<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906212925057.png" alt="image-20240906212925057" style="zoom: 50%;" />



<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906213122385.png" alt="image-20240906213122385" style="zoom:50%;" />



<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906213637864.png" alt="image-20240906213637864" style="zoom:50%;" />



<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240906214107845.png" alt="image-20240906214107845" style="zoom:50%;" />



* ==国内外芯片介绍==

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240907085411626.png" alt="image-20240907085411626" style="zoom:50%;" />

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240907090615116.png" alt="image-20240907090615116" style="zoom:50%;" />



<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240907091208975.png" alt="image-20240907091208975" style="zoom:50%;" />



1. **问：**==现在的类脑芯片因为各种各样的元器件，大规模集成会带来很大的不稳定，所以未来如何解决？==
   **答：**类脑的器件领域目前也比较火。现有的类脑芯片，还是使用传统的电路设计方法，并没有从底层去进行设计，取得一些突破，例如从晶体管层面获得一些突破。

   类脑器件（采用模拟电路的形式，故在生产制造时存在的问题仍然是器件一致性的问题）可以有不同的设计形式，单个期间目前已经有很多课题组或者公司在做，甚至类脑期间的阵列，但是大规模的集成的类脑芯片仍然是问题。
   数字电路，标准化做的比较好，所以可以将当个芯片的规模做的非常大。 
   模拟电路的话，工艺生产问题导致每个器件表现不一致，从而无法集成为大模型的芯片。
   随着制造工艺的发展，器件方面实现突破，进而实现大规模芯片。

2. **问：** ==忆阻器之类模拟电路的器件，通过微分成数字电路计算的器件，这样电路的稳定性可以提升，这个方向如何？==
   **答：** 类脑器件很难解决大规模存的问题，或者大规模编程的问题，那么可以将一些功能放到数字电路上去做，这样存在的问题是数模转换，当规模较大时，主要的延迟或者功耗（瓶颈）在于数模转换电路的设计。也有相关研究去减少转换的开销。

3. **问：** ==忆阻器必要性什么？==
   **答：** 存算融合的终极器件。 可以极大降低功耗。 现在的存算一体在物理上还是分离的，只是尽可能减少通信的距离，但是忆阻器可以实现存算融合。

4. **问：**==根据一个连接矩阵，进行优化，让它尽量在一个芯片内进行计算。能不能反过来我根据硬件的特征。我去设计一个连接矩阵，让它基本上都在芯片内进行计算的？==
   **答：** 这个问题相当于是将问题抛给了做算法的同学，让他们在设计算法时，考虑硬件的限制。

5. **问：**==为何类脑芯片的设计相比CPU的规模差了那么多？==
   **答：**晶体管相当于最小的元器件，一个晶体管就是一个开关的操作， CPU里面可以有百万个晶体管，而类脑芯片的设计，每个神经元和突触都需要多个晶体管去模拟，故最终能模拟的数量是远小于cpu能达到的量级。如果忆阻器未来发展的很好，那可能一个忆阻器就可以实现一个神经元的模拟，最终集成的数量就会非常大。

6. **问：** 
   **答：** 

    



## 8.23 晚上：问答  吴思老师

1. **问：** 自由能原理会是神经科学/脑科学领域的万有引力定律吗？
   **答：** 不会。物理学更多是一个科学问题，脑科学更多是一个工程问题。大脑是一个进化的过程，他更多是解决一个个问题。==不同的物种大脑所采取的工作机制都是不一致的。==

2. **问：**吴老师您好，想问您  认知大模型还有多远？实现路径和关键是什么？什么时候可以商业化？
   **答：** 不确定，应该不远了。

   实现的路径是：大数据，大模型训练的范式。 认知大模型应该学习AI的发展道路，连猜带蒙的训练，揣摩
   商业化：不好说，当前好多AI公司都在亏损。

3. **问：**如果想要解决意识的本质，或者是意识的起源问题，有哪些前提条件是必需的呢？比如说数学工具、算力、实验数据、实验仪器等等。（PS. 这个问题是针对PPT中4W问题提出来的）
   **答：** 意识？大模型真的产生了意识吗？ 意识的定义存在争议。意识这个问题太复杂。  认知大模型到后期需要关注这个问题。

4. **问：**能不能点评或者是推荐一下国外的一些做的比较好的计算神经科学的lab或者是组？尤其是非欧陆美英的地区。如果可以的话也请介绍一下国内的？
   **答：** 国外好的大学，神经科学系通常都会配一到两个组做计算神经科学的组，都不是特别集中，哥伦比亚大学，日本的一些大学等等都还是不错的。  国内的话一搜就能搜到，不是很多。

5. **问：**如何看待欧洲脑计划（HBP）的不太理想的收场，这是否意味现在并非进行大规模神经元模拟的合适的时机？
   **答：** 一定要上升到理论层次，一些大问题才能被解决，例如天体运动规律，光靠收集数据是不行的。
   大脑的奥秘绝不是靠收集数据就能解决的，而是需要提出一系列理论来支撑，理论神经科学。
   实验（收集大量数据）和 理论  双驱动

6. **问：**如何理解和看待类似《Could a Neuroscientist Understand a Microprocessor?》这篇文章所提到的目前神经科学手段的局限性？（文章大意是用EEG电极测量CPU无法得到软件的运行原理）
   **答：** 可以拆开所有部件，去看每个部分，但是没有对应的运行原理支撑，我们是无法了解大脑运行机制的。

7. **问：**对涌现和系统科学的研究与数学建模，是否有助于加速计算神经科学领域的GPT或者是AlpgaGo这类杀手级应用的出现？
   **答：**在系统层面对大脑的建模是必须的，对局部进行建模都实现不了高级功能，那更实现不了高级应用。
   高级功能的认知和建模-----------------类脑级别的应用

8. **问：**连续吸引子网络精度可以从哪些方面提升？
   **答：** 连续吸引子神经元数量越多，精度一般都会提升的。

9. **问：**吴老师能否介绍一下计算神经科学领域的不同研究分支，可否推荐一些经典的review帮助新手摸清研究方向？本科生在已有一定编程基础下该如何进一步参与研究工作？
   **答：** 当前计算神经科学这个学科还在整合，无法给出不同的分支，不同的分支其实还没有形成。
   目前也没有经典的Review的文章。  本科生直接参加课题组吧，从一个具体的科学问题入手。

   入门门槛比较高，因为涉及到了多学科的交叉，所以需要的知识比较多。

10. **问：**计算神经模型如何用到AI的任务上，比方说目标检测，人体姿态估计等，有没有好的思路，切入点？
    **答：** 目前做出了很多工作，
    切入点：不能直接拿AI的任务去找解决方法。
    通常的思路是先建模，感觉挺有意思，然后考虑是否可以将这个模型应用于AI领域。
    且对于具体的任务，应该更多考虑关于大脑可以做的许多任务，而不是AI可以做的任务。

11. **问：**连续吸引子网络，SNN等是否能进行无监督学习，有没有经典的例子？
    **答：** 连续吸引子网络的学习就是无监督的。   SNN当然也可以做无监督学习。

12. **问：**认知神经科学的学生想转计算神经科学专业，在学术背景方面需要做哪些方面的努力？
    **答：** 在数理，编程方面学习就行

13. **问：**相比于神经模型，模拟真实大脑的神经元的连接方式的难点，是因为存在大量的recurrent，feedback之类的非线性的连接吗？
    **答：** 的确，这是一个点，还有一个点是实验数据太少。
    当前特别关注：假设我有一些数据和任务的情况下，如何通过训练的方式去构造Network，达到很好的效果。

14. **问：**国内外，有没有类似咱们这次培训课程的计算神经课程之类的？
    **答：** 貌似没有这么细致的课程。

15. **问：**吴思老师心中，如何定义智能，怎么看待目前所谓最先进的智能，比如 llm
    **答：** 不好定义。  智能的定义存在争论。  生物大脑的适应性-----在长期进化下最重要的智能。  
    具身智能。     发展具身智能还是要回到大脑是如何在长期进化中具有这种适应性的。

16. **问：**智能，是大脑各层机制的综合体现。那对于实现智能，是不是意味着，如果缺少这个机制中的任何一个环节，就无法实现智能。以及对于实现智能，这么多环节中，吴思老师认为，哪些部分是最重要的？记忆，学习，推理，感知？
    **答：** 这个问题定义较高，对于我们做实验来说，我们能够解决记忆问题，学习问题，推理问题等一个问题解决了，就是实现了智能。
    这几个方面都非常重要。我们当前需要部分实现，而不是一下全部实现。

17. **问：**硬件实现神经拟态的关键是啥以及现在发展的情况？
    **答：**  硬件实现神经拟态的关键不知道是啥。     目前硬件的实现挺难的，肯定要做简化，计算建模就已经比真实生物神经系统简单了，而要在硬件上实现要继续做简化。

18. **问：**混沌系统是不是神经元运算的关键，尤其是混沌边缘？（物理背景）
    **答：** 混沌系统能帮助我们理解神经系统的特色。混沌系统无法描述计算，  混沌边缘是个很重要的概念，但是对于理解大脑是完全不够的，我们需要理解大脑如何接受信息，加工信息，输出信息全部解释并回答出来，要回答这些需要概率理论，机器学习的方法，此时更像是一个工程问题，而不是科学问题。

19. **问：**神经元适应性是否是重要的话题？以及神经元适应性的研究有哪些应用？为什么要把神经元适应性应用到神经网络计算中？
    **答：** 神经元适应性是客观存在的。 大脑要把适应性用在损失的实时计算中， 在连续吸引子网络中加入适应性后，大量实验现象都能被解释了。   适应性当前并没有加在ANN中，==那应该可以加在SNN中。==

20. **问：**如何跟踪吴老师课题组科研进展，有没有github之类的？
    **答：** 课题组的微信公众号。 github上只有brainpy的东西。

21. **问：**语言，想法，思考过程等在大脑中的编码原理，目前有相关工作吗？
    **答：** 没有相关工作，在这方面基本脑的模型都没有。

22. **问：**SNN未来的发展方向应该是怎样的，目前学生从事SNN的研究又应该从哪些方面进行入手？
    **答：** SNN定义是生物神经的SNN，还是借鉴ANN的SNN?
    把大脑加工信息的结构，原理加入到SNN中，偏向于生物神经的SNN。

    但是读文献，SNN这个名词已经被做Spiking Neural Network 所占领，如果是这个方向，那就不仅仅要学习大脑脉冲发放的方式，而是要加入大脑各种结构，功能的东西，才有可能击败ANN，如果只是把ANN变为脉冲，那面临的困境即使performance无法超过ANN。

23. **问：**神经网络是连接主义，连接主义存在遗忘问题，请问当前的计算神经科学的理论是否解决了遗忘问题？ann跟人类大脑相比还是存在较为严重的“灾难性遗忘”，哪些理论或方法可以缓解ann的遗忘问题？
    **答：** ANN带来的遗忘问题在大脑中的不存在的，因为本身学习机制就不一样。
    ANN的学习机制是，面对新任务时，直接使用之前的权重去学习，这当然就会忘了旧的任务知识。而大脑根本不是这样去学习的，在学习了新任务后，大脑会将他们存储于其他脑区，当再学习其他任务时，根本就不存在遗忘问题了。

    走向类脑，就不存在灾难性遗忘的问题了。

24. **问：**吴老师，能否推荐一些认知（计算）神经科学专业的硕士研究生相关的就业方向（目前硕士三年级由于没有论文发表，感觉申请好学校的博士存在一定困难）。
    **答：** 回答不了，猜测应该不好就业。

25. **问：**如何评价一个计算神经模型的好坏，比方说AI模型可以在benchmark上进行精度排行。
    **答：** 从计算神经科学来说，缺乏benchmark。

    但如果你关注建模的精准度，那就应该对比生物领域的真实机理。

26. **问：**请问吴老师如何看待研究神经计算在发展AI中的必要性，就是，有没有可能未来的AI突破完全不需要神经计算的发展，而是仅靠数理上的发展就可以实现强人工智能的突破？这样会不会就导致研究神经计算失去了它最重要的意义？即，您觉得未来AI的发展和神经科学的发展关系大不大？
    **答：** 从理论上完全有可能仅靠数理发展，就实现强人工智能。 但不太可能。
    类脑是发展人工智能的重要途径。 神经计算建模不是要发展AI，而是要看大脑是如何形成的，如何进化的，了解意识形成，了解人是怎样的。AI是神经计算建模的副产品。

27. **问：**吴老师如何看待脑机接口的发展和应用前景？（脑机接口的范式似乎在近十年内没有特别大的变化，但目前操控机械臂仍然困难）
    **答：** 当前并没有实质性突破。      有两点：1.我们对大脑的工作原理完全不清楚，从大脑中读信号完全是统计学习的方法，没有充分利用这些数据且记录的神经元数量有限。   2.具身智能在机器人这方面进展特别差， 即便把所有信号读出来，现有的机器也无法人类所具有的功能。       
    人本身的研究很重要。    

28. **问：**现有的文献中模拟LFP（**Local Field Potential, LFP**）或是EEG有神经元网络模型和神经群模型，在做课题时一般如何选择？两类模型的适用场景能介绍一下吗？在多脑区建模方面有什么建议吗？
    **答：** LFP目前有在做，EEG建模不好做。

    **空间分辨率**：LFP 提供较高的空间分辨率，在毫米级，而 EEG 在空间分辨率上相对较低，通常是厘米级。
    **细胞活动的直接性**：LFP 更加直接地反映了局部神经元的活动，而 EEG 反映的是大规模脑区的整体活动。

29. **问：**就老师目前的研究经验和自身体验来说，大脑是机器吗？
    **答：** 大脑有些时候是机器，有些时候不是机器，区别大脑是不是机器的关键在于：意识。 
    只有意识参与进来，大脑才有了意义。

30. **问：**大部分神经网络都有着固定的模型框架，但我们知道大脑是会不断生成新神经元的，以及神经元可能会被多方面利用，例如盲人的视觉神经元可能被用于听觉方面，计算神经科学应当如何建模这一点
    **答：** 现有模型确实都是固定的框架。 大脑发育过程的模拟----还没有做。

31. **问：**如何看待马斯克提到的意识上传
    **答：** 当成一种理想。

32. **问：**
    **答：**

    
