 https://braincog.ai/~yizeng/

中科院 曾毅教授



# Parallel Spiking Unit for Efficient Training of Spiking Neural Networks

Li Yang，曾毅， 中国科学院大学，用于SNN高效训练的并行脉冲单元



* **论文题目：**

  Parallel Spiking Unit for Efficient Training of Spiking Neural Networks

* **研究动机、目的：**

  高效的并行计算已成为推动人工智能发展的关键因素。然而，SNN的应用受到其固有的顺序计算依赖性的限制。这个限制源于每个时间步的处理必须依赖于前一个时间步的结果，这显著阻碍了SNN模型在大规模并行计算环境中的适应性。

* **做了什么：**

  为了解决这一挑战，作者团队引入了创新的 Parallel Spiking Unit（PSU）及其两个衍生版本：Input-aware PSU（IPSU）和 Reset-aware PSU（RPSU），实现了脉冲计算的并行化。 本质上是对LIF或者IF神经元的一种变换。

* **怎么做的：**

  这些变体巧妙地解耦了脉冲神经元中的泄露积累和发放机制，同时以概率方式管理重置过程（在先前研究并行脉冲神经元中，直接忽略了重置过程，而这篇文章加入了重置过程）。通过保持脉冲神经元模型的基本计算属性，我们的方法使得SNN中的所有膜电位实例可以并行计算，从而实现并行脉冲输出生成，显著提高了计算效率。

  具体实现方式：首先不考虑重置过程，对SNN神经元模型的公式进行解耦，形成对应的权重矩阵，进而可以直接计算出每一步的膜电位，而不需要等待前一步计算出来，再计算当前步的膜电位。接着加入重置过程，从脉冲神经元的重置过程中可以看到，多了 $$-V_{th}S[t]$$这一项，也变成相应的系数形式（$$-BS$$）并代入脉冲计算的公式中，此时脉冲生成函数变为 $$S = Θ(AI-BS-V_{th})$$ ,出现了矛盾，接着对重置项当中的S进行替换，变为 $$σ(AI)$$ ，然后  $$S = Θ(A_II-Bσ(A_RI)-V_{th})$$ , 也就是对应的两个变体，当两个系数矩阵均为A时，就是原版的PSU。

* **取得进展：**

  在各种数据集（包括静态和序列图像、动态视觉传感器（DVS）数据以及语音数据集）上的全面测试表明，PSU及其变体不仅显著提升了性能和仿真速度，还通过增强神经活动的稀疏性提高了SNN的能效。
  另外对比显示PSU和RPSU具有更小的脉冲发射率

* **存在问题，缺点：**

  相比之前一个论文提出的 Parallel Spiking Neurons(PSN)，PSU仅仅是加入了重置过程，在结果上仅提升了一点，且代码并没有公开，无法看到其与PSN的实际差别。





* **论文题目：** Brain-inspired Evolutionary Architectures for Spiking Neural Networks   用于SNN的脑启发进化架构

* **研究动机、目的：**  人脑通过自然进化形成的复杂独特的神经网络拓扑使其能够同时执行多种认知功能。生物网络结构的自动进化机制启发我们探索脉冲神经网络（SNNs）的高效架构优化。

* **做了什么：** 

  本文通过结合脑启发的局部模块化结构和全局跨模块连接，演化SNN架构，而不是使用手工设计的固定架构或层次化的网络架构搜索（NAS）。

  进化神经架构搜索（ENAS）是一种模拟物种进化或种群行为的计算范式。候选架构将被初始化并编码，然后通过一些进化计算（EC）方法，如遗传算法（GAs）、粒子群优化（PSO）、进化策略（ES） 和差分进化（DE）进行优化和迭代，直到种群的适应度逐渐演化，获得一组高质量的解决方案。与基于RL的NAS相比，ENAS在计算资源上更为高效，并且比基于梯度的NAS需要的先验知识更少，能够广泛应用于解决复杂的多目标优化问题。

  * 编码脑启发的神经电路

    在我们的模型中，进化SNN架构的搜索空间包括由五种神经电路模式组成的局部模块化结构，以及模块之间的全局无限制长期连接。人脑中常见的神经电路模式包括前馈兴奋（FE）、前馈抑制（FI）、反馈抑制（FbI）、侧抑制（LI）和相互抑制（MI）。作者通过脉冲神经元来实现这些模式，不同模块中的模式组合各异，这受到大脑区域不同结构的启发。全局连接模拟了大脑区域之间的连接方式，不是传统的层次结构，而是自由连接的，没有限制连接的方向以及是否跨越模块。

  * Evolutionary Neural Architecture Search for SNN 

    **多目标优化：** 对于SNN来说，准确性的提高通常伴随着能量的损失，即发射更多的脉冲。为了找到具有较低能耗和较高效率的SNN架构，本文将验证误差和脉冲数量作为优化目标。

    

* **怎么做的：** 局部方面，受脑区启发的模块由多个神经模式组成，这些模式之间具有兴奋性和抑制性连接；全局方面，我们演化模块之间的自由连接，包括长期的跨模块前馈和反馈连接。作者团队进一步引入了一种基于少量样本性能预测器的高效多目标进化算法，使SNN具备高性能、高效率和低能耗的特点。

* **取得进展：**静态数据集（CIFAR10, CIFAR100）和神经形态数据集（CIFAR10DVS, DVS128-Gesture）的广泛实验表明，我们提出的模型提升了能效，并在性能上取得了一致且显著的进展。

  从结果上看，本文提出的方法相较于现有的其他研究确实达到了较高的准确率，在某些数据集上甚至超过现有最高的准确率。

* **存在问题，缺点：**

  文中没有在ImageNet数据集上进行实验，显然应该是结果不太好。





* **论文题目：**An Efficient Knowledge Transfer Strategy for Spiking Neural Networks from Static to Event Domain

* **研究动机、目的：**

  脉冲神经网络（SNNs）在时空动态方面非常丰富，并适合处理基于事件的神经形态数据。然而，基于事件的数据集通常比静态数据集的标注要少。这种小数据规模使得SNNs更容易过拟合，并限制了它们的性能。为了提高SNNs在基于事件数据集上的泛化能力，我们利用静态图像来辅助SNN在事件数据上的训练。在本文中，我们首先讨论了直接将在静态数据集上训练的网络转移到事件数据时遇到的领域不匹配问题。我们认为，特征分布的不一致性成为阻碍从静态图像向事件数据有效转移知识的主要因素。

* **做了什么：**

  为了解决这个问题，我们从特征分布和训练策略两个方面提出了解决方案。

* **怎么做的：**

  首先，我们提出了知识转移损失，包括领域对齐损失和时空正则化。领域对齐损失通过减少静态图像和事件数据之间的边缘分布距离来学习领域不变的空间特征。时空正则化利用事件数据在每个时间步的输出特征作为正则化项，提供动态可学习的领域对齐损失系数。此外，我们提出了滑动训练策略，通过概率逐步用事件数据替换静态图像输入，使网络的训练更加平稳和稳定。

  本文使用的神经元模型为LIF，首先进行测试，发现同样的网络，在静态数据集上进行训练得到的特征和事件数据非常不一样，也就说无法使用静态数据作为预训练数据，接着迁移到事件数据上。r然后设计损失函数衡量这两个网络的差异，在训练过程中不断缩小这种差异。 接着作者说如果仅仅使用 领域对齐损失 可以会丢失时间维度上的信息，所以又加入了时空正则化，为域对齐损失提供了动态可学习的系数，并且这种自适应系数确保了在每个时间步长为数据特征分配特定的权重。为了防止模型在某个时间步过拟合，作者将每个时间步的事件数据分类损失（反映事件帧特征对分类的贡献）作为正则化项。

* **取得进展：**

  我们在神经形态数据集上验证了我们的方法，包括N-Caltech101、CEP-DVS和NOmniglot。实验结果显示，我们提出的方法在所有数据集上均比当前最先进的方法表现更好。

* **存在问题，缺点：**

  针对性较强的文章，当然也没有在ImageNet数据集上进行实验。



# 论文简读的格式示例

**论文题目：**



**研究动机、目的：**



**做了什么：**



**怎么做的（主要思想方法）：**



**取得进展：**



**存在问题、缺点：**





# TIM: An Efficient Temporal Interaction Module for Spiking Transformer

**论文题目：**TIM: An Efficient Temporal Interaction Module for Spiking Transformer
	为Spikformer网络设计一个有效的时序时序交互模块

**研究动机、目的：**

​	脉冲神经网络（SNNs）作为第三代神经网络，因其生物学上的合理性和计算效率而受到关注，特别是在处理多样化数据集方面。受神经网络架构进展启发的注意力机制的整合，催生了 Spiking Transformers 。这些网络在提升 SNNs 能力方面显示出前景，尤其是在静态和神经形态数据集领域。
​	尽管如此，这些系统仍存在明显的差距，尤其是在==脉冲自注意力（SSA）机制在利用 SNNs 时间处理潜力方面的有效性。==

**前人工作：** Advances in SNN Temporal Processing    (SNN时序处理方面的研究进展)

​	在提升脉冲神经网络（SNNs）的时间信息处理能力方面，研究人员已取得显著进展。例如，[Kim et al., 2023] 通过估计权重的Fisher 信息探讨了SNNs中时间信息的动态特征。PLIF [Fang et al., 2021b] 通过引入可学习的膜电位常数，增强了SNNs在不同时间步的信号整合。IIRSNN [Fang et al., 2020] 通过突触建模改善了SNNs的时空信息处理能力。[Zhang et al., 2021] 发现基于脉冲时序的事件驱动学习可以显著提高分类准确性。TA-SNN [Yao et al., 2021] 引入了基于时间的注意力机制，使得可以自适应地分配不同时间步的重要性。TCJA [Zhu et al., 2024] 融入了评估脉冲在时间和空间维度中重要性的注意力机制。[Shen et al., 2024] 通过引入树突状非线性计算显著增强了SNNs的时间信息处理能力。ETC [Zhao et al., 2023] 引入了时间增强一致性约束，使得SNNs能够从不同时间步的输出中学习，从而提升性能并减少延迟。BioEvo [Shen et al., 2023] 采用了生物启发的神经电路搜索方法，自适应地协调不同电路，以提高SNNs在感知任务和强化学习中的表现。

**Spikformer中存在的限制：**

​	时间信息的保留和提取完全依赖于神经元膜电位的动态变化（当前时刻）。在现有的脉冲自注意力机制中，时间信息尚未得到充分考虑，这一缺陷显著限制了Spikformer在处理时间序列信息方面的能力。

**做了什么：**

​	为了解决这个问题，引入了时间交互模块（TIM），这是一种新型的基于卷积的增强模块，旨在增强 SNN 架构中的时间数据处理能力。（感觉和Spikeformer中引入SCS去替代SPS有点类似，都是对卷积层的高度利用）

**怎么做的（主要思想方法）：**

​	原始的SSA结构在得到脉冲形式的Q,K,V矩阵后，直接进行矩阵乘法，而这篇论文在得到Q,K,V之后，令Q经过一个模块之后，在进行矩阵乘法。这个模块就是本文的工作。接下来解释这个模块。

​	首先我们知道输入进来的数据是T步的数据，取第t-1个时间步数据和第t个时间步数据，第t-1个时间步数据经过一个卷积层和LIF层，接着乘以α，而第t个时间步直接乘以（1-α），最后将他们进行相加，再次进入到LIF神经元中，得到新的第t步的Q。

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240817171821910.png" alt="image-20240817171821910" style="zoom: 67%;" />

​	与传统的SSA相比，我们引入的f操作（采用一维卷积）并没有显著增加参数数量。通过结合TIM构建的注意力矩阵，该模型不仅能够处理当前时刻的信息，还能利用过去时刻的输出信息，从而有效捕捉时间序列的内在动态。这一增强显著提升了模型的时间记忆能力，使其能够在每个时间步利用历史信息。这个特性使得模型能够根据不同任务和数据的特征动态调整其行为，从而提高计算效率和模型的泛化能力。

**取得进展：**

​	TIM的整合过程流畅高效，所需的附加参数极少，同时显著提升了其时间信息处理能力。通过严格的实验，TIM 在挖掘时间信息方面表现出色，在各种神经形态数据集上取得了最先进的性能。

**存在问题、缺点：**

​	1.读完发现，其实论文核心思想很简单，但是跑的数据比较多，所以提出的这个架构应该是测试了好多个。
​	2.另外就是这种修改网络架构的模型，还是缺乏理论解释，和ANN魔改模型一样了。





# Directly Training Temporal Spiking Neural Network with Sparse Surrogate Gradient

**论文题目：** Directly Training Temporal Spiking Neural Network with Sparse Surrogate Gradient
2024.06.28 放到了arxiv上

**研究动机、目的：** 
受到事件驱动计算和节能特性的启发，脑神经网络（SNNs）引起了广泛关注。然而，脉冲神经网络的全或无特性==阻碍了其在各种应用中的直接训练==。最近，代理梯度（SG）算法使得脉冲神经网络在神经形态硬件中得以大放异彩。然而，==引入代理梯度导致SNNs丧失了其原有的稀疏性==，从而可能导致性能下降。

​	多种代理梯度变体被用于直接训练脉冲神经网络（SNNs）。它们处理的膜电位范围直接影响梯度的前向传播深度。==更广泛的代理梯度覆盖更大的膜电位输入范围，从而促进更显著的梯度更新。然而，这可能导致不良更新，从而阻碍网络寻求最优解，产生梯度适应问题。==另一方面，==更狭窄的代理梯度更接近狄拉克δ梯度的轮廓，更准确地反映了原始梯度更新方向。梯度的高度稀疏性通常会限制进一步的前向传播，这可能导致梯度消失问题。== 诸如ESG等方法在训练后期采用更清晰的代理梯度，虽在一定程度上解决了梯度适应问题，但有可能削弱网络的学习能力。这一观点得到了多种学习率衰减策略的支持，这些策略显示即使在高级训练阶段，网络仍然具有强大的学习潜力。==因此，充分利用代理梯度的学习能力并在直接训练过程中保持梯度的稀疏性是一个亟待解决的问题。==

**做了什么：**

​	本文首先分析了使用SGs进行直接训练的现有问题，并提出了 Masked 代理梯度（MSGs）以平衡训练效果和梯度稀疏性，从而提高SNNs的泛化能力。此外，本文引入了时间加权输出（TWO）方法来解码网络输出，强调了正确时间步的重要性。

**怎么做的（主要思想方法）：**

​	为了应对上述挑战，我们从稀疏性和时间性两个角度优化脉冲神经网络（SNNs）。在稀疏性方面，我们提出了 masked 代理梯度（MSG）方法。在训练过程中，仅通过随机梯度下降（SGD）更新特定子集的参数，其余参数则保持为零，以维持稀疏特性。与evolving surrogate gradient（ESG）不同，MSG在所有训练阶段保持相同的代理梯度设置，平衡了传统代理梯度的有效性与原始脉冲神经网络梯度的稀疏性。在==通过代理梯度更新时，MSG考虑了SNN固有的梯度稀疏性，从而为SNN优化提供了有效的正则化。==这一策略消除了网络优化因过度依赖代理梯度而陷入局部最小值的潜在困境。
​	鉴于代理梯度在优化中的局限性，SNNs固有的时间依赖性未能被准确捕捉。这种忽视可能导致每个时间步的输出对最终结果的贡献存在差异。==在时间性方面，我们提出了 Temporal Weighted Output （TWO）方法，区别于基于梯度下降的调整方法==。该方法利用历史统计信息捕捉基于时间的输出的重要性，从而实现更精确的解码。

<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240817212917096.png" alt="image-20240817212917096" style="zoom:67%;" />

**取得进展：**

大量在不同网络结构和数据集上的实验表明，使用MSG和TWO进行训练的表现超越了现有的最先进技术。 ==除了ImageNet数据集==

==显著降低了ANN 转 SNN的时间步长，并提高了准确率。==
==使用普通网络（vgg,resnet）即可达到较高准确率。==

**存在问题、缺点：**

仅仅测试了VGG，Resnet，以及其余几个网络，并不能看出这个方法的广泛适用性。





# Temporal Knowledge Sharing enable Spiking Neural Network Learning from Past and Future

**论文题目：**Temporal Knowledge Sharing enable Spiking Neural Network Learning from Past and Future

​	时序知识共享使脉冲神经网络能够从过去和未来中学习

**论文影响说明：**  impact statement   感觉就像是工作原因及工作总结

​	脉冲神经网络由于其类似大脑的信息处理机制，受到了各个领域的广泛关注。脉冲神经元的存在赋予了SNNs高效的时空动态，使其特别擅长处理具有时间特征的数据。然而，SNNs通常面临着由于长时间步长导致的延迟、时间信息利用率低以及测试与训练之间需要步长一致等问题。这些挑战阻碍了SNNs在各类设备上的部署。通过引入我们的方法，我们不仅提升了SNNs处理时间特征的能力，还减少了测试阶段所需的步骤。我们的方法解决了测试与训练之间步长一致的需求，使SNN能够在较短的步长下实现优于训练阶段的性能，从而显著提高了SNN的适应性和部署便捷性。

**研究动机、目的：**

​	脉冲神经网络（SNNs）由于其类似大脑的信息处理机制，受到了各个领域研究人员的广泛关注。然而，SNNs通常面临着==时间步长较长、时间信息利用率低以及测试与训练之间需要一致的时间步长等挑战。== 这些问题导致了==SNNs的高延迟==。此外，==时间步长的限制要求对模型进行重新训练以适应新任务的部署，从而降低了适用性==。

**做了什么：**

​	为了解决这些问题，本文提出了一种新颖的观点，将SNN视为==时间聚合模型==。我们引入了时序知识共享（TKS）方法，促进了不同时间点之间的信息交互。TKS可以被视为一种  ==temporal self-distillation== （时序自蒸馏形式）。

**怎么做的（主要思想方法）：**



**取得进展：**

​	为了验证TKS在信息处理中的有效性，我们在静态数据集如CIFAR10、CIFAR100、ImageNet-1k以及神经形态数据集如DVS-CIFAR10和NCALTECH101上进行了测试。实验结果表明，我们的方法在与其他算法的比较中达到了==最先进的性能==。此外，TKS解决了==时间一致性问题==，使模型具备了卓越的==时间泛化能力==。这使得==网络可以在较长的时间步长下进行训练，并在较短的时间步长下保持高性能==。这种方法显著加快了SNNs在边缘设备上的部署。最后，我们进行了==消融实验==，并在==细粒度任务上==测试了TKS，结果展示了TKS在高效处理信息方面的==增强==能力。

**存在问题、缺点：**
