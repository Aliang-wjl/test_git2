# 第一条思考

SNN所涉及到的时间维度，是否真的是需要该维度。因为在我们目前的考量中，大脑依靠的是记忆，对于时间的发生从来都是随机的。

SNN的用处貌似并不是在一个任务上要达到多么高的准确率，而是在多个任务上都达到还不错的准确率。

SNN的时间维度应该是用来记忆的，搜寻相关内容。

SNN中的时间常数有可能是一个非常重要的参数，应该设置成跟随网络的深度而逐渐变化？就像学习率一样。

运用CNN中的残差结构时，应考虑到卷积之后的数据形式与脉冲形式是不一致的，如果使用残差需要转换卷积之后的数据形式。

通常SNN的学习算法可以分为有监督，无监督，基于奖励的学习以及 ANN to SNN 的四个方法。



当前大多数研究，最终的结果依据都是脉冲数量的大小，大脑内是否是这样的机制运行？

低精度ANN与SNN之间的联系，SNN精度低的原因更多应该还是因为时间步的问题，增加了时间步，导致误差的增加？



在SNN中使用最大池化的方式，相比平均池化可以得到更加明显的效果提升，这也与之前自己的两篇小论文工作相呼应——对于时间维度的数据，最大池化具有更好的性能。



大脑是否有电位的重置机制，为何LIF神经元会有这种 重置，一个神经元在发射一次脉冲后，电位应该也是下降的趋势而不是直接变为某个值，接着下降吧。



## 2024.07.24

脉冲网络是否更有利于网络泛化性的学习？？



spiking-resnet 给的学习率较低的话，会极大地影响训练速度。
在mnist数据集上进行测试，学习率为 1e-4 时，学习的较慢，且学习的上限到了94.82% (使用1e-4时，第一步的准确率也是非常低的，达到了16.8%)， 学习率为 1e-2时，学习的较快，最终测试集准确率为99.28%，注意这不是最高的，因为我们只训练了20轮。

sew-resnet 在设置与残差相连接的方式为 ADD时（和上述spiking-resnet残差连接一样），学习率设置为1e-2时，最终测试集准确率为99.18%， 设置为1e-4，最终测试集准确率为94.9%，这样学习的加快， 而当使用AND时，学习率要设置的大点，如 1e-2，最终测试集准确率为：99.24%，当学习率为1e-4时，最终测试集准确率为：94.74%
![image-20240724211352526](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240724211352526.png)

这两个网络的区别为： spiking-resnet网络进行残差相加时，如果包含了下采样，就是两个自然数序列的相加，如果没有包含下采样，就是脉冲序列和自然数的相加。   而sew-resnet网络进行残差相加时，永远是两个脉冲序列相加。

------综合对比下来，发现学习率需要设置较大时，最终测试集准确率较高，且高了将近4.5个%点



## 2024.07.25

创建脉冲神经元时，切记每次都要创建一个新的，而不能连续使用一个已经创建好的，否则会报错，因为数据维度在网络中会发生变化。



## 2024.07.28

关于torch.fx函数的使用



# 2024.08.05

我们使用Python Tonic库[32]来便于加载N-MNIST和DVSGesture数据集。



# 2024.09.20

使用低精度ANN对SNN进行蒸馏，或者说如何并联学习，是否能突破时间步的限制？只用两步即可？
主要是SNN的精度比低精度量化的ANN还要低。

1bit量化神经网络的激活函数不是常见的sigmoid或者ReLU之类的，而是Sign（阈值比较，大于阈值，输出1，小于阈值，输出0）
低精度量化神经网络QNN。 （Low precision quantization neural network）

ReLU与IF神经元的有效叠加？

并行与剪枝的叠加使用。

1bit的二值化神经网络BNN的输入和IF模型下的SNN最大的差别在没有时间信息。因为SNN是一串编码，信息不止是编码0和1，还存在于脉冲时间序列中。按理来说SNN的脉冲编码蕴含比BNN更多的信息，准确度会高一些，但是实际上并没有什么优势，感觉SNN的训练还是有点问题。（==或者说IF神经元还可以优化？ 如何基于激活值对神经元公式进行优化？==）

二值化神经网络不是简单把输入量化到0/1，而是量化为-1/+1，只不过用0表示-1，用1表示+1。然后算一下+1×+1，+1×-1，-1×+1，-1×-1，结果再换成对应的0/1表示。然后就可以发现输入的01计算和输出的01表示其实可以就是个1bit的XNOR运算。

