# 关于文献阅读

阅读完文献后，至少要回答以下问题:

1. 这篇文章到底在解决什么问题？
2. 这个问题为什么在这个领域重要？
3. 这些作者是如何解决这个问题的？
4. 这个问题的解决有什么亮点，局限，有什么应用？



## 主题：Parallel spiking neurons with high efficiency and ability to learn long-term dependencies
Fang wei   田永鸿, 北京大学， 

高效且能学习长期依赖关系的并行脉冲神经元

* **摘要：**  传统的脉冲神经元只能以串行方式进行前向传播（模拟），且难以学习长期依赖关系。作者发现，移除脉冲神经元的重置操作，就可以实现并行化（前向传播）的脉冲神经元，且具有极高的模拟速度。提出了PSN, masked PSN(保证在t个时间步内，前面的输入会对后面输出产生影响，而后面的输入不会对前面的输入产生影响)，保证输入在每个时间步上权重都不变就形成了 slide PSN。 

* **介绍：** 原始脉冲神经元与并行脉冲神经元的区别：

  <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240801184454802.png" alt="image-20240801184454802" style="zoom:67%;" />

  ​    	**贡献：** 

  1. 我们分析了去掉传统神经元中的重置操作的影响，结果显示，去除重置后脉冲神经元可以实现并行化。
  2. 通过将没有重置的神经元动态重写为通用公式，我们得到了一种具有完全并行化神经元动态的并行脉冲神经元（PSN）。对于逐步串行前向处理和变长序列处理，我们还导出了masked PSN和 slide PSN。
  3. 我们比较了PSN家族与传统脉冲神经元的模拟速度，结果表明，采用并行神经元动态的PSN在性能上具有显著优势。我们在序列、静态和神经形态数据分类任务上评估了PSN家族，结果显示其准确性高于以往的脉冲神经元。

* **相关工作：** 

  

* **方法：** 

  1. 经过分析，脉冲神经元的重置也许可以去除，但是需要实验验证去除带来的影响。
     对于LIF神经元和IF神经元，H[t]的计算并不需要逐步进行，而是可以在所有输入到来后直接计算得出。
     例如假设，在一段时间时间步内，H[t]<$$V_{thr}$$​ ，那么在这段时间内的H[t] = V[t],就可以同时计算出这段时间内的H[t].
     公式4 和 5 是 IF神经元，6和7是 LIF神经元。

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240802094934654.png" alt="image-20240802094934654" style="zoom:50%;" />

  2. 将上述公式 5 和 7 写成一般形式，可以得到公式8，而 9 和 10 则是并行脉冲神经元的表达式，其中W和B分别为可学习的权重和可学习的阈值，N表示的是Batch_size，T为时间步。==BPTT算法也仍然能够被使用，且不是串行的，而是并行的。PSN的模拟比普通神经元快得多，因为矩阵与矩阵的乘法在GPU中得到了很大程度优化。==

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240802095828501.png" alt="image-20240802095828501" style="zoom:50%;" />

  3. 高阶PSN（H需要由两个矩阵相乘得到）存在的问题就是延迟问题，因为其需要等到所有X[i]到了之后才能得到输出结果。为了解决延迟问题，作者提出了masked PSN  以及  K阶的 M-PSN
     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240802105846581.png" alt="image-20240802105846581" style="zoom:50%;" />

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240802110014205.png" alt="image-20240802110014205" style="zoom:50%;" />

     查看公式13，λ是从0增到1的，故在训练初期，公式结果 >=1，最后就是$$M_k$$了

  4. PSN 和 masked PSN 的参数是时间相关的，这需要额外的操作来处理具有可变长度的序列。为了解决这个问题，我们修改了参数，使其在时间步之间共享，并提出了k阶 slide PSN，其神经元动态为
     可以在推导公式的过程中，把 t-1看做一个整体， X[t - (k-1) + i]

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240802110417869.png" alt="image-20240802110417869" style="zoom:50%;" />

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240802111355761.png" alt="image-20240802111355761" style="zoom:50%;" />

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240802111406478.png" alt="image-20240802111406478" style="zoom:50%;" />

     

* **试验：**

  <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240802180319427.png" alt="image-20240802180319427" style="zoom:67%;" />

​	<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240802180555833.png" alt="image-20240802180555833" style="zoom: 67%;" />

​	

* **讨论与总结：**



# Parallel Spiking Unit for Efficient Training of Spiking Neural Networks
Li Yang，曾毅， 中国科学院大学，用于SNN高效训练的并行脉冲单元



* **摘要：**
  	**背景：** 高效的并行计算已成为推动人工智能发展的关键因素。然而，SNN的应用受到其固有的顺序计算依赖性的限制。这个限制源于每个时间步的处理必须依赖于前一个时间步的结果，这显著阻碍了SNN模型在大规模并行计算环境中的适应性。

  ​	**论文工作:** 为了解决这一挑战，我们的论文引入了创新的并行脉冲单元（PSU）及其两个衍生版本：输入感知PSU（IPSU）和重置感知PSU（RPSU）。

  ​	**工作有效性解释：**这些变体巧妙地解耦了脉冲神经元中的漏积累和发放机制，同时以概率方式管理重置过程。通过保持脉冲神经元模型的基本计算属性，我们的方法使得SNN中的所有膜电位实例可以并行计算，从而实现并行脉冲输出生成，显著提高了计算效率。

  ​	**实验：**在各种数据集（包括静态和序列图像、动态视觉传感器（DVS）数据以及语音数据集）上的全面测试表明，PSU及其变体不仅显著提升了性能和仿真速度，还通过增强神经活动的稀疏性提高了SNN的能效。

  ​	**展望：**这些进展突显了我们方法在高性能并行计算应用中革命性推动SNN部署的潜力。

* **介绍：**

  **之前的研究（串行处理）**：通过采用将训练好的人工神经网络（ANNs）转换为脉冲神经网络（SNNs）[6]–[8]、应用直接训练与替代梯度以应对脉冲的不可微特性[9]、[10]，或结合生物学上合理的机制来指导或辅助SNN训练[11]–[13]等策略，SNN在处理复杂网络和任务方面展现了显著的能力。这导致SNN与ANN之间的性能差距逐渐缩小[14]–[16]。

   **现在研究：（可学习的参数以及并行计算）** 主要集中在脉冲神经元模型的精细化改进上，特别是广泛使用的漏积分发放（LIF）神经元，旨在显著提升计算模型在这一领域的性能。这包括以可学习的方式调整特定的神经元属性，如泄漏系数[17]和阈值[18]。SPSN [19] 旨在通过随机化神经元输出来并行化脉冲神经元中的计算，而PSN [20] ==上一篇文章== 则绕过重置过程，使用权重参数进行训练。

  **当前问题：** 尽管有这些进展，目前的并行SNN模型仍面临限制。它们在处理复杂任务时效果不佳，或者表现出更高的发放率，从而削弱了SNN固有的低能耗计算优势。

  **本文贡献： **在本文中，我们提出了一种创新的方法——并行脉冲单元（PSU），以及其高级变体：输入感知Input-aware PSU（IPSU）和Reset-aware 重置感知PSU（RPSU），每种变体都具有独特的可学习参数。

* **方法：**

  1. Vanilla LIF Neuron    Vanilla  香草 ----计算机领域翻译为基础的，标准的
     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804113113962.png" alt="image-20240804113113962" style="zoom:67%;" />

  2. Parallel Spiking Unit  并行脉冲单元
     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804113228839.png" alt="image-20240804113228839" style="zoom:67%;" />
     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804113354770.png" alt="image-20240804113354770" style="zoom:67%;" />

     

     然后加入重置过程，论文考虑的是软重置的过程，查看公式（3），可以看到，当加入重置过程后，我们其实是多了 $$-V_{th}S[t]$$这一项，在每次迭代过程中，看公式1中，$$V[t-1]$$ 每次都需要乘以 $$1-τ/1$$, 所以我们多处的这一项的就需要每次都乘以 $$1-τ/1$$, 展开如下：

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804113907135.png" alt="image-20240804113907135" style="zoom:67%;" />

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804113916843.png" alt="image-20240804113916843" style="zoom:67%;" />

     这样一来，他把这个重置过程融合到公式1中，其实就是公式3和公式1的融合，然后需要注意的就是本身公式3使用的S和实际的脉冲S并不是等价的，两者应该是差了一个时间单位，因为公式3使用的S是当前时间t,但公式2计算出的S是t+1时刻的。

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804114637044.png" alt="image-20240804114637044" style="zoom:67%;" />

     考虑到需要并行计算，那我们急需要将BS中的S进行替代

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804114748649.png" alt="image-20240804114748649" style="zoom:67%;" />

     σ()，为sigmoid函数，函数里面的A可以和前面的A一样

  3. Input-aware and Reset-aware PSU      输入感知和重置感知的PSU

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804114956641.png" alt="image-20240804114956641" style="zoom:67%;" />

     这里本质上就是两个A矩阵的不同。

     ![image-20240804115113661](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804115113661.png)

     PSU、IPSU 和 RPSU 的比较。它们都使用因果掩蔽技术来掩蔽未来时间步对当前时刻的影响。RPSU 从更好的脉冲估计的角度参数化学习重置过程中的矩阵 AR。而 IPSU 则从更好的泄漏积分的角度参数化学习负责泄漏积分的矩阵 AI。这两者都旨在在并行化过程中保留重置过程的特性，并通过参数化学习更好地适应数据。

* **试验：**

  1. 实验步骤
     我们采用完整的SNN计算框架BrainCog [37] 来验证所提出的PSU的有效性。在四种类型的数据集上进行了广泛的实验，包括静态图像（CIFAR10）、序列图像（Sequential CIFAR10和CIFAR100）、DVS（DVSCIFAR10）和语音数据集（SHD），如图3所示。我们利用各种模型配置来适应不同的数据输入，并确保公平比较。我们为静态图像数据集采用了类似于[38]中VGG模型的CIFARNet。对于序列CIFAR数据集，我们使用了一个6层的一维卷积结构，并辅以两个全连接层进行输出解码。在DVS数据集的情况下，我们遵循了[39]的方法，使用了类VGG模型，具体为VGGSNN。我们实现了一个四层的全连接网络来对SHD数据集进行分类。采用直接编码以提高训练效率，并将输出神经元的阈值设为无限大。为了优化SNN的训练并提高模型的泛化能力，我们实现了反正切（Atan）[17] 和分段线性（PWL）[40] 函数作为替代梯度函数。这解决了反向传播训练中脉冲的非可微分特性所带来的挑战。此外，我们采用了余弦退火策略来调整学习率，并结合了标签平滑技术以提高性能。与我们的方法相关的额外参数在表I中列出。CE和MSE分别表示交叉熵损失和均方误差损失。而AdamW和SGD则分别指代AdamW优化器和随机梯度下降优化器。

  2. 仿真速度

     本研究的主要目的是通过并行化脉冲神经元的动态过程（包括训练和测试阶段）来提高仿真速度。图4展示了我们提出的并行脉冲神经元模型与传统的LIF模型、PLIF和SPSN在SHD和DVS数据集上的比较仿真速度。我们考察了五种不同的时间步设置：4、8、16、32和64，测量完整数据集上进行十个周期所需的时间，以导出平均值。图中的纵轴表示当前神经元模型与相同配置的LIF神经元在训练和测试过程中的速度比。值得注意的是，虽然SPSN的仿真速度明显更快，但其训练稳定性令人质疑，尤其是在像DVSCIFAR10这样的数据集上，其测试精度仅为12%，如图4(b)所示。因此，SPSN被排除在进一步的实验之外。我们的结果表明，与原生LIF模型相比，仿真速度有显著提升，这预计将显著提高SNN在优化和部署中的效率。尽管IPSU和RPSU包含可学习参数，但它们在训练和测试过程中的推理时间大致相等。在图中，对于相同的时间步，训练过程中的时间比纯推理过程中的时间要低，因为反向传播阶段占据了总计算时间的更大比例。值得注意的是，在推理阶段，PSU相比FCNet实现了更高的VGGSNN加速，这归功于前者更深的层结构，更能从完全并行计算中获益。

     ![image-20240804155517823](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804155517823.png)

  3. 神经元活动分析

     ![image-20240804161032794](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804161032794.png)

  4. 与之前工作的比较

     ![image-20240804160946732](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804160946732.png)
     
     ![image-20240805102005923](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805102005923.png)

* **讨论与总结：** 从结果看，其实本文的工作可能并不是非常有意义，加上重置机制可能并不一定能给网络准确率的提升带来多大，从各个论文研究的方向上不难发现，修改脉冲神经元并不是一项简单的工作。





# 主题：Accelerating Spiking Neural Networks with Parallelizable Leaky Integrate-and-Fire Neurons
翻译：并行化LIF加速SNN(的训练以及推理)

谢布克大学     这篇文章写于2024年2月27

* **摘要：**  

  1. **背景：**脉冲神经网络（SNNs）在生物学上的合理性更高，能够有效学习时空特征，而消耗的能源比传统的人工神经网络（ANNs）更少，尤其是在神经形态硬件上。
  2. **问题：**LIF神经元是深度学习中最广泛使用的脉冲神经元之一。然而，其顺序信息处理导致在处理长序列时训练速度较慢，这对依赖大规模数据集的实际应用构成了重大挑战。
  3. **工作：**本文提出可并行化LIF（ParaLIF）神经元，通过在时间上并行化，从而加速 SNNs 的训练，适用于前馈和递归结构。
  4. **结果：** 在神经形态语音、图像和手势分类任务中，ParaLIF相比LIF显示出最高可达200倍的速度提升，并且在相似的稀疏性下，平均准确率更高。集成到一种最先进的架构中，ParaLIF在SHD数据集上达到了很高的准确率。
  5. **意义：** 这些发现突显了ParaLIF可用于快速、准确且节能的SNNs开发，特别适合处理包含长序列的海量数据集。

* **介绍：**

  1. **引言：** 在本文中，我们介绍了ParaLIF神经元，通过在时间上并行化其仿真来加速SNNs。ParaLIF是通过将传统 LIF 神经元的操作分离为一个用于输入积分的线性组件和一个用于放电生成的非线性组件。使用类似于 LMU 的并行化的技术[9]，线性组件在时间上进行并行化，以生成神经元膜电位。非线性组件为一个独立应用于膜电位的脉冲编码过程，因此是可并行化的。此外，我们提出了一种基于ParaLIF的可并行化递归架构，并进行了深入实验以比较LIF和ParaLIF在多种神经形态任务中的表现。

  2. **相关工作的介绍：** 将SNN模拟分解为三个主要部分：积分微分方程、传播脉冲到目标神经元、以及应用脉冲对目标神经元的影响。对微分方程的数值积分进行并行化被证明是简单的，而对脉冲传播进行并行化则更具挑战性。为了解决这个问题，提出了两种策略：一种是对神经元进行计算并行化，另一种是对突触事件进行并行化[10]。

  3. **最近工作介绍：** 作者先前的文章[8]中提出的随机可并行脉冲神经元（==SPSN==）探索并展示了脉冲神经元随时间并行化的有效性。文献[16]提出的并行脉冲神经元（==PSN==）沿用了类似的路径，消除了传统脉冲神经元中的重置组件。作者将神经元动力学重新表述为非迭代形式，实现了高度加速的并行模拟。与之前的SPSN相比，PSN使用确定性脉冲生成，不使用重置函数，这在概念上与LIF有所不同。LIF的时间常数参数被一组可学习的权重所替代，每个时间步对应一个权重。因此，PSN需要更多的权重和固定的输入长度。为了解决这个问题，作者引入了两个PSN变体：masked PSN，它通过掩蔽未来输入来实现即时的隐藏状态生成，以及sliding PSN，运用时间不变参数高效处理可变长度的输入序列，类似于卷积。

     本文提出的ParaLIF神经元是对之前提出的==SPSN[8]==的扩展。虽然保留了用于输入积分的线性组件，但我们结合了更广泛的脉冲函数，包括随机和确定性的。同时，本工作的贡献还包括启用可并行递归架构，并对多样化的数据集进行全面评估，包括音频、图像和手势分类，以展示所提方法的广泛应用潜力。

* **方法：**

  1. LIF神经元介绍

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804180056283.png" alt="image-20240804180056283" style="zoom:67%;" />

     离散化之后，公式表达式变为：

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804185427027.png" alt="image-20240804185427027" style="zoom:67%;" />

     

  2. Parallelisable Leaky Integrate-and-Fire    可并行的LIF神经元

     为了克服LIF神经元阻止并行化的限制，我们将其计算分为两个独立的组件：一个线性泄露积分（linear Leaky Integrator，LI）神经元和一个瞬时非线性放电函数（an instantaneous non-linear spiking function， 见图1-D）。这两个组件之间的独立性使得我们必须放弃膜电位的重置，从而导致对方程(6)的修改，如下所示：

     

     ![image-20240804210307099](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804210307099.png)

     ![image-20240804185827971](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804185827971.png)

     其实就是去掉了重置过程。

     * Leaky Integrator 膜电位

       写出公式7的一般化形式，得到公式8

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804212740753.png" alt="image-20240804212740753" style="zoom:67%;" />

       设$$m[0]=0$$, 可以看到每次都需要乘以a，得到以下表达式：

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804212834294.png" alt="image-20240804212834294" style="zoom:67%;" />

       定义 <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804212952278.png" alt="image-20240804212952278" style="zoom:50%;" />，可以得到公式10：

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804213031688.png" alt="image-20240804213031688" style="zoom:67%;" />

       上面这个公式10是n时刻的膜电压，我们如果想要计算所有时刻的膜电压，就可以通过$$h和v$$的卷积得到：

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804213330779.png" alt="image-20240804213330779" style="zoom:67%;" />

       利用卷积定理，等式11可以变为：（傅里叶变换）

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804213839330.png" alt="image-20240804213839330" style="zoom:67%;" />

       既然公式8写成了这种形式，那么公式5和7就可以写为：

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804215212330.png" alt="image-20240804215212330" style="zoom:67%;" />

       其中 <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804215334967.png" alt="image-20240804215334967" style="zoom:67%;" />  $$k = [β^{n-1}(1-β), β^{n-2}(1-β),......,β^0(1-β),]$$​

       公式13和公式14可以被融合为下面的公式：

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804215716711.png" alt="image-20240804215716711" style="zoom:67%;" />

       根据等式(15)，LI膜电位可以以并行的方式进行计算。此外，由于ℓ和k是常数参数，因此F{k}·F{ℓ}可以预先计算。

     * Firing Functions & Gradient Computation  脉冲发放函数以及梯度计算

       考虑到放电任务涉及将膜电位编码为脉冲，可以探索多种方法来实现这一目标。我们的工作重点关注两种随机函数和两种确定性函数，以及它们的一些变体和梯度计算策略。所有提出的放电过程都可以并行应用，在每个时间点独立进行。

     * Stochastic Firing Functions  随机发放函数

       1. 第一个随机放电函数是Gumbel-Softmax方法[23]，结合本文的神经元，将这种神经元称为“ParaLIF - Gumbel-Softmax”（ParaLIF-GS）。  该方法提供了一种高效的梯度估计器，替代了对分类随机变量的不可微分采样，用可微分的样本进行替换。它还可以通过将伯努利分布解释为具有两个类别P和1−P的分类分布来作用于伯努利变量。这样一来，便克服了通过包含分类潜变量的层反向传播梯度的能力限制。 ==没有给出具体的操作方法，需要查看原文==

       2. 第二种用于生成脉冲的随机方法称为Sigmoid-Bernoulli方法，看公式16和公式17：

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804224807532.png" alt="image-20240804224807532" style="zoom: 50%;" />

       ​	膜电位经过σ()函数后为一个概率分布，接着使用伯努利分布进行采样，得到脉冲。之后因为s[n]是个采样过程，无法微	分，这篇文章就是用公式16得到的概率分布来近似公式17的求导值了。
       ​	将该神经元称为“ParaLIF - Sigmoid-Bernoulli”（ParaLIF-SB）

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240804230447322.png" alt="image-20240804230447322" style="zoom:67%;" />

       ​	该方法的一个变体就是将公式16变为 $$ρ[n] = ReLU(tanh(u[n]))$$

     * Deterministic Firing Functions  确定性的发放函数

       1. 第一个确定性方法被称为“ParaLIF - Delta”（ParaLIF-D），它使用增量调制编码[25]。当当前时间步的膜电位与前一个时间步的变化超过预定义的阈值$$∆_{th}$$时，就会生成一个脉冲 。
       
       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805080048014.png" alt="image-20240805080048014" style="zoom: 67%;" />
       
       ​	对于该函数，需要使用文献[21]中的梯度替代。此外，还有ParaLIF-D的两个变体，即“ParaLIF-SD”（在编码之前对膜电	位应用sigmoid函数）和“ParaLIF-TD”（在编码之前对膜电位应用双曲正切函数）。
       
       2. 第二个确定性方法被称为ParaLIF - Threshold”（ParaLIF-T）,它的基本原理是在膜电位超过预定义的脉冲阈值时生成脉冲，这相当于LIF神经元的脉冲函数（公式(3)）。因此该方法没有类似于LIF神经元的重置过程。类似地，在编码之前对膜电位应用sigmoid或双曲正切函数会导致两个变体，即“ParaLIF-ST”和“ParaLIF-TT”。 ==文章没有给具体公式，只是说和公式3相似==
       
          <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805082713824.png" alt="image-20240805082713824" style="zoom:67%;" />
       
          如图2所示，ParaLIF-GS、ParaLIF-SB和ParaLIF-T对输入信号的幅度进行了编码。它们的脉冲列与LIF神经元的脉冲列非常相似。然而，随机神经元ParaLIF-GS和ParaLIF-SB即使在输入信号较低时也有可能生成异常脉冲。相比之下，ParaLIF-D则以编码输入信号导数的幅度，确切来说是正部分，来区分于其他方法。

  3.  Recurrent Architecture  循环结构

     递归神经网络（RNNs）擅长学习时间依赖关系，在多种任务中超越了前馈架构，尤其是在处理序列和时间序列数据时 [26]。利用这些优势，递归架构也在脉冲神经网络（SNN）文献中得到了应用 [7]。在这一部分中，我们展示了使用LIF的递归脉冲神经网络（RSNN）实现，并提出了我们可并行化的RSNN架构方案。

     * Recurrent LIF    循环LIF 
       将LIF神经元的公式5结合 上一步的S[n], 即可变为循环LIF，如公式20：   （在图1的b和c中也可以看出）

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805083553700.png" alt="image-20240805083553700" style="zoom:67%;" />

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805083420795.png" alt="image-20240805083420795" style="zoom:67%;" />

     * Recurrent ParaLIF  训练的并行LIF

       看图1-E和1-F，在PataLIF中引入了一个隐藏状态H[t]，其用公式表达为：

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805084724812.png" alt="image-20240805084724812" style="zoom:50%;" />

       公式22的引申就是 $$h[1:n] = Φ(u_h[1:n])$$,  逐元素计算

       公式21就是公式15的形式，只不过为隐藏层的膜电压，加下来计算实际输出的膜电压以及脉冲：

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805085052712.png" alt="image-20240805085052712" style="zoom:50%;" />

       可以看到最大的区别就是加入了隐层藏输出的脉冲以及 膜电压。

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805092215245.png" alt="image-20240805092215245" style="zoom:67%;" />

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805095956807.png" alt="image-20240805095956807" style="zoom:67%;" />
       
       

* **试验：**

  1. 评价指标
     分类准确率；脉冲频率(表示网络每毫秒生成的平均脉冲数)； 训练时间（相同数据集上跑完一个batch的平均时间）；
     加速因子(LIF神经元训练时间与ParaLIF训练时间之间的比率)

  2. 任务

     语音分类：the Spiking Heidelberg Digits (SHD) [27] and the Neuromorphic-TIDIGITS (N-TIDIGITS) [28] datasets. 

     图像分类： N-MNIST [30] dataset which is a neuromorphic version of MNIST. 

     手势识别：the DVSGesture dataset [31]

  3. 网络架构

     对于不同的数据集使用了不同的网络

     对于SHD数据集，使用了两个网络：3个全连接层，每层有128个神经元的前馈SNN ；  一个包含256个神经元的RSNN

     对于N-TIDIGITS数据集，使用的网络是SHD中的RSNN

     对于N-MNIST和DVSGesture数据集，使用了一个网络：2个全连接层，每层有128个神经元的前馈SNN ；  

     **在这些网络中**，均包含了具有C个LI神经元的完全连接输出层，其中C为类别的数量。这个输出层并不会生成脉冲。

     下面是损失函数，其中 1是指示函数，y是目标类别，T是时间步长的数量，ui是神经元i的膜电位。（交叉熵损失）

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805094019300.png" alt="image-20240805094019300" style="zoom:67%;" />

     意思就是当 i = y的时候，计算当前的损失值，默认第i个神经元输出的就是第i个类别。当i !=y 的时候，就是0。我们最终就是看第i神经元输出的是1，就认为当前网络的输出类比是i。 ==如果存在多个呢？==

  4. 结果

     ![image-20240805100148956](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805100148956.png)

     该图上看的话，脉冲发放率低，不过的迭代次数，且为脉冲神经元为ParaLIF-SD时具有更好的表现。（==注意网络比较简单==）

     ![image-20240805100200824](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805100200824.png)

     分析脉冲发放率对准确率的影响

     ![image-20240805100058571](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805100058571.png)

     在数据集行的比较

     ![image-20240805101248552](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805101248552.png)

     与当前SOTA模型的比较

  

* **个人总结：**  看完这篇文章，感觉作者工作的意义并不是很大，反而第一篇并行SNN简洁且具有借鉴意义，唯一缺点就是没有重置函数。











# Spikformer: When Spiking Neural Network Meets Transformer

Transformer的脉冲形式， 这是V1版本（ICLR会议），后续还有一个V2版本（在投状态，其中作者与通讯作者包括了SEW-ResNet的第一作者和通讯作者，通讯作者也是北大鹏程实验室，田永鸿教授）。
作者：北京大学，鹏程实验室，第一作者：zhou zhao kun，指导教师：袁粒，北大助理教授，鹏程实验室。 

**International Conference on Learning Representations**

* **摘要：**  

  ​	==文章主要创新点：== 这篇文章将脉冲神经网络（SNN）和自注意力机制相结合。

  ​	==每个关键点的作用：== 前者提供了一种高效能和事件驱动的深度学习范式，而后者则能够捕捉特征依赖关系，使得Transformer能够实现良好的性能。探索它们之间的结合在直觉上是有前景的。

  ​	==具体过程：== 在本文中，我们考虑利用自注意力机制的能力和SNN的生物特性，提出了一种新颖的脉冲自注意力机制（SSA）以及一个强大的框架，名为脉冲Transformer（Spikformer）。Spikformer中的SSA机制通过使用脉冲形式的查询、键和值来建模稀疏的视觉特征，而无需使用softmax操作。由于其计算是稀疏的并且避免了乘法运算，SSA非常高效且能耗低。

  ​	==实验结果：== 研究表明，带有SSA的Spikformer在神经形态和静态数据集上的图像分类任务中可以优于当前最先进的类SNN框架。Spikformer（6630万参数）与SEW-ResNet-152（6020万参数，69.26%）相当，使用4个时间步即可在ImageNet上达到74.81%的top1准确率，这在直接训练的SNN模型中处于领先地位。代码将会在Spikformer上开源。

* **介绍：**

  ​	Transformer网络核心为 自注意力机制( Self-attention)
  ​	==简要解释Self-attention:== 首先通过计算浮点形式的Query和Key的点积来获得矩阵；然后采用softmax进行归一化，其中包含指数计算和除法运算，以产生用于加权Value的注意力图。 VSA（Vanilla Self-attention 经典自注意力）中上述步骤与SNNs的计算特性，即避免乘法操作，不符合。此外，VSA较大的计算开销几乎禁止了直接将其应用于SNNs。因此，为了在SNNs上开发Transformer，我们需要设计一种新的有效且计算高效的自注意力变体，能够避免乘法操作。

  <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805152911900.png" alt="image-20240805152911900" style="zoom:67%;" />

  ​	作者提出的SSA不使用softmax函数，且SSA中脉冲形式的Query、Key和Value的计算避免了乘法操作，可以通过逻辑AND操作和加法完成。该过程可以被简化为 图b中的 1和 2， 当序列长度大于单个头部特征维度时，这种简化可以进一步降低了计算复杂度。工作的贡献可以被总结为以下三点：

  1. 我们为SNNs的特性设计了一种新颖的脉冲形式自注意力机制，称为脉冲自注意力（SSA）。使用稀疏的脉冲形式Query、Key和Value而不使用softmax，SSA的计算避免了乘法操作且高效。
  2. 我们基于提出的SSA开发了脉冲Transformer（Spikformer）。据我们所知，这是第一次在SNNs中实现自注意力。
  3. 大量实验证明，所提出的架构在静态和神经形态数据集上均优于现有的SNNs。值得注意的是，我们首次使用直接训练的SNN模型，在ImageNet数据集上以4个时间步达到了超过74%的准确率。

  ==这些架构有个共同点就是：需要的时间步并不是特别大，仅仅需要四个时间步，但是时间步这个参数在数据集非常小的时候体现并不明显，当数据集非常大，例如ImageNet2012数据集时，就会大大增加显存的占用以及计算参数，单GPU训练可能并不是很可以。==

  * 相关工作

    1. Vision Transformers

       对于图像分类任务，标准的视觉Transformer（ViT）包括一个分块模块、Transformer编码器层（或多层）以及线性分类头部。Transformer编码器层由自注意力层和多感知层块组成。自注意力是使ViT成功的核心组件。通过查询和键的点积以及softmax函数加权图像块特征值，自注意力能够捕捉全局依赖和兴趣表示（Katharopoulos et al., 2020; Qin et al., 2022）。一些研究工作已经进行了改进ViT的结构。

       使用卷积层进行块分割已被证明能加速收敛并缓解ViT的数据需求问题（Xiao et al., 2021b; Hassani et al., 2021）。

       有些方法旨在减少自注意力的计算复杂度或提高其建模视觉依赖关系的能力（Song, 2021; Yang et al., 2021; Rao et al., 2021; Choromanski et al., 2020）。

       ==本文专注于探索自注意力在SNNs中的有效性，并开发一个强大的脉冲Transformer模型用于图像分类。==

    2. Spiking Neural Networks.

       与传统的深度学习模型使用连续的十进制值传递信息不同，脉冲神经网络（SNNs）使用离散的脉冲序列来计算和传输信息。脉冲神经元接收连续值并将其转换为脉冲序列，包括漏积分-放电（LIF）神经元（Wu et al., 2018）、PLIF（Fang et al., 2021b）等。

       有两种方式来构建深度SNN模型：ANN转SNN转换和直接训练。

       **在ANN转SNN转换**中（Cao et al., 2015; Hunsberger & Eliasmith, 2015; Rueckauer et al., 2017; Bu et al., 2021; Meng et al., 2022; Wang et al., 2022），通过将ReLU激活层替换为脉冲神经元，将高性能预训练的ANN转换为SNN。
       转换后的SNN需要较长的时间步来准确逼近ReLU激活函数，这导致较大的延迟（Han et al., 2020）。

       **在直接训练领域**，SNN在模拟时间步骤内展开，并通过时间反向传播进行训练（Lee et al., 2016; Shrestha & Orchard, 2018）。
       由于脉冲神经元中的事件触发机制不可微分，需要使用替代梯度进行反向传播（Lee et al., 2020; Neftci et al., 2019）。Xiao等人（2021a）采用隐式微分在平衡状态上训练SNN。已将各种ANN模型转移到SNN中。然而，目前关于SNN上的自注意力研究尚空白。Yao等人（2021）提出了时间注意力以减少冗余时间步。张等人（2022a;b）均使用ANN-Transformer处理脉冲数据，尽管标题中有“Spiking Transformer”。Mueller等人（2021）提供了一个ANN-SNN转换Transformer，但仍然是纯粹的自注意力，不符合SNN的特征。

       ==本文将探索在SNN中实现自注意力和Transformer的可行性。==

       本文使用的神经元是 LIF，公式仍然如下：

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805165620229.png" alt="image-20240805165620229" style="zoom:67%;" />

       

       

* **方法：**本文的框架图如下图所示：

  <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805171153790.png" alt="image-20240805171153790" style="zoom:67%;" />

  1. Overall Architecture 整体架构

     公式表达Spikformer:

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805175859788.png" alt="image-20240805175859788" style="zoom:67%;" />

     公式5使用的就是LIF神经元。

     In the neuromorphic dataset the data shape is  $$I ∈ R^{T×C×H×W}$$, where T, C, H, and W denote time step,
     channel, height and width, respectively. A 2D image $$I_s ∈ R^{C×H×W}$$​ in static datasets need to be repeated T
     times to form a sequence of images.

     使用静态图片时，需要将数据复制T次，使得数据增加一个维度T

  2. Spiking Patch Splitting 脉冲patch划分  （SPS）

     对SPS模块进行解释：

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805180625670.png" alt="image-20240805180625670" style="zoom:67%;" />

     MP就是max-pooling，SPS块可以使用多个，只需要保证最后的输出通道数为D即可，假设使用了4个SPS，那么就让通道数的输出依次为：D/8, D/4, D/2, D。

     这里可以说以下其中维度的变化，其中H，W经过卷积，池化后变为H1,W1,接着N = H1*W1，C经过卷积以后变为D，将N与D进行维度置换，就相乘新的维度了（T,N,D）

  3. Spiking self attention mechanism

     普通的自注意力机制表达式如下： 

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805184018469.png" alt="image-20240805184018469" style="zoom:67%;" />

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805184044601.png" alt="image-20240805184044601" style="zoom:67%;" />

     d = D/H, H是头的数量，将其中的浮点值  转换为 脉冲形式的V,  就可以在SNN中直接使用自注意力，

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805184340329.png" alt="image-20240805184340329" style="zoom:67%;" />

     说明一下该公式的弊端：1）$$Q_F, K_F$$ 的==浮点矩阵==乘法和包含指数计算和除法操作的softmax函数，不符合SNN的计算规则。
     2）VSA的序列长度的二次 空间和时间复杂度，不符合SNN高效计算的要求。

     接下来作者提出Spiking self attention(SSA)：

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805185044450.png" alt="image-20240805185044450" style="zoom:67%;" />

     然后为了控制矩阵相乘带来的较大的值，他们添加了一个缩放因子 $$s$$ ，

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805185526041.png" alt="image-20240805185526041" style="zoom:67%;" />

     这样看来整个过程并不是很复杂。 需要再每个时间步长单独运行，也就是for对T进行循环。
     作者再次解释说：由于输入的X以及中间产生的各种变量均为脉冲形式，这就导致其本身蕴含的信息量较少，没必要使用原始的自注意力, 直接使用它定义的这种形式更好。

     接着作者对这种形式与原始的自注意力形式进行了量化的比较：

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805192245965.png" alt="image-20240805192245965" style="zoom:67%;" />

     $$A_I$$ 表示直接将浮点数的Q和K进行相乘，不使用其他函数
     $$A_{ReLU}$$ 表示将使用 ReLU(K) * ReLU(Q)，这样一来得到的结果就没有负值了
     $$A_{LeakyReLU}$$ 是相比$$A_{ReLU}$$  是有负值的
     $$A_{softmax}$$  是 原始的自注意力
     $$A_{SSA}$$  是 本文所说的方法

     上表中 OPS是操作数量，包括加减乘除等
     $$P_{μJ}$$ 指的是理论上执行一次自注意力机制的能量消耗。
      这么对比下来，确实本文所提到SSA非常好
     

* **试验：**

  <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806094946398.png" alt="image-20240806094946398" style="zoom:80%;" />

  在ImageNet数据集上结果，其中 Spikformer-8-384 ，8指的是编码器的重复次数，384指的是词向量的维度，公式4中的D。
  可以看出，Spikformer还是非常节能的，相比于原始的Transformer网络以及其余学者提到的方法。
  使用了数据增强方法(Yuan et al., 2021a)
  batch_size = 128 or 256; 余弦衰减学习率的初始值为0.0005; 缩放因子为0.125（Imagenet 和 cifar）
  输入的图像首先被调整为 224*224，接着打成几个patch，196个 16`*`16的patch
  310个epoch

  cifar10/100

  <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240805221941642.png" alt="image-20240805221941642" style="zoom:67%;" />

  没有使用数据增强方法

  <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806100811092.png" alt="image-20240806100811092" style="zoom:67%;" />

  在这两个神经形态数据集中，时间步是比较长的，且batch_size被设置为16。
  学习率为0.1 并使用学习率衰减策略
  DVS数据集的训练epoch为200， CIFAR10-DVS的epoch为106

* **消融实验：** Ablation study

  <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806103855570.png" alt="image-20240806103855570" style="zoom:67%;" />

  作者提出的方法不仅具有更低的能耗，且准确率还是非常高。

* **附录：**

  1. Multihead spiking self attention  多头自注意力

     ​	在实际应用中，我们将$$Q，K，V ∈ R^{T×N×D}$$整形为多头形式$$R^{T×H×D×d}$$，其中 $$D = H × d$$. 然后，我们将Q，K，V分成H    部分，运行H个SSA操作，称为H头SSA。Multihead Spiking Self Attention（MSSA）的公式如下所示：

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806105609324.png" alt="image-20240806105609324" style="zoom: 67%;" />

  2. Spiking self attention and time step  脉冲自注意力和时间步

     对于SNN层，T是一个独立的维度，需要进行T步的循环，在其他层中，它会和batch_size进行合并。这和前面的论文SEW-ResNet是一样的做法

  3. Experiment Details  实验细节

     * 训练：与标准ViT不同，Dropout和Droppath不适用于Spikformer。我们在每个自注意和MLP块之前删除层归一化（layernorm），并在每个线性层之后添加批次归一化（batchnorm）。在所有Spikformer模型中，MLP块的隐藏维数是4 ×D，其中D是嵌入维数。代理梯度函数的sigmoid，$$α = 4$$

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806122803856.png" alt="image-20240806122803856" style="zoom:67%;" />

       对于DVS128 Gesture，我们在Q和K之后放置了一个1维最大池化层，以增加数据的密度，从而在16个时间步骤中将准确率从97.9%提高到98.3%。我们设置了在 $$QK^TV ∗ s$$ 后脉冲神经元层的阈值电压$$V_{th}$$为0.5，其他层设置为1。

     * Theoretical  Synaptic Operation And Energy Consumption Calculation  理论上的突触操作数量和能量计算

     突触操作计算：

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806123245493.png" alt="image-20240806123245493" style="zoom:67%;" />

     ​	在这里，$$l$$ 表示Spikformer中的一个块或层，$$f_r$$表示该层输入脉冲序列的发射率，T表示脉冲神经元的模拟时间步长。
     $$FLOPs(l)$$ 指的是第$$l$$个块的浮点运算，即multiply-and-accumulate（MAC）操作的数量。而SOPs则是spike-based accumulate（AC）操作的数量。

     能耗计算：

     ​	作者根据**（Kundu et al., 2021b; Hu et al., 2021b; Horowitz, 2014; Kundu et al., 2021a; Yin et al., 2021; Panda et al., 2020; Yao et al., 2022）**估算了Spikformer的理论能耗。我们假设MAC和AC操作是在45纳米硬件上实现的[12]，其中EMAC = 4.6pJ ， EAC = 0.9pJ。Spikformer的理论能耗计算如下：

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806125455803.png" alt="image-20240806125455803" style="zoom:67%;" />

     $$FL^1_{SNNCONV}$$ 是将图片编码为脉冲序列的第一层，之后是n个Conv层，m个FC层吗，$$l$$ 个SSA块，这里n从2开始，跳过了第一个Conv.

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806131704078.png" alt="image-20240806131704078" style="zoom:67%;" />

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806131758260.png" alt="image-20240806131758260" style="zoom:67%;" />

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806131840375.png" alt="image-20240806131840375" style="zoom:67%;" />

     

  4. Additional Results  

     * Query,Key and Value 的脉冲发放率

       如图四，这几个脉冲发放率很小，所以带来的SNN的稀疏计算

     * 在ImageNet上的损失和准确率

     * 在CIFAR数据集上的其他实验结果

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806132423425.png" alt="image-20240806132423425" style="zoom:67%;" />

     * 对自注意力变体在ImageNet上不收敛的分析

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806133229239.png" alt="image-20240806133229239" style="zoom:67%;" />

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806133010008.png" alt="image-20240806133010008" style="zoom:67%;" />

       如上图所示，因为采取的代理梯度是sigmoid函数，当输入值与阈值相差非常大的时候，梯度几乎为0，无法训练，作者收集了第一次训练后，使用不同策略产生的输出值中， 前三个得到的输出值差别过大。

     * 迁移学习

       我们将Spikformer模型迁移到下游的CIFAR数据集上。在ImageNet上预训练的Spikformer-4-384和Spikformer-8-384/512进行了60个epochs的微调。CIFAR的输入尺寸为224 × 224。其余的超参数与直接在CIFAR上训练的模型相同。如表7所示，Spikformer展示出了很高的迁移能力。

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806133541810.png" alt="image-20240806133541810" style="zoom:67%;" />





# Spikformer V2: Join the High Accuracy Club on
ImageNet with an SNN Ticket

使用SNN的训练技巧使得网络在ImageNet上的准确率达到SOTA水平

第一作者：Zhaokun Zhou，北京大学 
通讯作者：Yuesheng Zhu,  IEEE, Senior Member
		    Shuicheng Yan, IEEE Fellow
		    Yonghong Tian, IEEE Fellow
		     Li Yuan，北大助理教授

* **摘要：**  简要介绍先前关于Spikformer的工作，接着开始说本次的改进部分。

  ​	==改进部分：==  此外，我们开发了 Spiking Convolutional Stem（SCS），并增加了补充卷积层来增强Spikformer的架构。增加了SCS的Spikformer被称为Spikformer V2。
  ​	    为了训练更大更深的Spikformer V2，我们在SNN内部引入了Self-Supervised Learning（SSL）。具体来说，受自监督Transformer的启发，我们通过掩码和重构的形式，对Spikformer V2进行预训练，然后在ImageNet上对其进行微调。广泛的实验证明，Spikformer V2优于先前的代理梯度训练和ANN2SNN方法。一个8层的Spikformer V2在4个时间步内达到了80.38%的准确率，经过SSL后，一个172M的16层Spikformer V2在仅1个时间步内达到了81.10%的准确率。据我们所知，这是SNN首次在ImageNet上实现了80%以上的准确率。

* **介绍：** 首先介绍背景，Spkiformer的相关工作，接着提出为何开发Spikformer V2。

  提高Spikformer的方法有两个：优化网络架构 和 改进学习方法。

  1. 优化网络结构

     * 删掉SPS中的最大池化层，作者说最大池化层会造成信息的丢失，且会模糊特征

     * 受到早期阶段在 ViT 中注入卷积诱导偏置的显著好处的启发，我们探讨了浅层卷积在Spikformer中的重要性以及卷积层数量对模型性能的影响，如图4所示。重新设计的patch分割模块称为 the Spiking Convolutional Stem（SCS），基于SCS的Spikformer命名为Spikformer V2。

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806164845557.png" alt="image-20240806164845557" style="zoom:67%;" />

     * 我们采用Mask Autoencoder风格对Spikformer V2进行预训练[5]。我们使用标准的ANN-Transformer作为解码器，以更好地重建被遮住了的图像，从而获得一个强大的Spikformer V2。需要注意的是，使用ANN-Transformer解码器并不影响Spikformer V2作为一个纯SNN的性质。在预训练后，解码器将被丢弃，只有Spikformer V2将用于后续的微调。

  2. Background and Related work

  3. Spikformer的介绍

* **方法：**  Spikformer V2

  本节提出了一种Spiking Convolutional Stem (SCS)  去替代先前提到的SPS，目的是为了解决这两个问题：模糊的脉冲块特征和无法进行掩码预训练。

  1. Spiking Convolutional Stem (SCS)  脉冲卷积主干

     作者团队实验发现：经过自监督学习和微调后的Spikformer使用SPS模块的结果低于有监督训练的结果（75.23% 对 78.23%）。他们将这一现象归因于两个主要原因：1) SPS由四个‘Conv-BN-LIF-MP’块组成，每个块包含一个最大池化层。连续的最大池化往往导致脉冲特征中信息的显著丢失，并在SPS块映射后导致特征模糊。另一项研究[29] 表明，SPS可能导致梯度反向传播错误。2）在早期的图片分割模块中，堆叠卷积可以大大提高优化稳定性并增强峰值性能[36]。尽管每个SPS块中都有一个卷积层，但仍不足以充分优化模型的性能。

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806182303510.png" alt="image-20240806182303510" style="zoom:67%;" />

     如上图所示，移除最大池化层，且在SCS中加入线性层会导致性能急剧下降，而加入卷积层（卷积核大小为3，步幅为1，填充为1）则会提升性能。为了平衡性能和效率，每个SCS块中引入了两个额外的卷积层。

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806182333664.png" alt="image-20240806182333664" style="zoom:67%;" />

     该图显示了SPS模块与SCS模块的不同，第一点就是更换max-pooling 为 conv2d+bn , 第二点图中无法体现出来，具体为在第一个卷积将特征维度扩大一个比率，在第二个卷积中再还原，模仿了MLP的操作。

  2. Self-supervised Pre-training  自监督预训练

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806184235487.png" alt="image-20240806184235487" style="zoom:67%;" />

     如上图所示，作者在实验中发现随着参数量的增加，网络出现了退化现象，所以就有了Spikeformer V2 + SSL

     ==重点：== 自监督学习（Self-Supervised Learning, SSL）中的掩码自编码机制[5]可以有效地训练深度和大型模型，并进一步提升模型性能。**[5]  需要看一下第五篇引文**    它采用解耦的编码器-解码器架构，通过使用解码器重建被掩码的图像，从而增强编码器的表示能力。

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806185501381.png" alt="image-20240806185501381" style="zoom:67%;" />

     自监督预训练 Spikformer V2 使用掩码自编码框架。SCS 脉冲 tokens 通过高掩码比率随机掩码。Spikformer V2 对可见图像patch 进行特征建模并输出脉冲token，而 Transformer 使用浮点掩码token和可见token重建图像。为了防止信息从掩码补丁中泄露，我们在 SCS 中采用了稀疏卷积和稀疏批归一化，遵循了 MC-MAE [74] 和 SparK [75] 中的方法。Transformer 是一个轻量级网络（3.41M），在预训练后被弃用。

     作者说他的这种方法代表了第三种方法：继 ANN to SNN 和 ANN 蒸馏到 SNN 技术之后，通过利用 ANN 来增强 SNN

     * Masking

       首先是对经过SCS分割的N个大小为$$(H/16) × (W/16)$$的path实施随机抽样策略进行掩码操作，尽管Spikformer V2的 patch 以脉冲的形式存在，我们观察到高掩码率（例如75%）仍然适用。这表明，即使在稀疏的脉冲信号中，仍然存在冗余信息，为未来高效SNN训练的探索提供了见解。

       关于SCS，我们采用与MC-MAE [74]类似的方法来获取相应的掩码。具体而言，我们分别将生成的掩码M上采样8倍、4倍和2倍，以作为第一个、第二个和第三个SCS块的掩码。第四个SCS块可以直接利用掩码M。

     * Spikformer V2 encoder

       Spikformer V2编码器。编码器使用了包含SCS的Spikformer V2，如第4.1节所述。在预训练过程中，编码器仅处理可见patch 。除了使用高掩码率外，我们进一步通过将时间步长设置为1来提高Spikformer的预训练效率。这确保了在预训练期间重建足够具有挑战性，这对于下游微调任务是有益的 [5]、[74]、[75]。

     * Transformer decoder

       我们采用轻量级的ANNTransformer作为解码器，它在图像重建能力上优于SNN解码器。解码器在预训练后被丢弃，这并不影响Spikformer V2作为纯SNN的事实。如图6所示，解码器接收一整套Token，包括编码的可见path和掩码token。掩码token是从一个共享的可学习向量复制而来的，作为缺失patch需要被预测的指示符。位置编码需要使用完整的，以确保掩码token可以获得关于其在图像中空间位置的信息。

     * Training Cost

       我们的动机是探索SNN中的自监督学习，并持续提升大规模Spikformer V2的性能。此外，我们还观察到它在降低训练成本方面的优势。我们比较了监督训练和预训练后的微调的训练成本，发现它可以在保持稳定训练和性能提升的同时实现训练加速，这在表3中展示了出来。随着模型参数大小的增加，相较于直接监督训练，SSL的训练加速比变得更加显著。值得注意的是，虽然在ANN中，预训练通常涉及超过800个epoch，但我们发现即使在SNN中使用较少的预训练epoch，也能在ImageNet上获得良好的微调结果。而且，微调性能随着预训练epoch的增加而提高，如表7所示。

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806210158699.png" alt="image-20240806210158699" style="zoom:67%;" />

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806210344458.png" alt="image-20240806210344458" style="zoom:67%;" />

* **试验：**

  ​	在本节中，我们首先评估了所提出的Spikformer和Spikformer V2在大规模图像分类数据集ImageNet [19]上的性能。接着，我们对Spikformer V2中融入的结构增强以及通过自监督学习（SSL）预训练所取得的性能提升进行了分析。此外，我们还展示了Spikformer在小规模静态数据集上的表现。进一步地，我们评估了Spikformer在神经形态数据集上的分类性能。最后，我们在本节的结尾部分给出了消融实验和部分可视化结果。

  1. Supervised learning experiments on ImageNet

     ![image-20240806211134650](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806211134650.png)

  2. Self-Supervised Learning on ImageNet.

     ​	为了进一步提高Spikformer V2的性能，并解决其模型规模限制的问题，作者使用masked 图像建模的自监督预训练引入Spikformer V2。

     ![image-20240806211222140](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806211222140.png)

  3. Small datasets classification

     和Spikformer一样了，展示的是Spikformer的结果，其说明了Spikformer V2 在小数据集上的性能是相较于Spikformer变低了，因为SCS是为大数据集准备的，而SPS是为小数据集准备的。

  4. Ablation Studies

     消融实验也是一样的，基本照搬 Spikeformer

* **讨论与总结：**





[Convolutional stem is all you need](https://zhuanlan.zhihu.com/p/385140954)    可能本篇文章的想法来自于这篇文章

![image-20240806160139196](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240806160139196.png)





* # Advancing Spiking Neural Networks towards Deep Residual Learning

  中文翻译： 推进脉冲神经网络向深度残差学习发展
  Yifan Hu,      Lei Deng, Member, IEEE,       Yujie Wu,      Man Yao,     Guoqi Li(通讯)， Member, IEEE
  第一作者是清华大学， 鹏程实验室
  2021年就发在了arxiv， 2024年发表在了 IEEE Transactions on Neural Networks and Learning Systems 期刊
  sew resnet 也是2021年就发在了arxiv， 且在2021年中了会议neurIPS
  
  [李国齐简介](https://mp.weixin.qq.com/s?__biz=MzI4NjcwODgyOA==&mid=2247487216&idx=1&sn=7938ef06a1067cda376fba558272a821&chksm=ebd98bc3dcae02d555339347eb89a5e2ae60b06e80d7e81cc82e3a41998f49ddbb5d33e531e1&scene=27)
  他和田永鸿教授合作开发了惊蛰框架
  下载的文献版本中没有关于代码的介绍。

  * **摘要：**  
  
    1. **背景：** 尽管神经形态计算取得了迅速进展，脉冲神经网络（SNNs）的容量不足和表示能力不足严重限制了它们在实际中的应用范围。残差学习和快捷方式被证明是训练深度神经网络的重要方法，但之前的研究很少评估这些方法对==基于脉冲的通信和时空动态特性的适用性==。 这种忽视导致了先前残差SNN中信息流受阻和随之而来的性能退化问题。
       - **Spike-based Communication** 是神经系统中信息传递的核心方式，通过脉冲信号来进行高效的交流和信息处理。
       - **Spatiotemporal Dynamics** 关注系统在空间和时间上的变化，这有助于理解神经系统如何动态地处理信息和适应环境。
    2. **本文工作：**为了解决这一问题，我们提出了一种新颖的 SNN-oriented 的残差架构，称为MS-ResNet，该架构建立了基于膜的短连接路径，并进一步证明通过引入块动态等距理论（block dynamical isometry theory），可以在MS-ResNet中实现梯度范数等式，这确保了网络在深度不敏感的情况下可以表现良好。
       - **Dynamical Isometry** 是一个用来描述矩阵在不同维度下如何保持某种几何性质的理论。在高维数据分析中，特别是在稀疏表示和低秩矩阵分解中，这种理论用于确保算法在高维数据空间中仍能有效地近似和恢复原始结构。
    3. **工作意义：** 因此，我们能够显著扩展直接训练的SNN的深度，例如，在CIFAR-10上达到482层，在ImageNet上达到104层，而没有观察到任何轻微的性能退化问题。
    4. **实验结果：** 为了验证MS-ResNet的有效性，我们在基于静态帧数据集和神经形态数据集上进行了实验。MS-ResNet104在ImageNet上达到了76.02%的优越准确率，这在直接训练的SNN领域中是我们所知的最高结果。同时也观察到很高的能效，每个神经元平均只需要一个脉冲即可对输入样本进行分类。

  * **介绍：**

    ​	具有丰富神经元动态和多样编码方案独特特性的脉冲神经网络（SNNs），代表了一种典型的仿脑计算模型。与传统的人工神经网络（ANNs）不同，SNNs能够通过时空动态编码信息，并利用异步二进制脉冲活动进行事件驱动通信。近年来神经形态计算的进展显示出它们在能效方面的巨大潜力。理论上，SNNs至少与ANNs一样具备计算能力，并且通用逼近定理也适用 SNNs。因此，不足为奇的是，SNNs已在各种应用任务中得到报道，如图像分类、目标检测和跟踪、语音识别、光流估计等。然而，实际中，缺乏强大的SNN模型严重限制了它们在复杂任务中的应用能力。

    ​	在本文中，我们首先说明了训练深度SNN时使用普通ResNet时的退化问题。我们观察到脉冲PlainNet和普通ResNet随深度增加的准确率改善趋势分别停在14层和令人惊讶的仅20层。为了解决这一退化问题并充分释放深度SNN的潜力，我们提出了面向SNN的残差架构MS-ResNet，其中去除了块间的LIF(·)，构建了一个干净的快捷连接，对整个网络施加了恒等映射。然后，我们从前向传播和梯度范数两个角度分析了新结构的优越性。观察到MS-ResNet可以避免推理中无效的残差表示，这指的是一个块的残差路径对网络的整体能力没有贡献。此外，我们证明了MS-ResNet可以通过块动态等距框架[32]实现梯度范数的平等，而普通的脉冲ResNet则不能。因此，我们的模型表现出了很好的可扩展性，并且可以至少扩展到在CIFAR-10上的482层和在ImageNet上的104层，而不观察到退化问题。

  * **准备工作和动机：** 

    1. 这项工作使用的是文献7中提到的LIF模型。

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808155445561.png" alt="image-20240808155445561" style="zoom:67%;" />

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808155524824.png" alt="image-20240808155524824" style="zoom:67%;" />

       根据ANN中的 {Conv-BN-Nonlinearity} 这种架构，作者参照文献24提出的TBDN方法，将膜电位公式修改为以下：

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808155539933.png" alt="image-20240808155539933" style="zoom:67%;" />

       其中$$μ_{c_i}和σ^2_{c_i}$$ 分别是按照每一个维度的通道平均值和方差。

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808155801603.png" alt="image-20240808155801603" style="zoom:67%;" />

    2. The Degradation Problem in SNNs  脉冲神经网络的退化问题

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808161602523.png" alt="image-20240808161602523" style="zoom:67%;" />

       其中W/O LIF(.) 表示的是将图一脉冲残差块最后对残差操作的LIF神经元进行去除，其余保持不变。基于表2，作者分析转换后的脉冲残差网络之所以出现退化问题主要是因为 LIF神经元的存在。

    3. Spiking residual blocks 脉冲残差块

       这一节作者开始解释为什么Vanilla ResnNet中的LIF神经元会出现退化问题并介绍他们所提模型的优越性。

       首先展示一下作者提出的残差块（==感觉属实有点牵强==）

       ![image-20240808162535796](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808162535796.png)

       短连接中的信息流动在概念上更接近于神经元膜的突触输入总和，而不是原始结构中的脉冲活动，因此我们将新结构命名为Membrane-Shortcut ResNet（MS-ResNet），以突出快捷路径的变化。  （==因为最后求和的内容不是脉冲与脉冲的相加，而是实数与实数的相加，所以作者说更接近突触输出总和，这里我认为还是牵强的解释==）

       * **Residual Representation and Workload Balance at Inference  推理时的残差表示和工作负载平衡**

         设残差为0，且阈值 $$V_{th} <= 1$$ ，即实现了恒等映射，如下图  （==论文少说了一句需要保证前一时间步电压为0==）

         <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808164102940.png" alt="image-20240808164102940" style="zoom:67%;" />

         作者基于这一点，说了一句：在脉冲神经网络中实现恒等映射并不能真正解决退化问题（==这和SEW-ResNet网络的说法相悖，SEW-ResNet论文作者说的是因为很难实现恒等映射，所以这个结构不好==）

         关于层积累，我们主要关注每个残差块输出的变化，并倾向于检查当后续神经元的激活状态在从残差路径接收信息后与执行恒等映射的神经元不同时需要满足的条件。

         成功改变激活状态的概率可以分解为两个条件，可以写成：

         <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808165842508.png" alt="image-20240808165842508" style="zoom: 50%;" />
  
         因为脉冲只有0和1这两种类型的值，所以可以得出上面的公式，且这里忽略了膜电位的衰减。
         假设残差路径的输出是一个连续随机变量，其概率密度函数为 φ(u)，并且可以近似地视为与单个神经元的输入相互独立，我们有：

         <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808172936487.png" alt="image-20240808172936487" style="zoom: 50%;" />

         当 $$F(o^{t,l})∼N(0, σ^2_x), with σ_x = V_{th} = 0.5$$ 时，可以得到$$P (si ∈ S) ≈ 16$$​%， 

         当仅仅考虑来自第l层和l+1层之间没有改变激发状态的神经元的输入时，可以得到：

         <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808174318055.png" alt="image-20240808174318055" style="zoom:50%;" />

         这里作者还说 ，残差块之间LIF（·）中的门控函数使得块的输出在某种程度上仅仅是两者之间的选择，而不是和我们所期望的那样由残差路径辅助的主捷径。而对于作者所提出的MS-ResNet，因为去除了块间的LIF（·），也就不存在这样的问题。

         <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808175422657.png" alt="image-20240808175422657" style="zoom: 80%;" />

         进行了一个公式变换，最终可以成为公式(10)的形式， $$I^{t,l}$$ 是第$$l$$ 层的输入。

         为了更直观地验证不同层级残差块的放电模式可能如何改变，我们采用结构相似性指数（structural similarity index measure，SSIM）来量化相似度，其公式为：

         <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808180117386.png" alt="image-20240808180117386" style="zoom: 67%;" />

         x,y是两个输入图像，像素值为对应层的发射率，当x == y时，SSIM的值加1。

         如果发放模式在层级间均匀变化，它将显示为图2b的雷达图中的一个圆形。虽然由于降采样层引起的异质性，严格的圆形不适用于整个神经网络，但我们的MS-ResNet在雷达图上展示出比普通脉冲ResNet更接近圆形的曲线。特别是在普通ResNet-56中有五层的SSIM值为+1，这意味着当信息流经残差块时，发放模式不会发生变化，这些层未能帮助进行特征提取。因此，后续层需要补偿前面层的无作用状态，这表现为第6层后信息发生剧烈变化。在训练深层网络时，网络中的负载不平衡，尤其是反映了脉冲ResNet的不合理性，并在MS-ResNet中得到有效缓解。

       * Gradient Evolvement at Backpropagation   反向传播中的梯度演变

         ==动态等距性（Dynamical Isometry）是一个在数学和物理学中使用的概念，特别是在研究动力系统、流形和某些几何结构时。简单来说，动态等距性指的是在某种变换或演化过程中，系统的某些性质（例如距离或形状）保持不变。在数学上，特别是在理论深度学习和神经网络的研究中，动态等距性经常用来描述一种现象：在网络的训练过程中，信息的传播不改变输入数据的几何性质，比如距离或方向。这种性质有助于改善训练效果，并确保在深度网络中信息不会在传递过程中被过度扭曲。==

         动态同态性，即输入-输出雅可比矩阵（Jacobian matrix）的奇异值均衡，近年来已被发展为对神经网络行为良好的理论解释。在本小节中，我们使用动态同态性框架分析，MS-ResNet可以实现梯度范数的均等，而普通脉冲ResNet则不能。

         神经网络的一般形式可以表示为（多个层的堆叠）：

         <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808195021542.png" alt="image-20240808195021542" style="zoom: 67%;" />
  
         $$θ_i$$是第$$i$$层的参数矩阵，定义 $$∂f_j / ∂f_{j-1} = ∂J_j$$,  <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808195848109.png" alt="image-20240808195848109" style="zoom:50%;" />为$$tr(J)$$​ 的期望，<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808195913081.png" alt="image-20240808195913081" style="zoom:50%;" />为<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808195945796.png" alt="image-20240808195945796" style="zoom:50%;" />
         这样期望和方差就定义好了。

         公式12表示的一般神经网络，将第j个块的雅可比矩阵表示为$$J_j$$，<img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808200352776.png" alt="image-20240808200352776" style="zoom: 50%;" />

         

       * Spike-based Convolution and an Extra LIF(·) at the Top  基于脉冲的卷积 以及 顶部的LIF 神经元

         ​	需要指出的是，没有插入块间LIF(·)的普通ResNet对于脉冲神经网络（SNNs）也是不理想的。神经形态计算的主要节能来源之一是基于脉冲的卷积（CONV），这意味着CONV层将接收和处理二进制脉冲输入，可行性在于用SNNs中的脉冲驱动突触积累（AC）操作替代ANNs中的乘积累加（MAC）操作。特定的优化，例如查找表[38]，可以进一步应用以提高其在神经形态设备上的效率。然而，一旦移除块间LIF(·)，下一个块顶部的CONV将接收连续输入而非二进制脉冲，导致难以从基于脉冲的操作和丰富的输入/输出稀疏性中获益。此外，移除块间LIF(·)将使得残差路径中的CONV层与其后续块顶部的CONV层连接，形成{CONV-BN-CONV-BN-LIF}结构。这两个CONV3x3在效果上等同于单个CONV5x5，我们推测这将使网络实际上变得更浅，并因此削弱其特征提取能力。事实证明，在出现退化问题之前（表II），W/O LIF(·)的准确性将低于普通模型。因此，我们在每个残差路径的顶部增加了额外的LIF(·)，以保持基于脉冲的CONV并充分利用CONV层。此外，还需注意，在整个卷积部分的末端增加了额外的LIF(·)，以确保即使部分信息仅通过快捷连接流动，后续的FC分类器仍会接收到脉冲信号。

         其实就是对其提出的结构解释。 

       * Depth Analysis on CIFAR-10  在CIFAR-10数据集上对深度进行分析

         ​	我们还对CIFAR-10使用MS-ResNet进行了深度分析实验。表II显示的结果表明，我们的MS-ResNet可以扩展到更大规模，而不会像普通脉冲ResNet那样面临退化问题，同时与W/O LIF(·)相比，能够保持浅层网络的更高准确性。此外，为了避免像普通脉冲ResNet一样仅能将准确性峰值推移至一定程度的可能性，我们将n设置为36和80，得到了在CIFAR-10上非常深的MS-ResNet110和MSResNet482，它们的测试准确率分别为91.7%和91.9%。尽管由于有限的正则化方法和由此产生的过拟合问题，测试准确率的提升并不显著，但这确实证明了我们模型的可扩展性和避免退化的能力。

  * **试验：** 

    1. ImageNet：

       这篇文章说 该开始网络在ImageNet上进行训练时，会出现严重的过拟合问题，为了充分发挥深度脉冲模型MS-ResNet104的潜力，采用了一种先进的训练方法，结合了更强的数据增强和正则化，并提出了一种包括T=1预训练阶段和正式训练阶段的两阶段训练过程，以节省训练时间（更多训练细节见附录B）

       优化后的MS-ResNet104在ImageNet数据集上获得了74.21%的准确率，同时作者发现仅将推理图像从224×224放大到288×288，就可以将准确率提高到更具竞争力的76.02%。

    2. CIFAR-10-DVS：

       加了一个下采样层，说了自己达到了当前最高的准确率且参数少。

    3. Firing Patterns of MS-ResNet

       放电率比较低，MS-ResNet34和MS-ResNet104的放电率分别为0.225和0.192。

    4. Energy Efficiency Estimation

       主要说明了自己能耗低。

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808213805159.png" alt="image-20240808213805159" style="zoom: 50%;" />

    5. The Loss Landscape of MS-ResNet

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808224501503.png" alt="image-20240808224501503" style="zoom:67%;" />

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240808224804810.png" alt="image-20240808224804810" style="zoom: 67%;" />

       
  
  * **讨论与总结：**





# SynA-ResNet: Spike-driven ResNet Achieved through OR Residual Connection

通过 OR 残差连接实现脉冲残差网络



* **摘要：**  

  ​	**背景：** 脉冲神经网络因其生物学逼真性和执行节能脉冲驱动操作的能力而在类脑计算领域受到广泛关注。随着对 SNNs 性能要求的提升，训练更深层网络的趋势变得至关重要，而残差学习则是训练深层神经网络的关键方法。在我们的研究中，我们发现 SEW-ResNet，这一深度残差脉冲神经网络的著名代表，包含了==非事件驱动==操作。

  ​	**工作：** 为此，我们提出了一种新颖的训练范式：首先通过 OR Residual Connection（ORRC）累积大量冗余信息，然后利用 Synergistic Attention（SynA）模块过滤冗余信息，该模块在主干网络中促进特征提取，同时抑制短连接中的噪声和无用特征的影响。

  ​	**发现：** 在将 SynA 集成到网络中时，我们观察到 “natural pruning” 现象，即在训练后，网络中的部分或全部短连接被丢弃，并不会影响模型的分类准确性。这显著减少了计算开销，使其更适合在边缘设备上部署。

  ​	**实验：** 对各种公共数据集的实验结果证实，SynA-ResNet 实现了每个神经元仅使用 0.8 次脉冲的单样本分类。此外，与其他残差 SNN 模型相比，它展现了更高的准确性和高达 28 倍的能耗减少。

* **介绍：**

  基于事件的编码，这种方法使脉冲神经网络（SNNs）能够与人工神经网络（ANNs）表现相当，并且由于事件编码样本的额外时间相关性，具有超越 ANNs 的潜力。

  加法操作倾向于降低稀疏性，导致模型变得不稀疏化，在硬件中造成功耗的增加。

  为了应对稀疏性问题，同时减少高等级量化与二进制信号相关的性能下降，我们提出了一种新颖的方法，将逐位 OR 操作用于残差连接与 SynA 注意力机制相结合。与传统的加法操作相比，逐位 OR 操作保留了更多冗余信息，而 SynA 注意力机制则增强了主干网络提取关键特征的能力，并减少了在短连接中进行特征提取时的噪声干扰。这种集成策略不仅在保持完全脉冲驱动架构的同时提高了模型性能，还导致了一个有趣的现象：在我们的实验中，我们观察到 SynA-ResNet 模型中某些或所有短路的神经元的放电率偶尔会降至零。这表明这些短路在推理过程中并未积极参与。值得注意的是，尽管这些活跃连接明显减少，模型却实现了更高的准确性。这一特性提供了双重好处：不仅提供了额外的节能优势，还增强了 SynA-ResNet 的边缘部署潜力，使其特别适用于性能和效率都至关重要的资源受限环境。

* **相关工作：** （可以把里面提到的文献都至少读一遍）

  1. Residual connection in spiking neural networks.

     脉冲神经网络中的残差连接。为了解决深层次 SNN 中梯度消失或退化的问题，研究人员已将残差学习引入 SNNs。Hu 等人[18] 首次提出了一种转换方法，将 ANN 中的连续值缩放以对应于 SNN 的放电率，并引入了补偿机制来抵消转换误差。Panda 等人[19] 提出了具有随机 softmax 的反向残差连接，并通过转换方法进行了验证。Zheng 等人[6] 引入了 TDBN，一种归一化方法，使脉冲残差网络能够训练高达 50 层。Fang 等人[5] 提出了 SEWResNet，利用独特的网络结构训练超过 100 层的网络。Hu 等人[7] 设计了 MS-ResNet，考虑了脉冲通信的特性和神经动态特征，成功地在 CIFAR-10 数据集上训练了一个 482 层的网络。

  2. Attention in neural networks.

     神经网络中的注意力机制。CBAM 模块的出现[20] 引发了研究界对注意力机制的广泛关注，这主要归因于其卓越的性能和类脑特性。在 SNN 研究领域，Yao 等人[21] 首次将注意力机制扩展到时间域，考虑了 SNN 事件编码中的帧间相关性。随后，他们拓展了通道和空间注意力在 SNN 中的应用，最终形成了多维注意力 (MA) 模块[8]，该模块集成了时间维度注意力。相关研究也产生了能够处理时空相关性[22]，[23] 和多尺度视角[22] 的注意力模块，通过多种方法实现了最先进的准确性。

  3. Spike-driven in spiking neural networks.

     脉冲神经网络中的脉冲驱动。SNN 中的脉冲驱动计算使得大量张量乘法操作可以用稀疏加法操作替代，从而实现低功耗特性。这种能力要求使用二进制 (0/1) 张量作为操作单元。尽管 SEW-ResNet 在很大程度上实现了脉冲驱动操作[5]，但每个短路后的第一个卷积涉及非二进制输入张量元素，阻碍了完全的脉冲驱动实现。Chen 等人[24] 通过在训练过程中引入辅助累加路径解决了这个问题，但以模型性能下降和增加时间步长为代价。相反，Hu 等人[7] 设计了 MS-ResNet，以确保所有卷积操作中的脉冲驱动行为。脉冲驱动特性的意义正在获得越来越多的关注，努力将其整合到脉冲transformer[25] 中。预计脉冲驱动计算将成为 SNN 和类脑硬件之间的重要纽带。

* **方法：**

  1. Network input

     以往的研究[26] 表明，直接编码[27] 比速率编码[28] 和时间编码[29] 更为有效。因此，我们选择在 ResNet 的初始模块中（Conv-BN-LIF）使用 LIF 神经元[30] 作为编码器。因此，位于块之前的卷积层不会执行脉冲驱动计算。然而，这种影响很小，因为第一个卷积层有着最少的参数量和最小的计算复杂度。这些非脉冲驱动计算所引入的额外复杂性可以忽略不计。对于静态图像数据集，我们将输入图像复制 T 次，并沿时间维度排列以捕捉事件流信息。这导致了一个四维的输入数组，表示为 [T, C, H, W]。然后，使用 STBP 算法将数据沿时间和空间维度前向和后向传播。

  2. Leaky Integrate-and-Fire neuron model  LIF神经元模型

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240809215359284.png" alt="image-20240809215359284" style="zoom:67%;" />

     本文的超参数设置如表1所示

  3. OR-Spiking ResNet  

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240809215542702.png" alt="image-20240809215542702" style="zoom:50%;" />

     先前提到的三个脉冲残差网络：Spiking ResNet, MS-ResNet and SEW-ResNet.

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240809220357778.png" alt="image-20240809220357778" style="zoom:67%;" />

     脉冲元素中，颜色深表示放电率高，权重矩阵中，颜色深表示权重大。

     表2是图中 G(.) 函数的具体操作

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240809221037073.png" alt="image-20240809221037073" style="zoom:50%;" />

  4.  OR-Spiking ResNet with Synergistic Attention

     ​	如前所述，我们选择了具有最高冗余的按位操作 OR 作为残差连接的方法。随后，我们通过 ORRC 增强了其主要特征，同时抑制了背景噪声被提取为特征。这种方法使得使用明显不同的训练范式训练高性能的脉冲驱动模型成为可能。图 1 展示了 OR-Spiking ResNet 中 SynA 的机制和预期效果（请注意发光和模糊效果）。为了评估其在 OR-Spiking ResNet 中的有效性，我们在第 V-B2 节进行了全面的消融实验，并在第 V-B3 节提供了 SynA 能效来源的详细分析。在 SynA 有效性的实验验证过程中，我们观察到随着带 SynA 的残差模型继续训练，部分或全部快捷路径自然丢失，而性能没有损失。我们将这种现象称为“自然剪枝”。对这一有趣特征的深入分析以及注意力的影响和基本原理将在第 V-B3 节的消融研究部分进行。

  5.  Analysis of energy consumption  能量消耗分析

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240810141751237.png" alt="image-20240810141751237" style="zoom: 80%;" />

     ![image-20240810104941552](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240810104941552.png)

  6. Ablation study   消融实验

     

  7. 

* **试验：**

* **讨论与总结：**



[MAE论文阅读](https://www.bilibili.com/read/cv14591955/?jump_opus=1)





# 十二

# ==1==



* **摘要：**  
* **介绍：**
* **方法：**
* **试验：**
* **讨论与总结：**





# ==专题类文章==



## 1. 主题：Human brain computing and brain-inspired intelligence

人脑计算和脑启发智能

类型：Special Topic



* **摘要：**  人类大脑的功能性研究是非常关键的。

* **介绍：** 首先，由于人类大脑的大小、神经形态细节和拓扑结构较为复杂，建立人类大脑的计算神经网络模型极其困难的

  ​	    第二，设计和制造神经形态芯片是实现具有人类大脑规模和复杂性的神经网络的唯一途径。

  ​            神经形态系统最近的焦点主要集中在脑科学上。

  ​	    混合神经网络（HNN）被用于这些神经形态的系统，该网络是ANN结合SNN。

  ​	    在规模和架构上越接近大脑，模拟模型与真实大脑（在静息状态和任务执行过程中）的相似性越高

  ​            整合模型、数据和专用可计算性是大脑计算的基本原则
