# ==文献阅读   SNN综述性文章==

1. ## ==**Towards spike-based machine intelligence with neuromorphic computing(基于神经形态计算的脉冲机器智能)**==

* **期刊：**nature 64.8 1区top

* **摘要：**SNN这一跨学科领域始于硅电路在生物神经系统中的应用，在2018年中期就已经发展到算法的硬件实现部分了。这篇文章将系统阐述神经形态计算的算法和硬件的发展，并强调学习和硬件框架的基础。重点是算法硬件的协同设计。

* **介绍：**
  
  之前科学家认为大脑中信息的传输就像无线电一样，现在大脑被认为是和计算机一样。
  
  大脑的功耗将近20W，而电脑进行1000种对象识别的功耗为250W。
  
  大脑功能依赖于：vast connectivity, structural and functional organizational hierarchy, and time-dependent neuronal and synaptic functionality。
  
  大脑和硅基计算机的计算原理之间存在的鲜明对比：（计算和存储的是否连接，大脑的三维连接以及连接的数量，大脑的随机性）
  
  1.  计算机中的计算（处理单元）和存储（记忆单元）分离，这与大脑中的计算（神经元）和存储（突触）机制形成了鲜明对比；
  2.  大脑中的大规模三维连接目前超出了硅技术的范围，硅技术受到二维连接和有限数量的互连金属层和路由协议的限制;
  3.  晶体管在很大程度上被用作开关，以构建==确定性==的布尔（数字）电路，与大脑中固有的==随机性==基于尖峰的事件驱动计算相反。尽管如此，硅计算平台（例如，图形处理单元（GPU）云服务器）一直是当前深度学习革命的推动因素之一。然而，阻碍实现“泛在智能”（跨越基于云的服务器到边缘设备）的主要瓶颈是大的能量和吞吐量要求。例如，在由典型的2.1瓦时电池支持的嵌入式智能玻璃处理器上运行深度网络，将在25分钟内完全耗尽电池。-----==主要还是说了能耗的问题，同时表明虽然现有的加速硬件与大脑机制相反，但是其仍然在推动深度学些前进的过程中发挥了巨大作用。从这一段开始便引出了SNN==
  
  在当前情况下，发展SNN首先需要解决算法的问题，包括不同的学习机制，同时需要重点考虑时序这一特点。
  
  我们可以将神经形态计算领域描述为一种协同努力，它在硬件和算法领域具有同等的权重，以实现基于脉冲的人工智能。
  
  首先讨论算法在视觉任务方面的应用，接着讨论了算法-硬件协同设计的前景，其中==算法弹性(resilience)可以用来对抗硬件脆弱性==，从而实现能量效率和准确性之间的最佳折衷。 
  
* **Algorithmic outlook 算法展望**

  * **Spiking neural networks 脉冲神经网络**

    * LIF模型，突触可塑性。
    * 神经形态工程师的主要目标是建立一个具有突触可塑性的尖峰神经元模型，同时利用基于事件的数据驱动更新（使用基于事件的传感器16,17），以实现计算高效的智能应用，如识别和推理等。

  * **Exploiting event-based data with SNNs**  使用SNN开发基于事件的数据（事件相机？）

    * 他们相信SNN的最终优势来自于其可以充分利用基于时空时间的信息的能力。
    * 研究人员证明了使用基于事件的传感器进行跟踪和手势识别的好处，然而，这些应用中的大多数使用DLN来做的。SNN之所以没有被使用的原因是其==缺乏适当的训练算法。==，这个算法的要求是能够有效的利用脉冲神经元的时间信息
    * 在准确度方面，SNN比较弱。
    * SNN的另外一个限制是==基于峰值的数据可用性==，说人话就是没有合适的训练数据。  编码技术使SNN可以在传统静态图像数据集上进行训练测试。
    * SNN最终的能力应该来自于他们处理和感知来自不断变化的真实世界中的连续输入流的能力，说人话就是各种宏观上离散，但是个体上连续的数据。对于所有变量来说是离散的，但是每一个变量本身都是连续的。

  * **Learning in SNNs**  SNN中的学习（搭建SNN，训练SNN）

    * **Conversion-based approaches**  基于转换的方式
      ANN-SNN，这种方法确实可以实现较高的准确率，不过由于激活函数的不同，ANN会产生负值，但是SNN没有负值，只能选择丢弃这个值，这就带来了准确率的下降，且ANN-SNN的转换推理时间非常长(几千个时间步)，这就与我们使用SNN的本意相违背。

      一个是速率只有正值，另外一个就是如何获得每一层的最佳发射速率(29-31，这些工作在训练DLN中引入了额外的噪音以及 leaky ReLU),
      <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240714171912921.png" alt="image-20240714171912921" style="zoom:80%;" />
      
      ==如何高效转换应该是一个关键，或者说不是转换，而是借鉴ANN有用的一些思想，结合SNN，形成一个基于ANN的SNN==
      
    * **Spike-based approaches**   基于脉冲的方法
      
      监督或者无监督

      只使用SNN，极大地利用SNN的优势：稀疏性以及效率，但是深层网络训练困难，因为产生了之前ANN发展初期也存在的一个问题：梯度消失——在SNN中称为消失的前向脉冲传播。
      
      当前的两种类型的算法：固定SNN的输出，去探寻SNN的反向传播规则；对(实值)膜电位进行随机梯度下降，不固定输出数量，而是认为正确的输出神经元将发射更多的脉冲。
      
      将局部学习应用于复杂任务的每一层是非常具有挑战性。所以使用分层训练的思想
      
      有学者提出：分层进行局部学习训练，全局反向传播，思想很好，准确率还是低一点。
      
      ![image-20240714182507525](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240714182507525.png)
      
    * **Implications for learning in the binary regime**  使用二进制制度进行学习时所产生的影响
      ==解释：==在神经网络中，有时候会使用二进制方式来进行计算，这种计算方式利用二进制计算的速度快、存储空间小的优势。

      目前也存在二进制，三进制的ANN，此时神经元输出以及权重仅能使用-1,0,1，三进制的ANN同样可以获得较高的分类精度

      那么，我们能否利用这种 SNN 时空处理架构和适当的学习方法，以最小的精度损失将权重训练压缩为二进制，还有待观察。
      
      LIF神经元的神经元动力学中的随机性可以提高网络对外部噪声的稳健性。我们是否可以使用SNN的这种时间处理结构和适当的学习方法，并在最小精度损失的情况下将权重训练压缩到二进制状态，还有待观察。

  * **Other underexplored directions**  其余探索不足的地方

    * **Beyond vision tasks**   视觉任务之外的任务
      SNN在自然语言处理上的应用很少很少。作者相信SNN处理传感器数据应该是很有前景的方向。
    * **Lifelong learning and learning with fewer data**    终身学习和用更少的数据学习
      深度学习模型在持续学习时会遭受灾难性的遗忘。例如，当一个在任务A上训练的网络后来暴露给任务B时，它会忘记所有关于任务A的信息，只记得任务B。
      我们需要考虑SNN中数据处理的额外时间维度是否可以帮助我们实现持续学习。
    * **Forging links with neuroscience**   加强与神经科学的联系
      ==个人认为这应该是最需要研究的方法，多学科结合应当能够产生强大的力量，甚至超越所有的ANN，实现真正的人工智能，但其实困难在于算法的发展受限于硬件设备的发展，所以只能找折中的办法==

* **Hardware outlook**  硬件展望
  电路可能需要实现很复杂的功能才有可能模拟大脑的一部分。

  * **The emergence of neuromorphic computing**  神经形态计算的出现
    某位学者没有专注于基于 AND 和 OR 门 的布尔计算，而是专注于模仿神经元和突触的模拟分布式电路。  这种设备与电路的协同设计是目前神经形态计算中最有趣的领域之一。

  * **The ‘Big Brain’ chips**  大脑芯片
    该芯片的特点是集成了数百万个神经元和突触。

  * **Asynchronous address event representation**   异步地址事件表示
    传统的芯片设计是参考全局时钟部分（分步？）执行的。 由于SNN是稀疏的，所以使network-on-chip Non-volatile technologies用异步事件驱动的计算更合适。

  * **Network-on-chip.**    NOCs  片载网络
    大规模芯片中使用NOCs是必要的，因为传统工艺的连接主要是二维的

  * **Beyond-von-Neumann computing** 超越冯诺依曼计算

  * **Non-volatile technologies**  
    Non-volatile technologies指的是一类存储技术，其特点是在断电情况下能够保持数据的存储状态，而不会丢失。==这与易失性存储技术相对==，易失性存储技术在断电后会丢失其中的数据。因此，==非易失性存储技术通常用于存储需要长期保存==、不容丢失的数据，如固态硬盘、闪存存储等。

    该技术表现出了生物突触的两个最重要的特征：突触效能和可塑性。

  * **Silicon (in-memory) computing**    硅（内存）计算
    事实上，目前几乎所有主要的存储器技术都在探索各种形式的内存计算。虽然这些工作大多数都集中在通用计算应用程序，如加密和DLN，但他们可以很容易地在SNN中找到应用。

* **Algorithm–hardware codesign**  软硬协同设计

  * **Mixed-signal analog computing** 混合信号模拟计算
    从本质上讲，无论是以局部化学习的形式，还是使用树突学习等范式，我们认为一类更好的、具有较强容错性的==局部学习算法==——即使要增加学习参数的代价——将是推动模拟神经形计算前进的关键。此外，芯片上学习的容错性可以用于开发低成本的近似模拟-数字转换器，而不会降低目标应用程序的准确性。

  * **Memristive dot products**   忆阻点积

    忆阻点积是实现原位神经形态计算的一种可行方法。

    Memristive dot products”可以理解为在神经形计算中使用的一种技术，它涉及到使用memristor（一种基于电阻变化的器件）来执行点积运算。在人工智能和神经形计算中，点积运算用于将神经元的输入信号和相应的权重进行加权求和，这对于模拟大脑神经元之间的连接和计算起到了关键作用。

    在神经形计算中，memristive dot products技术可以用于模拟神经元之间的连接强度和权重，从而实现类似于大脑内部信息处理的功能。利用memristive dot products技术，可以实现更高效的神经形计算，而且这样的计算方式还能更好地模拟人类大脑的工作原理。

    遗憾的是，在表示点积的忆阻阵列中产生的电流既与空间有关，也与数据有关，这就使得交叉条电路分析成为一个很复杂的问题。  ==大多数工作都集中在DLN，而不是SNN==

    我们相信基于最先进设备的交叉阵列模型的抽象版本，以及努力建立点积误差的理论界限，是迫在眉睫的一件事。这将使算法设计人员能够探索新的训练算法，同时考虑硬件不一致性，而无需进行耗时且反复迭代的设备-电路-算法模拟。

  * **Stochasticity**  随机性
    由于存在固有随机性的新兴设备，随机SNNs具有重要意义。最近关于实现随机二值SNNs的大部分研究都侧重于小规模任务，例如MNIST数字识别。这些研究的共同主题是利用随机STDP-like本地学习规则来生成权重更新。我们认为，在STDP学习中的时间维度为权重更新提供了额外的带宽，使其朝着正确的方向（朝着实现整体准确性）前进，即使受到二进制范围的限制。==将这种二元局部学习方案与基于梯度下降的学习规则相结合==，用于大规模任务，同时利用硬件中的随机性，为能效神经形态系统提供了有趣的机会。

  * **Hybrid design approaches** 混合设计方法
    ==结合各种有效的方法来最大程度实现SNN的训练及性能的提升==
    作者团队相信，这种结合的本地-全局学习方案可以用来降低硬件复杂性，同时也最大限度地减少最终应用程序的性能下降。

* **Conclusion**
  **“智能”是跨学科研究的中心主题。**
  如今，在我们周围几乎所有的技术中实现“智能”已成为跨越各种学科的研究的核心主题。在这方面，这篇观点阐述了神经形态计算作为一种能够通过硬件（计算）和算法（智能）的协同进步来实现机器智能的能效方式。我们首先讨论了使用脉冲神经范式的算法意义，这与传统深度学习范式中的实值计算形成对比，脉冲神经范式采用了事件驱动的计算。我们已经描述了实现学习规则（如基于脉冲的梯度下降学习、无监督STDP以及相关的从深度学习转换到脉冲领域的方法等）在标准分类任务中的优势和局限性。未来的算法研究应该利用基于脉冲的信息处理的稀疏和时间动态，结合可以实现实时识别的补充神经形态数据集；而硬件开发应专注于事件驱动计算、存储器和计算单元的联合位置，以及模拟动态神经突触功能。特别值得关注的是新兴的非易失性技术，可以实现原位混合信号模拟计算。我们还讨论了促进算法-硬件代码设计的跨层优化的前景，例如利用算法上的韧性（如局部学习）和硬件可行性（如实现随机基元的简便性）。最后，利用传统和新兴设备构建基于脉冲的能效智能系统的前景符合当前推动实现普遍智能的兴趣。现在是时候进行跨学科的交流，通过跨越设备、电路、架构和算法的跨学科努力，最终合成一种真正能效和智能的机器。

2. ## ==**Neural heterogeneity promotes robust learning**==   神经异质性促进稳健学习

   * 期刊：Nature Communications  IF:16.6  1区top

   * **摘要**：作者团队比较了具有不同异质性的脉冲神经网络性能，发现异质性大大提高了任务性能。对于时序性较强的任务异质性作用更明显。  训练后的神经网络中神经元参数的分布与实验观察到的相似。
     
     大脑中观察到的异质性可能不仅仅是嘈杂过程的副产品，而可能起着让动物在不断变化的环境中保持学习的作用。
     
     ==神经异质性解释：== 神经异质性是指神经元之间或不同区域之间的结构和/或功能上的差异性。这种异质性可以出现在不同层面，包括形态学、电生理学和分子水平上。在形态学上，神经异质性可以指不同神经元的形态特征、突触连接的密度和位置等方面的差异。电生理学上的神经异质性可能表现为不同神经元的兴奋性和抑制性表现的不同，或者说不同区域之间的电活动特征差异。分子水平上的神经异质性则可能表现为在不同神经元或不同区域中表达的离子通道、神经递质受体和信号传导分子的差异。这种神经异质性对神经系统的功能和信息处理起着重要作用，因为它可以提供神经网络更丰富的功能和适应能力。研究神经异质性有助于更好地理解神经网络的复杂性，并有助于揭示不同神经元类型和区域在脑功能中的作用。
     
     **假设大脑在发育过程中，神经异质性的发育也是多方面同步发展的，进而造就一个人逐渐变得可以处理多种复杂的事情，那么如果我们只深入研究一种方向，是否就可以得到一个专注于某类型任务的SNN？------借鉴于ANN的发展道路**
     
     ==文中的异质主要指的是神经元的时间常数不同。==
     
   * **介绍**：在神经元的时间尺度中引入异质性，提高了整体性能，网络学习到的结果与试验观察结果有一样的神经参数分布，故作者认为大脑中观察到的异质性是适应新环境能力的重要组成部分。

   * **Results**

     * **Time scale heterogeneity improves learning on tasks with rich temporal structure.**
       
       * 三层脉冲神经网络（输入层，循环连接层，输出层）
       
       * 通过给每个神经元一个单独的膜和突触时间常数，引入了异质性。
       
       * 我们比较了四种不同的条件：初始值（==膜时间常数和突触时间常数==）可以是同质的或异质的，训练可以是标准的或异质的。更详细地，时间常数或用单个值初始化(同质初始化)，或者根据gamma分布随机初始化(异质初始化)。在这两种类型的训练中，使用代理梯度下降来优化模型的参数。突触权重在标准训练和异质训练中都是可训练的，而时间常数在标准训练中保持不变，在异质训练中可以修改。
       
       * ![image-20240715095820440](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240715095820440.png)
       
         ![image-20240715094908229](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240715094908229.png)
       
       * 引入异质性可以在只增加非常少量参数的情况下大幅提高性能（SHD为0.23%，因为我们添加了一些特定于神经元的参数，但没有添加任何特定于突触的参数，而且绝大多数参数是突触权重），而且无需使用任何额外的神经元或突触。因此，异质性是一种代谢高效的策略。它还是一种对神经形态学计算有兴趣的计算高效策略，因为在模型模拟时，向模型添加异质的时间常数会使内存使用和计算时间增加O(n)，而添加更多神经元会增加O(n2)（因为该模型是完全连接的，因此突触权重的数量与神经元数量的平方成正比）。
       
       * 使用的数据集中，对于F-MNIST，是将图像灰度值当成了输入电流。  N-MNIST和F-MNIST具有最小的时间结构，因为他们都是来自静态图片。DSV128具有一些时间结构，但是丢弃这种时间信息可以获得更好的结果。
       
       * 作者证明了得到的结果不是他们对时间常数进行调优的结果，是进行多次比较得到的。
       
       * 作者表明，增加更多的神经元同样会带来准确度的升上，但时间上会慢很多。==这里有个重点是：从之前的ANN中我们其实也能得到经过优化的小网络也能在部分任务上获得好的甚至更高的性能，这恰恰符合大脑在面对自然界中的各种信息，所表现出来的优越性能，所以是否后面发展SNN可以朝着小且复杂的方向？ 但复杂仍然会带来高功耗的问题，这又和大脑低功耗相违背。==
       
       * 作者还尝试了改变脉冲发射阈值和重置电位，但是没有什么影响，他认为改变这些等同于对膜电位常数进行缩放而不是改变膜电位。Bellec等人发现，引入自适应阈值确实提高了性能，作者认为大概是自适应阈值允许更丰富的时间动态。
       
     * **Predicted time constant distributions match experimental data**
       * 有趣的是，实验数据中每种细胞类型的分布参数也是不同的，这是我们模拟中没有复制的特征，因为所有细胞都是相同的。这表明，在引入不同细胞类型方面进一步增加多样性可能会导致更好的性能。
       
     * **Heterogeneity improves generalisation: speech learning across time scales.** 
       异质性提高了泛化能力：在不同时间尺度上的语音学习。
       
       * 异构初始化本身就足以实现这种更好的泛化性能，而时间常数的训练提高了峰值性能，但没有提供额外的泛化能力。意思是多次训练并不能带来泛化性能的提高。
       * ![image-20240715105959443](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240715105959443.png)
       
     * **Heterogeneity improves robustness against mistuned learning.**
       异质性提高了对误调学习的稳健性。
       "mistuned learning" 意指学习误调，即学习过程中出现了错误的信息或者不准确的数据。在这种情况下，因为学习的内容不准确或者不适当，可能会导致对所学知识的误解或者错误的应用。因此，所谓的“mistuned learning”指的就是出现了学习偏差或错误的情况。
       
       这里做的实验就是将在某一个数据集上表现最好的超参数应用到另外一个数据集上，去测试表现，结果得到的是异质网络仍然能够变现良好，同质网络无法训练（个人认为这一点是显然的，在ANN中早就验证过了。）

   * **Discuss**

     * 对于异质网络来说，学习更为稳健，网络能够学习来自一系列不同环境的信息，即使学习的超参数被错误地调整。

     * 当学习规则允许调整时间常数以及突触权重时，发现了一致的时间常数分布，类似于对数正态分布或伽玛分布，这在定性上与实验数据中测得的时间常数相匹配。我们的模型与这两种可能性是一致的，即这些时间常数分布是在个体的一生中学习到的，或者它们是作为进化过程的结果而被发现的。我们从中得出结论，神经异质性是大脑一种代谢效率高的策略。异质网络在神经元或突触数量上没有额外的成本，并且表现与拥有数量级更多神经元的同质网络一样好。这一收益也延伸到神经形态计算系统，因为向神经元模型中添加异质性仅增加O(n)的时间和内存成本，而添加更多的神经元则成本为O(n2)（因为所有神经元都是相互连接的）。此外，在某些神经形态系统如BrainScaleS中，这种异质性已经作为制造过程的一部分而存在。除了整体性能更好之外，异质网络更稳健且能够在更广泛的环境中学习，这在生态学上显然是有利的。同样，在神经形态计算和潜在的机器学习中，这也具有相应的益处，因为它降低了超参数调整的成本，而这往往是开发这些模型的最大成本之一。

     * 我们的计算结果显示，异质性具有明显的优势，这在直觉上是合理的。在一个层中具有不同时间常数的异质性使得网络能够以不同的时间尺度整合传入的脉冲，对应于更短或更长的记忆痕迹，从而使读取层能够捕捉多个尺度上的信息并表示更丰富的函数集。延伸这一思路并找到异质性优势的严格理论解释将非常有价值。

       

3. ## ==Supervised learning in spiking neural networks: A review of algorithms and evaluations==
   脉冲神经网络中的监督学习：算法和评估综述

* **期刊：**Neural Networks  IF = 7.8  2020年  1区top

* **作者及单位：**  Wang Xiangwen 西北师范大学    被引用210次

* **摘要：** 作为一种新的类脑计算模型，脉冲神经网络通过准确定时的脉冲序列来编码和处理神经信息。脉冲神经网络由生物学合理的脉冲神经元组成，已成为处理复杂的时间或时空信息的工具。然而，由于其==复杂、不连续和隐含的非线性机制==，为脉冲神经网络制定高效的==监督学习算法==是困难的，也成为这个研究领域中的重要问题。本文对脉冲神经网络的监督学习算法进行了全面的综述，并从定性和定量两个方面进行了评估。首先，对脉冲神经网络与传统人工神经网络进行了比较。然后介绍了脉冲神经网络监督学习的一般框架和一些相关理论。此外，还从适用于脉冲神经网络架构和监督学习算法固有机制的角度对近年来的监督学习算法进行了综述。还对一些代表性算法的脉冲序列学习性能进行了比较。此外，==我们提供了五个定性性能评价标准==，用于评估脉冲神经网络的监督学习算法，并基于这五个性能评价标准提出了一个==新的分类系统==。最后，概述了这个研究领域的一些未来研究方向。

* **介绍：** 近年来提出的SNN的监督学习算法可以从不同的角度分为几类。

  从适用于网络架构的角度来看，它们可以分为单层SNN，多层前馈SNN和递归SNN的监督学习算法。

  从运行模式的角度来看，它们可以分为在线学习算法和离线学习算法（或批量学习算法）。

  从信息编码的角度来看，可以将它们分为产生单个脉冲和对输入的时间或时空数据产生脉冲序列作为输出的算法。

  从结构动力学的角度来看，它们可以分为固定SNN结构中的学习和不断变化的SNN结构中的学习（上一篇文章中讲的同质异质为时间常数的差别）。

  从知识表征的角度看，可以将它们分为非知识型的学习和知识表征学习。

  <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240715154424826.png" alt="image-20240715154424826" style="zoom:67%;" />

  

* **Comparison between SNNs and traditional ANNs**    SNN与传统ANN的比较

  * ![image-20240715170314153](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240715170314153.png)

  * 解释上面的表格：

    1. 信息编码和表示。SNN使用时间编码方案将信息编码为脉冲序列，而ANN使用速率编码方案将信息编码为标量。这是SNN与ANN之间的基本差异之一。因此，SNN在信息表示能力上比ANN更为强大，特别是对于复杂的时空数据。

    2. 计算单元和网络模拟。SNN的基本计算单元是脉冲神经元，由微分方程表示，而ANN的基本计算单元是人工神经元，其中输入由激活函数处理。这是SNN和ANN之间的另一个基本差异。针对SNN的相应模拟策略主要是基于时钟驱动和事件驱动，而ANN的模拟策略是逐步进行的模拟过程。

    3. 突触可塑性和学习。SNN的突触可塑性机制强调了突触前后神经元之间的时序相关可塑性（STDP），而ANN的机制通常满足Hebb规则。在设计有监督学习算法时，SNN通常具有各种思路（参见第4-6节），而ANN主要基于寻找损失函数的导数。

       ==Hebb规则：== 该规则简洁地总结为：“同时或连续激活的神经元应该具有较强的突触连接”。更具体地说，Hebb规则描述了神经元之间突触连接发生可塑性的机制。当一个神经元的活动持续地导致另一个神经元兴奋时，突触连接将被增强，这被称为正向突触可塑性。而当一个神经元的活动持续地抑制另一个神经元时，突触连接将被削弱，这被称为反向突触可塑性。

    4. 并行和硬件实现。SNN可以实现快速和大规模的并行信息处理，而ANN相对较弱。SNN使用离散脉冲序列而不是模拟信号来传输信息，这在能耗更低的硬件实现中更为适用（Farsa、Ahmadi、Maleki、Gholami和Rad，2019）。

* **Basic theories of supervised learning for SNN**   SNN的监督学习基本理论

  1. 监督学习的通用框架
     从随机生成的初始突触权重矩阵W开始，SNN的每个监督学习过程可以分为以下四个阶段：
     
     1. 通过某些特定的编码方法将输入数据编码成脉冲序列 $s_i^n(t)∈S_i$.
     2. 将脉冲序列 $s_i^n(t)$ 输入到SNN中，然后使用某种特定的仿真策略来运行网络。最终，可以获得实际输出的脉冲序列  $s_a^m(t)∈S_a$.
     3. 根据期望的输出脉冲序列 $s_d^m(t)∈S_d$.，计算误差函数 $E(S_a,S_d)$ 的值，并调整突触权重：W ←W + ∆W。
     4. 确定通过学习过程获得的SNN的误差是否达到了预设的最小误差，或者学习周期的上限是否已经超过。如果不满足终止条件，则重复学习过程。经过监督学习后，网络的输出脉冲序列 $S_a$ 通过特定的解码方法进行解码。
     
  2. 监督学习相关理论
     1. 神经信息的编码与解码
     
        输入和输出脉冲序列的编码对 SNN 监督学习算法的性能有很大影响
     
        编码指的是将样本数据或刺激信号转换为脉冲序列，而解码则是编码的反向过程，将脉冲序列映射到输出或特定的响应。时间编码策略：首次脉冲法、延迟-相位法、种群编码法和Ben的 脉冲算法。
     
     2. 脉冲神经元的计算模型
        脉冲神经元是SNN的基本计算单元。根据复杂性，脉冲神经元的计算模型可以分为三类：具有生物合理性的生理模型、具有脉冲响应的非线性模型和具有固定阈值的线性模型。一些脉冲神经元模型，例如霍奇金-赫胥黎模型，过于复杂，因此很难利用它们实现大规模的SNN模拟；而一些简单的脉冲神经元模型，比如integrate-and-fire neuron 和脉冲响应模型（SRM），具有解析表达式，因此常用于SNN模拟，尤其是在监督学习中。
     
     3. SNN的仿真策略
        分为时钟驱动型和事件驱动型（后续应该会大力发展的方向，其更加符合SNN提出的目的），另外还有混合仿真策略。研究结果表明，不同的仿真策略会影响SNN的动态特性和学习性能。
     
     4. 脉冲序列的相似度度量
        脉冲序列的相似度度量是一种定量方法，用于计算脉冲序列之间的相似程度。通过计算脉冲序列的误差，一方面可以计算监督学习的精度。另一方面，在某些监督学习算法（如基于梯度下降的监督学习算法）中，定义的特定误差函数被应用于推导学习规则。

* **单层SNN的监督学习**

  1. 单层SNN

  2. 单层SNN的学习算法

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716093028439.png" alt="image-20240716093028439" style="zoom:50%;" />

     * 基于感知机的算法

       脉冲神经元的监督学习相当于通过调整突触权重来正确区分神经元在运行过程中所产生的期望输出脉冲的时间和其他时间。
       [SNN系列｜学习算法篇(1)Tempotron](https://blog.csdn.net/ly18846826264/article/details/105214150/)

       1.tempotron学习规则

       ![image-20240716085136825](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716085136825.png)

       **推导：**阈值下的后突触膜电位为所有输入脉冲的加权和：

       ![image-20240716092857176](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716092857176.png)

       ​	其中$t_i$表示第i个神经元的脉冲发放时刻（输入神经元在一个时间窗口内可能发放多个脉冲，也可能没有发放脉冲），$K(t-t_i)$表示的是第i个神经元发放脉冲后对后突触膜电位的贡献。

       ![{K_0}({e^{\frac{{ - (t - {t_i})}}{{{\tau _m}}}}} - {e^{\frac{{ - (t - {t_i})}}{{{\tau _s}}}}})](https://latex.csdn.net/eq?%7BK_0%7D%28%7Be%5E%7B%5Cfrac%7B%7B%20-%20%28t%20-%20%7Bt_i%7D%29%7D%7D%7B%7B%7B%5Ctau%20_m%7D%7D%7D%7D%7D%20-%20%7Be%5E%7B%5Cfrac%7B%7B%20-%20%28t%20-%20%7Bt_i%7D%29%7D%7D%7B%7B%7B%5Ctau%20_s%7D%7D%7D%7D%7D%29)

        	其中，$$τ_m$$和$$τ_s$$表示膜电压整合和突触电流的衰减时间常数，即快延迟时间常量和慢延迟时间常量。$$K_0$$是一个归一化因子，目的是保证$K(t-t_i)$的幅值为1，求解方法就是先假定$$K_0$$为1，求其余部分的最大值，接着设$$K_0$$​为该最大值的倒数即可。

       ​	 $K(t)$的图像为下图（刺激导致膜电位上升接着又逐渐衰减，在刺激达到很短的时间内，膜电位会上升到最大值。该图可以很好的模拟神经元受刺激再放电的过程）

       ![img](https://img-blog.csdnimg.cn/20200331080515815.jpg#pic_center)

       ![image-20240716102038891](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716102038891.png)

       ![image-20240716102552571](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716102552571.png)

     * 突触可塑性算法

       主要介绍STDP学习规则

       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716140005965.png" alt="image-20240716140005965" style="zoom:67%;" />

       ​         位于0左侧的数据点表示突触前神经元先于突触后神经元发放，此时突触的权重增加，称为长时程增强LTP；位于0右侧的数据点表示突触后神经元比突触前神经元更先发放，此时权重增长为负，称为长时程抑制LTD。两个神经元发放的时间间隔越小，长时程增强/抑制的效果表现越明显。

       ​        一般用下面这个式子进行拟合：
       ![image-20240716140951708](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716140951708.png)

       ​       但是STDP中对权重变化的建模不能采用上述的式子，因为突触前神经元一旦产生脉冲，其突触权重的变化必须综合考虑所有突触后神经元的历史脉冲，在大规模的脉冲神经网络中的实现是不现实的。通常的建模方式是用两个局部变量与$$A_{pre}$$和$$A_{post}$$​ 分别保存突触前和突触后神经元的脉冲发放所引起的突触强度变化的迹，最后两者相加便得到的变化。其数学表达式如下：

       ![image-20240716141156035](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716141156035.png)

       ​         在突触前神经元产生动作电位时，突触权重的减少与突触后的迹$$A_{post}$$ 成正比；而突触后神经元产生动作电位时，突触权重增强，与突触前的迹  $$A_{pre}$$  成正比。$$A_{pre}$$和$$A_{post}$$ 类似于计数器去记录神经元之间的激活情况。

     * 脉冲序列卷积算法

       没有细看

     * 其余监督学习算法

       没多看

  3. 脉冲序列学习的性能比较
     * ==总结：== 对于不同的算法，脉冲序列的长度和脉冲发放速率都会影响最终的结果，且从迭代次数来看，各个算法基本都能在迭代600次达到最佳准确率。

       

* **多层前馈SNN监督学习**

  1. 结构

     <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716105045585.png" alt="image-20240716105045585" style="zoom:67%;" />

  2. 学习算法

     [SNN编码方式的了解](https://zhuanlan.zhihu.com/p/587646331)

     * 梯度下降算法
       有学者提出了 SpikeProp 算法，其中使用了SRM，这是一种多层前馈SNN的反向传播训练算法。SNN中的所有神经元都被限制为仅发射单个脉冲。之后就是其他学者不断对该算法进行改进了。

       ==对SpikeProp算法的理解==
       
       ![img](https://pic2.zhimg.com/80/v2-d4140ec8dab7e27f91155c9fd2fe8c01_720w.webp)
       
       1. 对比图b和图c可以看出来，中间层神经元与输出层神经元的之间的连接可以是一个，也可以是多个。
       
       2. 接着从图c中可以得到一个等式：![image-20240716170559041](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716170559041.png)
       
          上式中，m表示的是不同时延的个数，其中$$y_i^k(t)$$​的表达式为：![image-20240716170716828](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716170716828.png)‘
       
          其中$$t_i$$是神经元i第一次超过阈值的时间，$$d^k$$是第k个突触的延迟（不同的突触分支上也会有脉冲产生）。
       
       3. 后突触膜电位的计算公式：![image-20240716171040613](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716171040613.png)
       
          这里 $$w_{ij}{k}$$ 是与第 𝑘 个延时突触终端相关联的权重值，把神经元 𝑗 第一次脉冲发射的时间定义为 $$t_j$$, 把脉冲目标发射时间定义为$$t_j^d$$, 此时可以得到误差函数： 
       
          ![image-20240716171241220](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716171241220.png)
       
            我们要对权重进行更新，所以令E对$$w_{ij}^k$$​​进行求导，和BP一样，令权重沿着梯度的相反方向进行更新，可以得到：![image-20240716171449715](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716171449715.png)
       
             在脉冲发放时刻，权重的变化为：
       
          ![image-20240716171821425](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716171821425.png)
       
          ​    因为脉冲发射时刻是在膜电位达到阈值的条件下定义的，具有不连续的性质，所以我们需要进行近似：假设在脉冲发放时刻的邻域里，$$t_j$$和$$x_j$$​​呈线性关系，即膜电位的增加会引起脉冲发射时间的减小（负相关），设![image-20240716172123277](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716172123277.png)，我们可以得到：
       
          ![image-20240716172312196](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716172312196.png)
       
            则权重在脉冲发射时间$$t_j$$的变化为:
       
          ![image-20240716172224060](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240716172224060.png)
       
          
       
     * 突触可塑性算法
       研究人员提出了各种结合突触可塑性与梯度下降规则的多层前馈SNNs的监督学习算法。

     * 脉冲序列卷积算法
       SPAN算法及其推广的其余算法

     * 其余监督学习算法

* **脉冲序列学习的性能比较**

  * ST-SpikeProp， Multi-ReSuMe and Multi-STIP(具有最高精度)
    Multi-STIP算法具有学习精度高、运行时间短等优点，适合于大规模前馈神经网络的训练。

* **Supervised learning for recurrent SNNs**   循环SNN的监督学习

  1. 循环SNN的架构
     完全循环SNN和局部循环SNN.  尽管完全循环SNN具有丰富的动态行为，但其结构非常复杂，难以分析和训练。因此，在实际应用中，通常需要简化完全循环SNN的结构。一种常见的简化方法是将反馈连接引入到多层前馈SNN中，所得到的网络称为局部循环SNN，其中主要连接是前向连接，同时包含一组反馈连接。根据反馈连接的方式，局部循环SNN可分为带外部反馈的循环SNN和带内部反馈的循环SNN。
     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129085508426.png" alt="image-20240129085508426" style="zoom:67%;" />

     

     具有外部反馈的循环SNN称为Jordan循环神经网络，其中输出层和隐藏层神经元之间的连接构成了反馈回路。上图—图13显示了带外部反馈的循环SNN的架构。
     具有内部反馈的循环SNN称为神经Moore机或Elman循环神经网络，其中隐藏层神经元的输出不仅向前输入到输出层，而且也返回到隐藏层自身。图14显示了带内部反馈的循环SNN的架构。

     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129090256939.png" alt="image-20240129090256939" style="zoom: 67%;" />

  2. 循环SNN的学习算法

     * 梯度下降
       Back-propagation through time（BPTT）是循环神经网络的监督算法之一。学者们扩展了该算法到SNN，形成了时序SpikeProp（SpikeProp through time ）(SPTT) 监督学习算法。

       还有其余基于梯度下降的算法

     * KL（Kullback-Leibler）散度算法
       一种通用的循环SNN监督学习规则：

     * 其余监督学习算法

  3. 脉冲序列学习性能比较

     * 在这一节，比较了FOLLOW（Gilra & Gerstner, 2017）和R-STKLR（Lin & Shi, 2018）的脉冲序列学习表现

* **Qualitative performance evaluation and taxonomy**    定性性能评估和分类方法

  1. 有监督学习算法的定性性能评价
     主要从以下几个方面进行评估：

     * 脉冲序列的学习能力
       尽管监督学习算法的应用与所选择的脉冲编码方法有关，从监督学习算法本身的表现来看，一些算法可以实现多脉冲学习任务，而另一些算法只能实现单脉冲学习任务。一般来说，基于脉冲序列学习的监督学习算法具有更强的学习能力和更广泛的适用性，但也比单脉冲学习算法更为复杂。

     * 离线和在线处理性能
       对于在线学习算法，学习时期内的突触权重被更新多次。突触权重的时变调整通常表示为∆w(t)。相比之下，离线学习算法中，学习时期内的突触权重只被更新一次，突触权重的调整通常表示为∆w。一些算法不仅可以在离线模式下运行，还可以在在线模式下运行，而另一些则只能在离线模式或在线模式下运行。对于能够以两种模式运行的算法，在线学习和离线学习中突触权重的更新具有以下关系：

       ![image-20240129095217651](../../../AppData/Roaming/Typora/typora-user-images/image-20240129095217651.png)

       

     * 学习规则的局部性质
       局部性意味着学习规则仅由神经元的突触前和突触后活动以及突触权重本身决定。

     * 最优解的稳定性

     * 脉冲神经元的适用性
       脉冲神经元是SNNs的基本计算单元。有许多SNN的监督学习算法，尤其是基于梯度下降的算法，仅限于使用分析可追踪的脉冲神经元模型，例如SRM等。然而，一些监督学习算法仅显式依赖于脉冲时间，而不涉及特定的脉冲神经元模型的属性。预期该算法应该独立于所使用的脉冲神经元模型正确地运行。

  2. **A taxonomy for supervised learning algorithms**  监督学习算法的分类体系

     * 脉冲序列学习
       单脉冲或多脉冲序列学习。
     * 运行模式
       离线或者在线。
     * 局部性
       
     * 最优解的稳定性
       
     * 脉冲神经元模型

* **未来方向：**
  一般来说，SNN的监督学习是一个重要的研究领域。国内外学者对此进行了大量的研究，取得了一系列丰硕的成果。然而，由于SNN固有的复杂性，很难构建具有广泛适用性的监督学习算法。有许多难题需要解决，新的学习机制和算法需要探索。通过分析和总结目前SNN的监督学习算法，我们可以预测未来需要解决的主要问题。

  1. Online supervised learning for real-time problems  实时问题的在线监督学习
     目前，面向 SNN 的在线监督学习算法较少，特别是基于梯度下降的监督学习算法。通过定义脉冲序列的实时误差函数并设定合理的突触权重调整方式，可以实现 SNN 中突触权重的实时调整。
  2. Supervised learning with various synaptic plasticity   基于多种突触可塑性的监督学习
     研究表明，突触前和突触后脉冲列的时序可以引起突触的长时程增强或长时程抑制。突触增强和抑制的诱导还伴有突触前神经元总体兴奋性的双向改造。突触增强或抑制能够逆行并快速传递到突触前神经元树突上的突触。突触前神经元的这种特异且快速的突触可塑性反向传播，与反向传播算法中的机制相似。它可以在生物神经网络中存在并发挥作用。面向复杂 SNN 结构构建利用多种生物突触可塑性机制的监督学习算法具有重要意义
  3. Supervised learning for hybrid SNNs    混合SNN的监督学习
     前馈 SNN 已被广泛研究用于处理非时态问题。然而，前馈 SNN 不能很好地解决现实世界的时态任务。循环 SNN 能够表示更加复杂的时间变化系统。将前馈 SNN 和循环 SNN 的优点结合构建高效的混合 SNN 具有重要意义。  ==强调了结构的动态性==
  4. Supervised learning for deep SNNs    深度SNN的监督学习
     未来有必要构建融合传统人工神经网络与 SNN 深度学习的有效计算模型，如脉冲深度信念网络和脉冲卷积神经网络。此外，应针对大规模脉冲深度神经网络构建高效的监督学习算法。
  5. Hardware implementation of supervised learning  监督学习的硬件实现
     
  6. Large-scale SNNs for pattern recognition   用于模式识别的大规模SNN
     由于 SNN 具有精确的脉冲时序编码和脉冲驱动的非线性动力学，因此它具有很强的自适应性和容错性。然而，由于现实世界中时间或时空数据复杂，SNN 大规模应用于模式识别的范围还不够广，解决复杂问题不够准确。

* **总结：**

  * 针对 SNN 脉冲精准时间编码和神经信息处理的特点，本文对 SNN 的监督学习算法进行了全面的综述。

 brain-inspired artificial intelligence  类脑人工智能 


4. ## ==A Survey of Neuromorphic Computing and Neural Networks in Hardware==

​		硬件中神经形态计算与神经网络研究综述
​		正文只有22页，第22-88页全是参考文献

* **期刊：** arxiv 2017.05.19

* **作者及单位：**  Catherine D. Schuman  

* **摘要：** 在这项工作中，我们对神经形态计算的研究和动机进行了全面调查。

  概括归纳为：发展动机，变化历史，模型，算法，学习，硬件实现，支持组件，通信方案，软件系统，应用类型，前瞻总结与挑战等。

  ![image-20240130100115019](../../../AppData/Roaming/Typora/typora-user-images/image-20240130100115019.png)

  

* **介绍：** 这些神经形态架构以其高度连接和并行性、低功耗以及将存储与处理协同放置而备受瞩目。虽然神经形态架构本身就很有趣，但由于摩尔定律接近尾声、登纳德缩放所关联的能量需求增加，以及所谓的冯·诺伊曼瓶颈——CPU和内存之间的低带宽，导致人们对神经形态架构的关注越来越多。神经形态计算机有潜力以比传统冯·诺伊曼架构更快、更节能、占地面积更小的方式执行复杂计算。这些特征提供了发展采用神经形态架构的硬件的充分理由。机器学习提供了对神经形态计算兴趣浓厚的第二个重要原因。这种方法显示出在改善某些任务的整体学习性能方面有潜力。这将从硬件的好处转移到了理解神经形态计算的潜在应用优势，承诺发展能够进行在线、实时学习的算法，类似于生物大脑的学习方式。神经形态架构似乎是未来实施机器学习算法最适合的平台。

* **动机：** 

  * 本文的重点并非直接放在人工神经网络或神经科学本身，而是关于开发非冯·诺伊曼硬件来模拟人工神经网络或生物神经系统。
  * 神经形态早期阶段：并行性，速度以及实时性是他的三个主要优势。
  * 神经形态系统的最受欢迎的动机是：低功耗，体积小。
  * 神经形态计算开发的另一个常见动机是解决冯·诺伊曼瓶颈[22]，这是由于目前系统中存储器和处理器的分离以及处理器和存储器技术之间性能差距所导致的。在神经形态系统中，存储和处理是合并的，减轻了冯·诺依曼瓶颈带来的问题。
  * 近年来，在线学习被定义为在任务发生变化时适应这些变化的能力，这已成为神经形态系统的一个关键动机。
  * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240130112705434.png" alt="image-20240130112705434" style="zoom:50%;" />
  * 很明显，动机主要就是图中的几条：**实时性能、并行性【早期，自定义硬件】、冯·诺依曼的瓶颈、可扩展性、低功耗【人类大脑：20W】、封装大小、容错、速度更快、在线学习、神经科学**

* **方法：**  

  * 如果神经形态设备的目标是利用设备以比传统冯·诺依曼结构更快的速度模拟生物大脑进行神经科学研究，那么就需要一个具有生物学逼真和/或合理的模型。如果应用是需要高准确性的图像识别任务，那么实现卷积神经网络的神经形态系统可能是最好的选择。

  1. Neuron Models  神经元模型
     神经元模型一般分为以下几类：

     * 生物可行性：明确模拟生物学中观察到的行为类型。
     * 生物启发：试图复制生物神经系统的行为，但不一定用生物上可行的方式。
     * 神经元+其他：神经元模型包括其他生物启发组件，通常不包括在其他神经形态神经元模型中，如轴突、树突或胶质细胞。
     * 整合-激发模型（Integrate-and-fire）：生物启发脉冲神经元模型的简化类别。
     * McCulloch-Pitts：神经元模型是原始McCulloch-Pitts神经元的衍生物，它是人工神经网络文献中使用的。对于这个模型，神经元j的输出受以下方程控制：
       <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240130113506878.png" alt="image-20240130113506878" style="zoom:50%;" />

     

     已经在硬件中实现了各种生物可行和生物启发的神经元模型。这些模型可能包括以下组成部分：细胞膜动力学，负责控制神经元细胞膜上的电荷泄漏等因素；离子通道动力学，负责控制离子进出神经元，改变神经元的电荷水平；轴突模型，可能包括延迟组成部分；树突模型，控制前后突触神经元对当前神经元的影响。
     生物可行性；生物启发；神经元+其他生物启发机制；整合-激发模型；McCulloch-Pitts Neurons(麦卡洛-皮茨神经元)

     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240130115414392.png" alt="image-20240130115414392" style="zoom: 67%;" />

     ![image-20240130115221580](../../../AppData/Roaming/Typora/typora-user-images/image-20240130115221580.png)

  2. Synapse Models   突触模型
     分为两类：

     * 受生物启发的突触实现
       包括基于脉冲的突触系统。
     * 用于传统人工神经网络 ANN 的突触实现。
       例如前馈神经网络

     特点：突触在神经学系统中最为丰富，许多硬件实现集中在优化突触实现上，因此突触往往相对简单。更        

     ​           复杂的突触模型中使用的是STDP(突触可塑性)。

     突触响应也非常复杂；突触的学习规则。

  3. Network Models  网络模型
     在选择网络模型时有许多因素需要考虑。其中一个因素显然是在前面章节中讨论的神经元和突触模型的生物启发和复杂性。另一个要考虑的因素是网络的拓扑结构；第三个因素是现有的训练或学习算法对所选网络模型的可行性和适用性；网络模型的一般适用性对一组应用也可能在选择适当的网络模型上发挥作用

     

  4. 

* **试验：**

* **讨论与总结：**



5. ## ==测试==



* **期刊：**
* **作者及单位：**
* **摘要：**
* **介绍：**
* **方法：**
* **试验：**
* **讨论与总结：**









# ==文献阅读   ANN  to SNN文章==

1. ## ==Spiking deep convolutional neural networks for energy-efficient object recognition==

   脉冲卷积神经网络做高效的目标识别
   [博客链接](https://blog.csdn.net/h__ang/article/details/91345702?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522170592752916800227461663%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=170592752916800227461663&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-3-91345702-null-null.nonecase&utm_term=snn&spm=1018.2226.3001.4450)

* **期刊**：International Journal of Computer Vision  2区 计算机科学 IF=19.5 2014年 
  
* **摘要：** 对CNN进行裁剪，以训练未裁剪CNN的方式继续训练已经裁剪的CNN，接着将学习到的网络权重直接应用到SNN上， 实现了ANN到SNN的转换，并对比了硬件能耗。

* **1 介绍：** 介绍了一些历史性的东西

* **2 Method**

  * 2.1 传统的CNN结构

  * 2.2 实现脉冲型CNN的挑战

    * 将CNN转换为SNN有两种方法，第一种就是直接训练类似CNN结构的SNN，尽管存在一些像STDP的脉冲学习规则以一种自组织、无监督的方式训练SNN，但在这方面的研究目前还处于起步阶段，并且还不清楚如何高效训练SNN来实现更高层次的功能；

      第二个方法就是训练原始的CNN，然后将学习到的权重应用到和训练的CNN结构类似的SNN中。主要的挑战就是在将CNN转换为SNN时分类任务有不可接受的精度损失。

    * CNN转换到SNN有精度损失存在于以下几个方面：

      1. CNN层的负输出值在SNN中很难精确的表示（==例如tanh激活函数，幸运的是近几年的模型大多使用Relu激活函数，所以也就不存在负值了，这篇文章写的比较早，当时ReLU还不火。==）
      2. 不像在CNN中，在脉冲网络中没有什么好办法表示偏置，每个卷积层的偏置可以是正也可以是负，这在SNN中不容易表示；
      3. Max-pooling需要两层脉冲网络，在CNN中，空间上的最大池化就是在一个区域内求最大值，在SNN中，我们需要两层网络来实现它，横向抑制后紧跟着小区域的池化操作，这个方法需要更多的神经元，而且由于额外的复杂度造成了精度损失。   ==没有看到如何转化的，横向抑制说的应该是在时间维度上的操作，紧接着在去掉时间维度后进行最大值的筛选或者求平均值来实现最大池化或者平均池化==

  * 2.3 本文提出的脉冲型卷积网络 

    * 裁剪CNN的方式：

      1. 将所有层的输出置为正——在预处理之后加上abs()函数来使得第一层卷积的输入为非负的，当预处理本身就可以得到非负值的时候这个步骤可以省略（对于CNN来说）；将tanh()函数变为 ReLU()激活函数。

         另外文章中说到使用ReLU激活函数的优点在于：其在大于0时的激活值的线性的，这在将CNN转换为SNN时最大限度的减少了准确性的损失。==直观来看的话是IF神经元模型模拟出的激活值可以和RELU激活函数相匹配==

      2. 将卷积层和全连接层的偏置都置零；
      3. 使用空间线性子采样代替空间最大池化。空间线性子采样使用总和为1.0的均匀权重的内核将小图像邻域上的所有像素相加。空间线性子采样函数可以很容易地转换到尖峰域。==这里的意思感觉就是在做平均池化==

    * ==裁剪后的CNN结构变为SNN的方式：==  新增脉冲产生和脉冲计数模块，其余模块均是剪枝后的CNN中继承过来， HalfRect激活函数就是ReLU。 

    * ![image-20240125094734190](../../../AppData/Roaming/Typora/typora-user-images/image-20240125094734190.png)

    * SNN结构中脉冲神经元的膜电位$V(t)$根据 integrate-and-fire 神经元模型导出的下面的等式在每个time step更新：![image-20240125095335669](../../../AppData/Roaming/Typora/typora-user-images/image-20240125095335669.png)

      

      ![image-20240125100021269](../../../AppData/Roaming/Typora/typora-user-images/image-20240125100021269.png)

      脉冲产生层经常定义为：$I_{ijk}(k=1,2,3)$ 可以看作是脉冲产生层的输入,参照上图中的$X_{ij}(t)$，在t时刻，在第k个 image map (通道)的神经元（i，j）, 如果满足 随机数(在0和1之间)  $rand() < cI_{ijk}$ ，则发射一个脉冲，否则不发射，这里c是一个对脉冲产生频率的缩放量。举个例子，我们一般取c=1/3，RGB图片的输入先被正则化到 [0.0, 1.0] 的范围，脉冲计数对所有类别神经元发射的脉冲进行计数（时间是从输入图片被送入网络开始的整个时间周期），然后发射脉冲最多的神经元对应的就是最终的输出类别。

      ==问题：需要去看原始的代码，因为整体看下来还是感觉说的不够清楚，没法自己去复现。==

* 3 ==Experiments 实验==

  * Tower Dataset1
    ![image-20240123091101700](../../../AppData/Roaming/Typora/typora-user-images/image-20240123091101700.png)

    综合结果来看，仍然是原始的CNN具有很好的结果，接着是裁剪后的CNN，以及SNN。且SNN在最后两个类别的识别效果相比前两个下降的比较多。整体来看，ANN-SNN的转化还是很成功的，不过从CNN结果来看，该数据集还是太简单才导致性能看起来没有下降很多。

  * Tower Dataset2
    ![image-20240123091353418](../../../AppData/Roaming/Typora/typora-user-images/image-20240123091353418.png)

    在数据集2中，SNN反而效果更好了，初步验证SNN或许真的可以替代ANN，==仍然可以认为数据集过于简单，考虑到这个原因，作者在后续也对其他数据集进行了实验==

  * CIFAR-10数据集
    这里作者用的CNN架构并不是性能最好的，因为复杂的CNN架构中有些组件不能转换成SNN，所以就用了一个简单的CNN，并将其转换为SNN去对比结果。这里他们在转SNN架构时，直接先对原始图像进行了归一化的操作。

    ![image-20240123091913868](../../../AppData/Roaming/Typora/typora-user-images/image-20240123091913868.png)
    可以看到，结果还是很好的，但是问题依旧是：在简单的CNN架构上进行转换，没有高性能可言，且通用性不够高

* ==4 Energy Analysis of Mapping to Neuromorphic Hardware==    映射到神经形态硬件上的能效分析

  * ![image-20240123093005295](../../../AppData/Roaming/Typora/typora-user-images/image-20240123093005295.png)

  * 这个表是基于两个已经发布的基于脉冲的神经形态电路的功率特性的总功耗。最后一列中的比例是 CNN功耗/SNN功耗，可以看出第一行方法实现的SNN结构非常节能。

    

* ==5 讨论部分==

  * 1.在输入特征上使用绝对值函数 abs()
  * 2.与有限分辨率CNN实现的比较

* ==6 总结部分==

  CNN转换为SNN需要考虑一下几点：

  * 负值的变换
  * CNN所有的激活函数需要使用 ReLU
  * 将所有的偏差置为0

2. ## ==Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing==

   权重、阈值权衡实现快速分类且高精度的脉冲神经网络
   [参考博客](https://blog.csdn.net/h__ang/article/details/91128509)
   
   * **期刊**：2015 International Joint Conference on Neural Networks IJCNN    CCF-C会议
   
   * **摘要： ** 由于ANN没有时间信息，所以转换为稀疏的脉冲发射型、事件驱动的SNN会有精度损失。作者分析了脉冲神经元的发射率和阈值这些参数的选择对ANNs转换为SNNs的影响，展示了一组优化技术来最小化转换过程中的性能损失。这些技术产生的网络优于之前在MNIST数据集上的表现最好的网络，包括在训练期间使用修正后的零偏置的线性单元（如ReLUs）、使用一种新的权重正则化方法帮助正则化发射率。
   
   * **1. 介绍：**
   
     * 在本篇论文中，我们将介绍对于深度脉冲神经网络的新的优化方法，它可以使得脉冲神经网络获得比之前的脉冲方法更高的性能，同时实现了低延迟、更少的操作。
   
       与在传统CPUs或GPUs上跑神经网络相比，神经形态平台的功耗可能要低好几个数量级，因为它允许分布式和异步基于事件驱动的计算，因此提高了可扩展性和减少了延迟。
   
       提到了上一篇文献，最近，“Spiking deep convolutional neural networks for energy-efficient object recognition”提出了一种转换的方法，它的表现比以前的方法都好，因为它将脉冲和非脉冲网络之间的特征差异考虑了。主要挑战就是对于脉冲神经元中的负值和偏置的表示，这个通过使用修正的线性单元（ReLUs）和将偏置设为零解决了。同时，卷积网络的最大池化操作被空间线性子采样替代，==同样地转换结果也有很小的损失。==
   
       在这篇工作中，我们展示了这种微小损失的来源，并且展示了优化的几种工具。我们发现如果SNNs以正确的方式驱动，接近无损的转换是可能的，并且还可以进行非常快速的分类仅基于少量输出峰值。
   
   * **2. NEURAL NETWORK ARCHITECTURES**  神经网络结构
     ==简单介绍了一下，这里不扩展了==
   
     * 2.1 基于ReLU的前馈神经网络
     * 2.2 卷积神经网络
     * 2.3 Dropout
   
   * **3. Spiking Neural Networks** 脉冲神经网络
   
     * 3.1 背景
   
     * 在传统的ANN中，整个输入向量同时被送入网络，然后逐层处理，最终产生输出值。在SNN中，输入通常以事件流的形式传入网络，然后神经元在这个时间段内整合数据，产生脉冲用于将信息传递给后续层，最终驱动输出神经元发射脉冲。这种方法有非常重要的优势：输入和输出的“假象同步”可以实现，并且时变输入可以更高效地处理，同时在特殊硬件上的更加高效的计算可以实现。
   
     * 3.2 脉冲网络转换
       基于上一篇文章(Spiking deep convolutional neural networks for energy-efficient object recognition)提出的转换方法，他们添加了新颖的正则化方法、发射率和阈值的分析。
   
       给出了ANN TO SNN 转换的几点建议：
   
       1. 对于网络中的所有单元使用ReLU；
       2. 在训练过程中将偏置设置为0，并利用反向传播更新网络参数；
       3. 直接将ReLU网络的权重映射为IF单元网络； ==仿真结果显示这两者在0-1之间的函数曲线较为一致==
       4. 使用权重正则化来获得接近无损的精度和更快的收敛；==其实就是引入了缩放比例==
   
       上面的这些建议适用于全连接层和卷积层，一旦人工神经网络中的ReLU在训练后被IF神经元取代，那么在固定仿真期间的性能损失主要来源于三个因素：
   
       1. 单个时间步内==没有接受充分的输入==使得膜电位超过阈值，即发射率低于它原来应该达到的发射率；
       2. 一个时间步内，单个时间步内接受的输入过多而导致输出脉冲过多。
       3. 结合前两点：脉冲输入的随机性与不均匀性，导致脉冲过激活或者欠激活。
       
       然而，我们要在脉冲阈值、输入权重和输入发射率之间找到一个权衡。具体来讲，高的输入阈值（或低的输入权重）会减小过激活和非理想脉冲序列的错误，然而同样增加了欠激活的风险，反之亦然。请注意脉冲阈值和输入权重的比率决定了脉冲的数量，不用手动调整参数，在这里我们提出了一个更加严格的方法来调整网络权重——通过计算权重的缩放因子对权重归一化从而减少了上面说的三个因素带来的错误。
       
     * 3.3 权重正则化
     
       这里提出了两种方法来对网络的权重归一化，并且确保激活值足够小，能够防止ReLU过高估计输出激活值。
     
       * (1) 最安全、最保守的方法就是考虑所有可能的正的激活值，并且通过可能的最大激活值（同时也是下一层的输入）对权重缩放。如果最大的正输入仅仅对应单个脉冲，那么网络同一个神经元在一个timestep内至多发射一个脉冲。通过这样做，脉冲神经网络变得具有鲁棒性——可以接受任意高的输入发射率并且完全消除由于太多输入而导致的转换损失。不幸的是，这意味着为了产生一个脉冲整合数据可能要花费很长时间，如果一个分类任务对性能要求比较高且可以接受更长的采样时间，这种方法就很合适。
     
         这种方法可能是一种寻找合适的权重缩放因子的方法。我们常将这种方法称为"model-based normalization"
     
       * (2) 训练集也可以用来评估网络中的激活值，而不是假设最大正激活值的最坏情况。在我们的实验中，观察到使用第二种方法创建的这个缩放因子相对保守，它的精度几乎没有损失，且极大地缩短了整合时间。对于这个方法，在训练完网络后，训练集前向传播，我们将ReLU过后的激活值保存下来，接着，权重根据最大的可能激活值缩放，所以这种情况下也只会同时发射一个脉冲。另外，这种方法也需要将最大的输入权重考虑进去（即它的缩放因子是激活值和权重值平衡之后的最大值），因为如果不考虑权重的话还是有可能出现在一个timestep内需要发射多次脉冲——此处的意思是可能存在多个最大值。
     
         然而这不是一个强有力的保证——保证在测试集上也可以维持这种性能表现，训练集应该代表测试集，同时结果显示了这种方法是很有效的。我们将这种方法称为"data-based normalization"。
     
       * **总结上面两个方法，第一个是只考虑最大值，第二个是考虑输入值与最大值**
     
       * ==和上一篇文章一样，依旧没有具体的实现，只有类似的伪代码==
     
     * **4 实验设置**
     
       * 4.1 数据集
         本文选的是MNIST数据集，之前脉冲神经网络实现手写数字分类的最高精度为 98.30%。
     
       * 4.2 结构
         两种网络结构，一个是四层的全连接网络，一个是卷积网络，测试集精度相差不大。
     
       * 4.3 脉冲输入
     
         先将MNIST图片的像素点都归一化到0-1，然后基于这些像素点，对于每个像素点利用泊松分布产生与像素值成正比的脉冲序列。
     
     * **5 结果**
     
       * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240123120357940.png" alt="image-20240123120357940" style="zoom: 50%;" />
     
         
       
       * ![image-20240123121219661](../../../AppData/Roaming/Typora/typora-user-images/image-20240123121219661.png)
         作为输入速率和发射阈值的函数，不同架构的分类性能和产生的脉冲数量。上图显示了ConvNets的结果，下图显示了FCN的结果。每个圆的颜色代表对MNIST测试集的平均准确性（5次试验的平均值），对每个输入示例使用0.5秒（500个时间步）的积分时间。圆的大小对应于整个网络每个示例呈现时产生的平均脉冲数量。右侧面板显示了标准化网络的相同数据，其中阈值在所有实验中都固定为1。导致测试错误率大于1.15（ConvNet）或2.2（FCN)的参数设置均未显示。
     
         通常，在增加阈值以在传播特征的检测之前整合更多尖峰与减小阈值以减少产生足够数量的尖峰所需的采样时间以最小化由于所传输消息的离散化而导致的误差之间存在权衡
     
         令人惊讶的是，全连接网络和卷积网络中生成的尖峰数量相当，尽管全连接网络使用的突触比卷积网络多60%。
     
       * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240123121513009.png" alt="image-20240123121513009" style="zoom:50%;" />
     
         
       
       * 使用SNN的原因之一是它们的可配置性。如果需要高精度，则高尖峰阈值有助于提高精度;如果短延迟很重要，则低发射阈值确保仅在几个输入尖峰之后响应。
     
   * 6 结论：作者研究了SNN中性能损失的典型来源，并提出了如何最好地解决这些问题的方法。
   
3. ## ==Conversion of continuous-valued deep networks to efficient event-driven networks for image classification==

   将连续值深度网络转换为高效的事件驱动网络进行图像分类     即 ann-snn 去做图像分类
   [博客链接](https://blog.csdn.net/h__ang/article/details/90609793)

* **期刊**：Frontiers in Neuroscience   3区 医学  IF = 4.3

* **摘要：** 17年的文章，当时卷积神经网络流行的cnn架构是Vgg-16,Inception-V3等，作者将这些网络模型转换成了Snn,并在MNIST、CIFAR-10和ImageNet数据集上取得了较好的结果。实验结果表明，在错误率仅增加几个百分点的情况下，SNN实现了计算次数2倍以上的减少，凸显出SNN的节能作用。

* **介绍：** 一个重点就是深度SNN可以在产生第一个输出脉冲之后就去查询结果，而ANN必须在所有层都处理完之后，最后一层产生结果。SNN适合处理来自基于事件的传感器的输入。多层脉冲神经网络已经在FPGA上实现了(2014年)。  为了减小深度ANN和SNN的差距，有必要开发深度SNN。成功的方法包括使用反向传播训练SNN，使用随机梯度下降来训练SNN分类器层，或者在训练期间修改ANN的传递函数，以便更好地将网络参数映射到SNN。此前有学者在AlexNet使用了最后一个方法，结果很不错，但是这种方法还没有被应用到VGG-16等大型网络上。 还有一种更直接的方式是使用预训练的ANN，并将其映射到SNN。  关于ANN-SNN的转换早在2013年就有人开始研究， 2015年有学者提出SNN输入输出关系与激活函数RELU之间的关系。 之后陆续有学者提出各种方法。==说到了前面的两篇文章==

  ==作者的见解：== 前面的这些方法在MNIST数据集上取得了很好的结果，但是当扩展到CIFAR-10数据集上时，SNN的准确率就低了。一个原因是ANN中的许多操作(最大池化层，softmax函数，batchnorm归一化)在SNN中是不存在的，因此作者说以前各类学者提出的方法都不通用。

  ==作者工作的贡献：== 解决了一些ANN-SNN转换的缺点。1 通过对脉冲神经元输出发射速率对等模拟激活值的近似的数学分析，我们能够推导出先前转换过程引入的误差的理论度量。基于这一新理论，我们提出了对脉冲神经元模型的修改，显著改善了深度SNN的性能。2 通过开发最大池化层、softmax激活、神经元偏置和批量归一化（Ioffe和Szegedy，2015）的脉冲实现，我们扩展了可以进行转换的CNN套件。特别地，我们首次演示了GoogLeNet Inception-V3可以被转换为等效准确的SNN。此外，我们还表明转换为脉冲网络与ANN网络压缩技术（如部分连接神经网络压缩）是协同的。  

* ==METHODS==  

  * Theory for Conversion of ANNs into  SNNs

    * ANN转换为SNN的基本原理是：脉冲神经元的发射速率应该与模拟神经元的激活值相匹配。==**这句话的意思：**脉冲神经元通常根据其发放的脉冲速率来进行建模，也就是单位时间内发放脉冲的次数。而模拟神经元（比如多数深度学习模型中的神经元）则使用一个连续的激活值（比如介于0到1之间的一个实数）来表示神经元的活跃程度。因此，这句话的含义是，当我们将模拟神经元替换为脉冲神经元时，脉冲神经元的发射速率应当能够与模拟神经元的激活值相匹配，以便在神经网络模型中能够正确地传递和处理信息。这也反映了神经元模型之间的转换和对应关系。==

    * ![image-20240126153624566](../../../AppData/Roaming/Typora/typora-user-images/image-20240126153624566.png)
      ![image-20240126153836111](../../../AppData/Roaming/Typora/typora-user-images/image-20240126153836111.png)

      

    * **2.1 Membrane Equation  膜方程**

      膜电位的重置有两种，一种是重置为0，另外一种就是减去阈值。不断对输入的z进行加和，当超过阈值时，就重置![image-20240122155824557](../../../AppData/Roaming/Typora/typora-user-images/image-20240122155824557.png)

      ![image-20240122170039585](../../../AppData/Roaming/Typora/typora-user-images/image-20240122170039585.png)

      从推导出的结果来看，第二种重置机制更有利于深层网络。（第一种引入了一个乘法误差项，且没有考虑膜电位超过阈值的部分，会导致信息的损失）

    * Firing Rates in Higher Layers   更高层的发射率
      ![image-20240122171117798](../../../AppData/Roaming/Typora/typora-user-images/image-20240122171117798.png)

      ![image-20240122171418395](../../../AppData/Roaming/Typora/typora-user-images/image-20240122171418395.png)

      
      
      上面两个公式就解释了误差项的累加问题，这也解释了为什么实现与人工神经网络激活值高相关性的发射率需要更长的时间，和为什么SNN发射率在高层会恶化的原因。

      ==细致的解释-gpt：==实现与人工神经网络激活值高相关性的发射率需要更长的时间是因为要在脉冲神经网络中逼真地模拟人工神经网络中的激活值，需要对膜电位的动态变化进行建模，并且需要考虑时间本身是离散的，而人工神经网络中的激活值是连续的。这就需要通过脉冲神经元的膜电位和随时间变化的发放脉冲来实现类似的激活值。因此，为了确保发射率与激活值高度相关，脉冲神经元需要更多的时间来处理和传递信息。
      关于SNN发射率在高层会恶化的问题，这可能与在网络的高层中信息的抽象和稀疏性有关。随着神经网络的层次结构越来越高，神经元的响应变得更加抽象和稀疏。这意味着在脉冲传播过程中信息的丢失可能会增加，这可能会导致在高层中发射率的退化。在这种情况下，神经元的发放脉冲可能无法准确地反映网络中的高级特征，从而导致与激活值的高相关性出现问题
      
    * **2.2 ANN操作的脉冲实现**
      在本节中，我们介绍一些新方法改善深度SNN的分类错误率，这些方法将会允许一个更宽范围ANN的转换，也会减少SNN的近似误差。

      * 2.2.1 Converting Biases   转换偏置

        * 以前的ANN到SNN的转换是没有考虑偏置的。在SNN中，偏置其实可以使用一个与偏置符号相同的常量输入电位来实现。

      * 2.2.2 Parameter Normalization 参数归一化
        近似误差的来源之一就是在模拟SNN时的时间步长，神经元的发射率被限制在一个区间 [0,$r_{max}$]  ，然而ANN通常不会有这样的约束。权重归一化是被等人引入作为一种避免由于太高或太低的发射率而造成的近似误差的方法。通过使用一种“data-based weight normalization mechanism”的方法展示了将ANN转换为SNN时性能上的提升，我们将这个方法扩展到带有偏置的神经元上，提出了一种方法使得归一化过程对异常值更加稳健。==这里提到的基于数据的归一化方法就是上一篇文章中说的==

        * Normalization with biases 带偏置的归一化
          定义$λ^q = max[a^l]$, $a^l$就是ANN通过RELU激活函数的值
          ![image-20240122173155195](../../../AppData/Roaming/Typora/typora-user-images/image-20240122173155195.png)

          data-based权重归一化机制是使用ANN中的ReLU单元的线性度，它可以简单地扩展到偏置，通过线性的缩放所有的偏置和权重以使得ANN的激活值a对于所有的训练样本都小于1。为了保留一个层内编码的信息，一个层的参数需要联合缩放。定义$λ^l = max[a^l]$, 如上图所示。

          这种权重归一化很好的控制了SNN中发射率饱和的问题，但可能会导致非常低的发射率，当原始的权重值很小的时候，这种做法的缺点明显在：如果存在很大的离群值，经会导致大部分归一化之后的权重值都很小。
          
        * Robust normalization  鲁棒归一化
          因为有上述问题，所以作者团队提出了一个更加稳健的替代方案——选择激活值中比例为p处对应的激活值，显然这样做会导致一些很大的激活值饱和——**所以选择归一化因子就是在饱和和不充分的发射率之间权衡**。

          通常来讲，一小部分神经元饱和导致的分类错误率比要比发射率过低这种情况小很多，这种方法也可以在训练期间和BN层相结合，对每一层进行归一化，因此会使得异常值少很多。
          
          ![image-20240126164441954](../../../AppData/Roaming/Typora/typora-user-images/image-20240126164441954.png)
          
          如图1A所示，图1A绘制了16666个CIFAR10样本的第一卷积层中所有非零激活的对数尺度分布。观察到的最大激活是第99.9百分位数的三倍以上。图1B显示了同一层中所有ANN单元的16666个样本中最高激活的分布，揭示了整个数据集的大方差，以及远离绝对最大值的峰值。这种分布解释了为什么通过最大值进行归一化可能导致SNN的分类性能可能较差。对于绝大多数输入样本，即使层内单元的最大激活也将远低于所选择的归一化尺度，导致层内的激发不足以驱动更高的层，并随后导致更差的分类结果。

      * 2.2.3 Conversion of Batch-Normalization Layers  BN层的转换
        ![image-20240122190346024](../../../AppData/Roaming/Typora/typora-user-images/image-20240122190346024.png)

      * 2.2.4 Analog Input to First Hidden Layer  第一个隐藏层的模拟输入
        ![image-20240126165044982](../../../AppData/Roaming/Typora/typora-user-images/image-20240126165044982.png)
        
      * 2.2.5 Spiking Softmax
        softmax通常用在深度ANN的输出，它会有一个归一化的效果，并且使得输出像是类别概率。之前的ANN-to-SNN转换的方法不会转换softmax层，而是通过在仿真期间神经元的==脉冲发射率==来确定预测的输出类别的，但是这个方法当最后一层接受的都是负输入时会失败，因为没有脉冲发射。

        * 第一种输出脉冲是由固定发射率的额外的泊松生成器触发的，脉冲神经元自己不发射，只是简单的累加它们的输入，脉冲是否发射由额外的生成器产生。
        * 第二种脉冲softmax函数是类似的，但是不依赖额外的时钟，为了确定神经元是否发射脉冲，我们需要计算脉冲电位的softmax输出，使得输出值在[0,1]之间，这两种方法的最终分类结果都是根据仿真时间内发射率最高神经元索引决定的。我们倾向于选择第二种方法，因为这种方法仅依赖于一个超参数。
        * 第三种方法是基于我们只需要根据softmax层的==输入膜电位==就可以确定最终的分类结果，这个简化可以加速推断时间，也可以通过减少随机性来提高精度。

      * 2.2.6. Spiking Max-Pooling Layers   脉冲最大池化层
        在这里，我们提出了一个简单的转换max-pooling机制，输出单元包含一个门控功能，它只会让==最大值发射率==神经元通过，而丢弃其他神经元，门控功能通过计算前突触发射率的估计值来控制。

    * 2.3 **Counting Operations**  计算量的统计

      * ![image-20240126165846393](../../../AppData/Roaming/Typora/typora-user-images/image-20240126165846393.png)
        
      
      ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190530171136419.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hfX2FuZw==,size_16,color_FFFFFF,t_70)
      
      这里的时间步T其实还是需要再实际中测试的，不一定说T越大越好。

  * ==3 Results==
    两种方法来改善转换为SNN后的分类错误率：

    1. 在转换之前训练一个更好的ANN
    2. 通过消除SNN的近似误差改善转换。

  * ==4 Discussion==
    这项工作提出了两个新的发展。第一个是一个新的理论，描述了近似的SNN发射率，其等效的人工神经网络激活ReLU。第二个是将几乎任意连续值的CNN转换为脉冲等价物的技术。

    

* ## 4 ==Spiking deep residual network==  

   脉冲深度残差网络  这篇文章提到了前面3篇文章

   * **期刊**：IEEE Transactions on Neural Networks and Learning Systems 1区top 计算机科学 IF=10.4 2021年11月1号见刊，但是2018年4月28日就已经在arXiv上发布。  Hu Yangfan  浙江大学

   * **摘要：**对于SNN来说，训练深层SNN仍然是一个巨大的挑战，而ResNet是一个先进的卷积神经网络可以训练深层的神经网络。所以就产生了将卷积的深度残差网络转换到脉冲版本的念头。将一个被训练的ResNet变成一个脉冲神经元网络——S-ResNet。作者提出了一个捷径转换模型，适当地缩放连续值激活以匹配SNN中的发射速率 ；和一个补偿的机制，去减少因为离散化造成的误差。

   * **1 介绍：** 目前，如何训练SNN仍然是一个开放性挑战，原因是其脉冲机制的不连续性。已经付出了许多努力来解决这个问题。这些方法可以分为四类。

     1. 第一类旨在通过一些近似方法使SNN可导，并应用梯度下降。
     2. 第二类从生物神经元汲取灵感，利用突触可塑性规则。例如stdp学习规则。
     3. 第三类将SNN视为随机过程，并使用概率推理进行学习。

        ==然而，这三种方法尚未得到充分发展，无法处理深层架构。==

     4. 最后一种方法采用转换思想，缩小SNN和ANN之间性能差距。它训练常规的ANN并构建转换算法将权重映射到等效的SNN。尽管这种方法可以构建比前三种方法更深的SNN，但构建大规模SNN仍然是一个巨大挑战，因为在网络架构变得更深（例如超过30层）后，转换后的性能将迅速下降。

     ​     在本文中，我们研究了通过转换深度残差网络[8]来学习深度SNN的方法。残差网络是一种尖端的CNN架构，允许网络变得极其深层，并且在许多应用中取得了巨大成功。先前的转换方法并不适用于残差网络，因为残差结构和传统线性结构之间存在结构差异。我们设计了一种快捷转换模型，以共同规范脉冲残差网络中的突触权重。此外，我们发现积累的传播误差对于阻碍大规模网络的无损转换是至关重要的，并开发了一种有效的方法来通过==轻微饱和化神经元的发射速率和迫使更深层的神经元更快地作出响应==来补偿传播误差。我们的方法在MNIST、CIFAR-10、CIFAR-100和ImageNet上取得了最先进的性能。

   * **2 相关工作**
     本文的相关工作总结了四个方向上各个学者所作出的努力，所以其实了解每一个方向，跟着看对应的参考文献就够了。

     * **第一个方向：**基于梯度下降的算法。这种方法旨在克服SNN中的非线性问题，并应用梯度下降优化算法。早期的工作可以追溯到Spikeprop[10]，它在SNN中实现了反向传播，假设势函数在激发时间周围的一个小区域内是可微的。以类似于反向传播的方式，tempotron学习规则[11]是从由膜电位和脉冲定时定义的成本函数导出的。最近，在[12，13，14，15，16，17，18]中展示了使用随机梯度下降（SGD）的SNN直接训练。

     * **第二个方向：**突触可塑性规律的学习。这种方法利用突触可塑性规则，如基于脉冲时序依赖性的突触塑性规则（STDP）[19]。STDP以其丰富的神经生理学证据，引起了众多研究者的兴趣。在[20，21]中，STDP已被证明能够以无监督的方式选择视觉特征。在[22]中，采用STDP规则、侧抑制和稳态，显示了随时间推移的稳定学习。在[23]中，提出了事件驱动的连续STDP来识别来自时序编码脉冲序列的模式。Rathi等人[24]利用修剪的SNN和自学习STDP学习规则展示了能耗和面积的高效性。

     * **第三个方法：** 统计算法。尽管大多数算法将SNN视为确定性系统，但证据表明大脑中的神经网络类似于随机系统[25]。在[26]中，噪音和不确定性被视为促进SNN中统计学习和自组织的有益因素。在[27]中，已经显示SNN能够执行贝叶斯推断，与贝叶斯推断在认知行为中普遍存在的理论相吻合。在[28]中，提出了一个具有随机突触的网络模型，用于蒙特卡罗抽样和无监督学习。

     * **第四个方法：** 将经过训练的ANN转换为其等效的SNN的研究始于Perez-Carrasco等人[29]，他们提出了一种方法，通过按照Leaky Integrate-and-Fire（LIF）[30]脉冲神经元的参数来缩放其CNN对应权重以获得SNN的权重。

     * 第二种  转换算法建立了ANN和SNN之间的映射，通过调整激活函数或人工神经来逼近脉冲神经元的平均发射频率。这一类方法包括使用Siegert神经元进行训练和映射[31]、使用SoftLIF函数[32]、使用噪声软加函数[33]、以及使用高斯分布的互补累积分布函数[34]。这些方法需要使用较不常见的激活函数进行训练，并且与最先进的ANN架构不兼容。

     * 第三种转换算法利用ReLU激活的非负性来逼近平均发射频率。此方法最早由Cao等人在[35]中提出。随后的工作引入了基于数据的归一化[36]和动态阈值平衡[37]等方法，以提高转换后的性能。
     
     * 在[38, 39]中，介绍了常见ANN操作的脉冲版本，用于转换现代ANN架构和预训练的ANN模型。Esser等人[40]利用TrueNorth芯片进行了硬件实现演示。
     
     ==当前困难：== 构建更深层次的SNN以获得更高的精度仍然是具有挑战性的。
     
   * **3 Building Spiking ResNet**   构建脉冲残差网络
   
     * 3.1 Spiking ResNet: An Overview   S-Resnet概述
       在ANN中，神经元接收来自前一层神经元的实值输入，而SNN中的神经元接收到来自前一层神经元的脉冲序列（一系列脉冲）作为输入。此外，在ANN中，神经元通过对加和后的输入应用激活函数来处理信息，而在SNN中，神经元通过整合传入的脉冲来处理信息。每个传入的脉冲都会引起神经元的突触后电位 (PSP) 发生变化。当PSP达到一定阈值时，神经元会发出脉冲并将信息传递给其下一层神经元。在将ANN中的权重映射到SNN之前，我们必须注意，由于我们在训练中使用了ReLU函数，因此ANN的激活可以是任何正实数。同时，脉冲神经元的发射速率是固定在[0，rmax]区域内的，其中rmax是生物神经元的最大发射速率，因为生物神经元在短暂的时间间隔（单个时间步）内不会发出多个脉冲。为了简化，我们假设神经元可以发射尽可能多的脉冲，即每个时间步发出一个脉冲。为了使ANN中的激活值匹配SNN中的发射速率（都在[0，1]区间内），需要共同对ANN中的权重和偏置进行归一化处理。
       ![image-20240123165612893](../../../AppData/Roaming/Typora/typora-user-images/image-20240123165612893.png)
   
       上图为S-Resnet网络形成图。ReLU激活函数由IF神经元替代。
       
       转换过程中去掉sum是因为IF神经元已经默认实现了求和操作。
       
       卷积层被类似卷积运算的突触连接层取代。卷积层的权重映射到相应的突触层，而偏置则转换为注入到脉冲神经元的恒定电流 [39]。同样，池化层和全连接层也被类似这些操作的突触所取代。对于平均池化，突触权重固定为(1/PoolSize)的平方。对于最大池化，我们使用逻辑比较来仅选择来自具有最高发射速率的神经元的脉冲，并通过将突触权重设为零来抑制其他输入脉冲。在池化层或全连接层之后，增加了一个额外的IF神经元层来集成来自这些类型突触的脉冲。由于在转换之前训练已经完成，因此批归一化直接应用于在批归一化层之前的卷积层。
       
     * 3.2 Conversion Model of Residual Network  残差网络的转换模型
       在这个小结中，作者主要说明了残差连接的过程中，短连接也必须进行归一化，否则会出现于原始ANN特征图不匹配的情况，从而导致精度下降。
       ![image-20240123170714002](../../../AppData/Roaming/Typora/typora-user-images/image-20240123170714002.png)
   
       ![image-20240123173941888](../../../AppData/Roaming/Typora/typora-user-images/image-20240123173941888.png)
   
       ![image-20240123174020238](../../../AppData/Roaming/Typora/typora-user-images/image-20240123174020238.png)
   
       短连接层应该使用这个归一化因子去对权重进行变化。
   
     * 3.3 Compensation of Propagation Error  传播误差的补偿
       这篇论文讨论的转换方法是基于人工神经元与脉冲神经元之间的一对一对应关系：ReLU的归一化激活被脉冲神经元的发射速率近似。
   
     * ![image-20240126210658303](../../../AppData/Roaming/Typora/typora-user-images/image-20240126210658303.png)
       
     * ![image-20240126210842773](../../../AppData/Roaming/Typora/typora-user-images/image-20240126210842773.png)
       
     * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240126211037184.png" alt="image-20240126211037184" style="zoom: 67%;" />
       
     * ![image-20240126211110202](../../../AppData/Roaming/Typora/typora-user-images/image-20240126211110202.png)
       
     * 最终作者通过上述模拟误差的操作，进行了拟合，给出了误差的计算公式，但显然，这是受限制的，仅对当前对比的ANN-SNN成立。
       
     * ![image-20240126211318261](../../../AppData/Roaming/Typora/typora-user-images/image-20240126211318261.png)
       
       然后考虑到误差传播公式可能对于其他转换来说都是一个指数模型，这在实际应用中过于复杂，==最终为避免指数操作，使用了一个权重更新规则来减少参数量，而不是调整每层的权重。==
   
   * **4.实验**
   
     * 作者在四个数据集上进行了实验：MNIST，CIFAR10，CIFAR100，ImageNet 2012。
     * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240126212126011.png" alt="image-20240126212126011" style="zoom:67%;" />
     * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240126212334505.png" alt="image-20240126212334505" style="zoom:67%;" />
     * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240126212442979.png" alt="image-20240126212442979" style="zoom:67%;" />
     * 作者还进行了能耗分析。参考每个平台的能源效率，ResNet的功耗是所有深度的S-ResNet的9倍以上。这一结果向我们展示了基于SNN的神经形态系统的光明前景，因为用神经形态硬件实现的SNN的估计功耗在一定程度上击败了用顶级节能FPGA平台实现的ANN。==这个对比== 就是说Resnet使用更节能，更高级的硬件设备， 而S-ResNet使用低端设备，但是两者准确率相差不大。
   
     ==值得注意的是：== 在实际应用过程中，比如医学信号的真实分类场景，使用这两个网络进行分类，是否Resnet准确率非常好，而S-ResNet却表现不行，因为毕竟现在都是在用训练集和测试集，没有考虑到真正实际应用中的准确率。但S-ResNet确实非常节能，这一点毋庸置疑。
     
     ==代码实现：== 目前github上应该有好多的代码，后续尝试复现并改成自己的。
   
   
   
* ## 5  ==Training spiking deep networks for neuromorphic hardware==  
  
   训练神经形态硬件的脉冲深度网络
   
   * **期刊**：aixiv 预印本   2016年11月16提交的手稿   
   
   * **作者：** Eric Hunsberger 滑铁卢大学
   
   * **摘要：** 本文提出一种训练脉冲深度网络的方法，该方法可以使用LIF神经元进行操作，在五个数据集上取得了脉冲LIF网络的最新成果，包括ImageNet 2012数据集。我们将深度ANN转换为SNN的方法具有可伸缩性，并适用于广泛的神经非线性--==大脑内部神经元的关系肯定是非线性的==。我们通过软化神经响应函数来实现这些结果，以使其导数保持有界，并通过在训练网络时添加噪声，增强对脉冲引入的变异性的鲁棒性。我们的分析显示，这些网络在神经形态硬件上的实现效率将比传统硬件上等效的非脉冲网络高出数倍。
   
   * **方法：** 我们首先使用传统深度学习技术在静态图像上训练一个网络；我们称之为ANN。然后，我们获取ANN的参数（权重和偏置），并将它们用于连接脉冲神经元，形成脉冲神经网络（SNN）。一个核心挑战是==以什么样的方式训练ANN==，使其可以转换为脉冲网络，并且使得生成的SNN的分类误差最小化。
   
     * CNN
   
     * LIF神经元模型
   
     * 带有噪音的训练
       图2：LIF神经元（τRC=0.02，τref=0.004）的滤波脉冲序列与输入电流的变化。实线显示了滤波脉冲序列的平均值，'x'点显示了中位数，实线误差线显示了第25和第75百分位数，虚线误差线显示了最小值和最大值。脉冲序列经过带有τs=0.003秒的α-滤波器的滤波处理。
   
       <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240126214823395.png" alt="image-20240126214823395" style="zoom:67%;" />
   
     * 转换为脉冲网络
       脉冲网络中的参数（即权重和偏置）均与ANN中的相同。卷积运算也保持不变，因为卷积可以重写为前突神经元 $i$ 和后突神经元 $j$ 之间的简单连接权重 $w_{ij}$。 同样地，平均池化操作可以被写成一个简单的连接权重矩阵，而且这个矩阵可以与下一层的卷积权重矩阵相乘，从而获得神经元之间的直接连接权重。当从ANN迁移到SNN时，网络中唯一发生改变的组件是==神经元本身==。==最显著的变化是我们用LIF脉冲模型替换了 `soft LIF rate model`==———这里作者是否说错了？因为下面说的都是软LIF好呀，然后作者还改进了，同时在结果中作者呈现的是soft LIF———。我们去掉了训练中使用的加性高斯噪声。我们还为神经元添加了后突触滤波器，这取消了脉冲产生的高频变化的重要部分。
   
     * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240127100020294.png" alt="image-20240127100020294" style="zoom:67%;" /><img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240127100235175.png" alt="image-20240127100235175" style="zoom:67%;" />
   
       第一个公式是原始的硬阈值，第二个是改进后的软阈值，第三个是为了控制平滑量新更改的软阈值函数。
   
       ==显然，这里作者并没有详细阐述自己的过程，仅仅简单说了一下==
   
   * **结果：** 展示了本文网络与其他学者的对比，其他神经网络形态芯片的错误率对比，同时作者在CIFAR-10数据集上对比了增加不同的修改带来的准确率
     ![image-20240127095418494](../../../AppData/Roaming/Typora/typora-user-images/image-20240127095418494.png)
   
     * **能耗问题：**  
   
       * 首先考虑原始网络的计算效率：==公式给了，但是还是不太清楚具体如何计算，后续需要查看相关资料==
   
       * 关于SNN计算效率阐述的较为清楚。
   
       * <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240127103125488.png" alt="image-20240127103125488" style="zoom:67%;" />
   
         **表4示出了呈现时间和突触时间常数的各种替代方案对许多数据集的网络的准确性和效率的影响。**
   
         与传统硬件相比，我们的网络在神经形态硬件上的估计效率。对于所有数据集，准确性和效率之间存在权衡，但我们发现许多配置在准确性方面牺牲很少的同时效率显著提高。τs是突触时间常数，c0是分类的开始时间，c1是分类的结束时间（即每个图像的总呈现时间）。
   
         表4显示，对于某些数据集（例如CIFAR-10和ILSVRC-2012），可以完全去除突触（τs = 0 ms），而不会牺牲太多准确性。有趣的是，MNIST网络的情况并非如此，它至少需要一些突触才能准确地发挥作用。我们怀疑这是因为MNIST网络的发射率比其他网络低得多（MNIST的平均发射率为9.67 Hz，CIFAR-10为148 Hz，ILSVRC-2012为93.3 Hz）。这种平均发射率的差异也是MNIST网络比其他网络效率高得多的原因。重要的是要调整分类时间，无论是在每个示例的总时间长度方面（c1），还是在分类开始时（c 0）。这些参数的最佳值非常依赖于网络，包括层数、放电率和突触时间常数。
   
         ![image-20240127105845478](../../../AppData/Roaming/Typora/typora-user-images/image-20240127105845478.png)
   
         分类时间对准确度的影响。各个迹线示出不同的开始分类时间（c0），并且x轴示出结束分类时间（c1）
   
         图3显示了分类时间如何影响各种网络的准确性。考虑到CIFAR-10网络在没有突触的情况下和有突触的情况下表现得几乎一样好，人们可能会质疑在训练过程中是否需要噪声。我们在没有噪声的情况下重新训练了CIFAR-10网络，并且在没有突触的情况下运行，但准确率无法达到18.06%。这表明噪音在训练中仍然是有益的。
   
         ==关于这个图的描述不是很清楚==，
   
     * **结论：**  概述了上面的工作并提出了未来可行的方向。
       本文的第一个主要贡献是证明最先进的脉冲深度网络可以使用LIF神经元进行训练，同时保持高水平的分类准确性。例如，我们描述了第一个能够在ImageNet上获得良好结果的大规模SNN。值得注意的是，所有其他最先进的方法都使用了整流并发放（IF）神经元[11,10,12]，这些方法对于深度卷积网络中常用的修正线性单元来说很容易适应。我们的研究表明，在从ANN转换为SNN时，准确性几乎没有下降。我们还研究了分类时间对准确性和能效的影响，并发现网络可以在准确性几乎不下降的情况下实现高效节能。通过使LIF响应函数变得平滑，使得其导数保持有界，我们能够使用标准的通过反向传播训练的卷积网络结合这种更复杂和非线性的神经元。我们的平滑方法适用于其他神经元类型，允许用于具有个性化神经元类型的神经形态硬件进行网络训练（例如[14]）。我们发现，对于我们使用的平滑量来说，从软响应函数转换为LIF神经元的硬响应函数引入的误差非常小。然而，对于那些具有需要更多平滑的严重不连续性的神经元来说，可能需要在训练过程中缓慢进行平滑，这样，在训练结束时，平滑响应函数可以任意接近硬响应函数。
   
     * 本文的第二个主要贡献在于证明，在神经元输出中添加噪音可以降低过渡到脉冲神经元时引入的误差。尽管没有噪音的ANN表现更好，但CIFAR-10网络的整体误差降低了0.6%。这是因为神经元输出的噪音模拟了脉冲网络在过滤脉冲序列时遇到的变异性。在使用太少噪音进行训练和使用过多噪音进行训练之间存在权衡，过少噪音会使SNN的准确性下降，而过多噪音则会使最初训练的ANN的准确性下降。 ==这种增加噪音训练的方法个人感觉行不通，因为正常来说数据中包含的噪音不可能是这么规律的噪音，所以这种方法的现实意义显然不大==
   
     
   
* ## 6. ==Deep Residual Learning in Spiking Neural Networks==

   [知乎链接](https://zhuanlan.zhihu.com/p/561909077)
   
   ==严格来说这篇文章不是ANN TO SNN，论文的目的是证明了直接训练SNN也是可以的。==
   在脉冲神经网络中的深度残差学习 
   这篇论文有代码，https://github.com/fangwei123456/Spike-Element-Wise-ResNet
   这篇文章提到了第四篇文章，两者都是基于Resnet想到的S-ResNet
   
   * **期刊**：NeurIPS 2021会议论文  
   
   * **作者：** Fang Wei  北大 计算机科学与技术系
   
   * **摘要：** 脉冲神经网络（SNNs）由于离散的二进制激活和复杂的时空动态，对基于梯度的方法提出了优化困难。考虑到ResNet在深度学习中的巨大成功，训练深度脉冲神经网络进行残差学习是很自然的选择。先前的S-ResNet模仿了人工神经网络中的标准残差块，并简单地将ReLU激活层替换为脉冲神经元，其存在退化问题，==几乎无法实现残差学习==。在本文中，我们提出了Spike-Element-Wise(脉冲元素形式的)（SEW）ResNet，以实现深度脉冲神经网络中的残差学习。我们证明了==SEW ResNet可以轻松实现恒等映射，并克服了S-ResNet的梯度消失/爆炸问题==。我们在ImageNet、DVS Gesture和CIFAR10-DVS数据集上评估了SEW ResNet，并展示了SEW ResNet在准确性和时间步数方面优于最先进的直接训练SNNs。此外，SEW ResNet只需添加更多层即可实现更高的性能，为训练深度SNNs提供了一个简单的方法。据我们所知，==这是首次实现直接训练超过100层深度的SNN==
   
   * **介绍：** 上一篇文章是将训练完的ResNet转为S-ResNet，然后用于测试，发现效果不错，这篇文章说的是虽然其效果不错，但是如果直接训练S-ResNet，梯度消失或者爆炸的问题依然是很严重的，可见S-ResNet并没有真正实现残差，那这篇文章提出了SEW ResNet，并和S-ResNet一起用于训练网络，发现SEW ResNet直接训练依然有很好的效果，且层数超过了100层，借此说明S-ResNet并不如他们的网络。
   
   * **相关工作：** 
   
     * SNN的学习方法
       ANN到SNN的转换[20, 4, 46, 49, 12, 11, 6, 54, 33]和使用替代梯度进行反向传播[40]是获得深度SNN的两种主要方法。
   
       （1）ANN to SNN方法首先训练具有ReLU激活的ANN，然后通过将ReLU替换为脉冲神经元 (==IF神经元==)，并添加诸如权重归一化和阈值平衡之类的缩放操作，将ANN转换为SNN。一些最近的转换方法已经在VGG-16和ResNet上实现了接近零损失的准确性。然而，由于转换是基于速率编码的，转换后的SNN需要更长的时间才能与原始ANN在精度上匹敌，这会增加SNN的延迟并限制其实际应用。
   
       （2）反向传播方法可以分为两类[26]。
   
       ​	    第一类方法通过在模拟时间步上展开网络来计算梯度[31, 19, 58, 50, 30, 40]，这类似于BPTT（BP的RNN版本），且使用代理梯度，编码方式不仅可以使用速率编码，也可以使用时间编码。
   
       ​	第二种方式是计算现有脉冲发放时间相对于膜电位在发放脉冲时间的梯度[5, 39, 24, 65, 63]。
       The second method computes the gradients of the timings of existing spikes with respect to the membrane potential at the spike timing [5, 39, 24, 65, 63].  ==这里应该说的是stdp，计算的是时间差==
       
     * Spiking Residual Structure——脉冲残差架构
   
       胡等人[17]（==这就是阅读的第四篇文章==）是第一个在ANN to SNN中引入残差结构，并在SNN中应用缩放的快捷路径以匹配原始ANN的激活的研究。其余学者也有进行后续研究的。也有改进归一化方法的。
   
   * **本文方法：** 
   
     * **脉冲神经元模型**（介绍现有的脉冲神经元模型，及数学公式。 论文中使用代理梯度方法，也就是，将σ的梯度用于反向传播，σ是与不可微的阶跃函数形状类似的可微函数） 
   
       ![image-20240717101917958](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240717101917958.png)
   
       ![image-20240717101930843](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240717101930843.png)
   
       公式1、2和3介绍了现有脉冲神经元的统一模型，公式1中的f（）有多种形式，公式4和5是IF神经元和LIF神经元模型的函数形式，是对公式1的补充说明。
   
       公式2是一个Heaviside阶跃函数，x大于等于0，y值为1，x小于0，y值为0。
   
       公式3的意思就是 有脉冲的话，就将电压恢复至重置电压，如果没有脉冲，就2等于公式1计算出的新的电压值。
   
     * **S-ResNet的缺点**
       
       **有相关文献证明：** ResNet的一个关键概念是恒等映射（identity mapping）。He等人[14]指出，如果添加的层实现了恒等映射，则一个更深的模型的训练误差不会大于其较浅模型的训练误差。
       
       符号含义：F是残差函数，X是输入，S也是输入（只不过是脉冲形式），Y和O是输出，右上角标代表第$$l$$​层(layer)，[t]代表时刻t，SN代表脉冲神经元(spiking neuron)。
       
       <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240717110041372.png" alt="image-20240717110041372" style="zoom:67%;" />
   
       * ==第一个缺点：S-ResNet 不适用于所有神经元模型去实现恒等映射。==
         ![image-20240717110458962](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240717110458962.png)
       
         公式6可以实现恒等映射，因为当残差为0，公式变为 $$Y^l = ReLU(X^l)$$，而$$X^l$$ 是上一层经过ReLU函数之后的值，所以此时公式的结果就变为  $$Y^l = X^l$$​
       
         公式7难以实现恒等映射，因为输入的脉冲序列$$S^l[t]$$ 里面的值只有 0 和 1 ， 当残差为0，公式7变为$$O^l[t] = SN(S^l[t])$$ , 而$$SN(S^l[t]) == S^l[t]$$ 这一公式很难成立。 当使用的神经元为IF神经元时，我们需要保证$$V[t-1] = 0$$, 且 $$0< V_{th} <= 1$$,  这样公式4变为  $$H[t] = X[t]$$, 而X[t] 本身为 [1, 0 , 0 ,1 ,0 ]这样的序列，这样的序列统一减去一个小于等于1的整数后，序列中的1变为大于等于0，而0变为负数，经过公式2之后，仍然为 X[t]，这样也就做到了输入等于输出。其余更复杂的神经元就更难实现了。
       
       * ==第二个缺点：S-ResNet存在梯度爆炸/消失的问题==
       
         ![image-20240717152908670](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240717152908670.png)
   
         ==本人观点：== 这也是很好理解的，因为每次导数都一致。 依然要注意这里的前提是假设恒等映射可以发生，如果很难发生，那么其都无法构建很深的网络。   表明即使给了很多先决条件，也还是很难做到恒等映射。
   
       * 基于上述分析，作者认为先前的 Spike-ResNet 忽略了脉冲神经元引起的高度非线性，并且很难实现残差学习。
       
       * 但是因为从ANN转SNN的关键步骤是使用发射率去匹配激活函数，相当于是有参考的来学习，而不是靠自己学习。
       
     * **Spike-Element-Wise ResNet   脉冲元素形式的残差网络**
   
       * ![img](https://pic3.zhimg.com/80/v2-f0e320f018d100eb90a39a43e95f425e_720w.webp)
   
         
   
         ![image-20240717154010613](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240717154010613.png)
   
         式（9）描述了上面那三张图中的右图，也就是本文新提出的方法。 实际就是，把残差F用SN激活一下，转换成脉冲A，然后脉冲A与输入脉冲S做 g 运算。公式（7）让非脉冲和脉冲做加法运算，就比较别扭；这里统一了形式，让脉冲和脉冲运算，就比较自然）
   
         g是element-wise function，这也是SEW ResNet的来源，即**s**piking **e**lement-**w**ise。
   
         论文中为g运算准备了3种形式。
   
         ![image-20240717155011483](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240717155011483.png)
   
         其中，第二种形式要求  残差 $$A^l[t]$$  = 1，这是比较难实现的，而第三种IAND相当于对第二种形式的改进，和第一种一样，实现恒等映射时，残差 $$A^l[t]$$  = 0， 只需要保证在每一个块的最后设置权重和偏差为0即可实现。
   
       
       
       * **关于下采样块的设计**
         <img src="C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240717160204585.png" alt="image-20240717160204585" style="zoom:67%;" />
       
         ResNet和 Spike-ResNet 两个网络在设计下采样块时，都是在残差连接之后接激活函数，而SEW网络则是在残差连接之前给残差和下采样的结果均接入激活函数==（因为多了下采样，是否可以不用这么麻烦，之前是因为形式不统一，现在形式统一了。）==
       
       * **解决梯度消失和梯度爆炸的问题**
       
         ==RBA块的形式如公式10所示：==
       
         ![image-20240717163806036](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240717163806036.png)
       
         $$X^l$$是上一层ReLu的产物，这里把残差F也ReLU一下，再和X做加法运算，这就和本文提到的SEW块结构非常相似了。
         RBA块被何凯明老师批判，因为两个加数都是经过ReLU的，自然都是大于等于0的，二者相加会导致Y≥X，随着层数加深，这种放大效果会越来越大。在实际的实验中也证明了RBA块效果不好。
       
         ==SEW块为何不会出现梯度消失和爆炸的问题?==
       
         ![image-20240717165607286](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240717165607286.png)
       
         假设恒等映射已经实现，我们可以得到公式11，与之前讨论公式8的形式一样 。
       
         
   
   * **结果：** 在ImageNet数据集上，本文的SEW-ResNet网络具有很好的泛化性，且随着网络层数增加，效果不断变好，反观S-ResNet却随着网络层数的加深，效果越来越差。
   
     ![image-20240717172813514](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240717172813514.png)
   
   * **讨论与结论：** 
   
     * 作为激活函数，SN比ReLU的非线性强得多，可能会有其他应用或效果。
   
     * 脉冲的二进制特点再次显现：RBA会有无穷大问题，但与RBA结构类似的SEW就不会，因为SNN传递的输入输出是二进制的。今后如果涉及什么无穷大、无穷小、上下界，都可以从二进制脉冲角度考虑一下。
   
     * 论文的切入点，应该是二元运算的成员，应该是相同种类的（都是脉冲）。在原ResNet中，都是浮点数。
   
       

7. ## ==测试==

   

   * **期刊：**
   * **作者及单位：**
   * **摘要：**
   * **介绍：**
   * **方法：**
   * **试验：**
   * **讨论与总结：**

   

8. ## ==测试==

   

   * **期刊：**
   * **作者及单位：**
   * **摘要：**
   * **介绍：**
   * **方法：**
   * **试验：**
   * **讨论与总结：**

9. 



# ==SNN论文阅读==

1. ## ==Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks==
   [github 链接](https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron)

   添加可学习膜时间常数以增强脉冲神经网络的学习能力

   这篇文章和ANN to SNN 文章中的第6篇文章 作者是同一个
   [知乎链接](https://zhuanlan.zhihu.com/p/561445448)

* **期刊：**  axiv 预印本    2021.08.17发布  上一篇文章也是2021年发布在NeurIPS会议

* **作者及单位：**  Fang Wei 等人    北京大学计算机科学与技术系

* **摘要：** 
  大多数现有的学习方法仅学习权重，并且需要手动调整决定单个脉冲神经元动力学特性的膜相关参数。这些参数通常被选为所有神经元的相同值，这限制了神经元的多样性，因此限制了所得 SNN 的表现力。在本文中，我们从膜相关参数在大脑区域间不同的观察结果中汲取灵感，并提出一种不仅能够学习突触权重，还能学习 SNN 膜时间常数的训练算法。我们表明，结合可学习的膜时间常数可以使网络对初始值不那么敏感，并可以加速学习。

  讲述了本文做的工作，大概两点：

  - 让膜时间常数（ membrane time constant）成为可学习的参数，而不是预置的超参数。
  - 在SNN中使用最大池化，而不是通常认为的平均池化，最大池化具有更多优势。

* **介绍：**
  通常SNN的学习算法可以分为有监督，无监督，基于奖励的学习以及 ANN to SNN 的四个方法。
  <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129173711800.png" alt="image-20240129173711800" style="zoom:67%;" />

  * 使用PLIF（也就是带膜参数常量的LIF神经元）作为神经元，基于反向传播进行训练。这提高了模型的鲁棒性和学习速度。
  * 重新评估池化方法。证明最大池化性能不比平均池化差，还能降低计算成本（二进制位运算）、保留神经元放电的异步性（下文详细说明）。
  * 在一些数据集上检验本文的模型，效果很好。

* **相关工作：** 

  1. 无监督学习的SNN
     当前方法仅适用于浅层SNN。

  2. 基于奖励学习的SNN
     模仿人脑的学习方式，即利用多巴胺能、血清素能、胆碱能或肾上腺素给予神经元奖励或惩罚信号[15, 6, 45]。
     目前出现了诸如策略梯度[58, 30]、时序差分学习[52, 16]和 Q-learning[6]等强化学习方法，最近还提出了一些基于 STDP 的启发式现象学模型[17, 73]。
     ==方向：结合强化学习方向的进展，将其与最新的STDP学习规则相结合，是否能有更好的效果？== 

  3. ANN to SNN 的转换

     ==速率编码：==脉冲神经网络（SNN）中的速率编码是一种常用的信息编码方式，它基于神经元的发放速率（即脉冲的频率）来表示输入信息。在速率编码中，信息的数量和强度通常由脉冲的频率来编码，频率越高表示对应的信息强度越大。

     ANN2SNN 仅限于速率编码，从而失去了时序任务中的处理能力。
     据我们所知，ANN2SNN 仅适用于静态数据集，不适用于神经形态数据集。

  4. 监督学习的SNN
     SpikeProp [5] 是第一种基于反向传播的 SNN 监督学习方法，它使用线性逼近来克服 SNN 的非可微阈值触发放电机制。
     Zenke 等人 [74, 46] 系统地研究了替代梯度学习的显著鲁棒性，并表明通过替代梯度方法优化的 SNN 可以与人工神经网络实现具有竞争力的性能。与 ANN2SNN 相比，替代梯度方法对模拟时间步长没有限制，因为它不基于速率编码[64, 74]。

  5. 深层SNN的 脉冲神经元和层
     脉冲神经元和层模型在 SNN 中发挥着至关重要的作用。
     到目前为止还没有对  学习膜时间常数对 SNN 的影响进行系统研究
     Wu 等人 [64] 发现归一化层对于深层 SNN 也是至关重要的，并提出了神经元归一化 (NeuNorm)，以平衡每个神经元的放电率，避免严重的信息丢失。Ledinauskas等人 [36] 最早提出在深层 SNN 中使用批归一化 [27] 来实现更快的收敛。

* **方法：**
  在本节中，我们首先在 3.1 节中简要回顾了LIF模型，并在 3.2 节中分析了突触权重和膜时间常数的影响。然后在 3.3 – 3.5 节中介绍了参数化LIF模型和 SNN 的网络结构。最后，我们在 3.6 节和 3.7 节中描述了脉冲最大池化和 SNN 的学习算法。

  1. LIF模型

  2. Function comparison of synaptic weight and  membrane time constant    

     突触权重和膜时间常数的功能比较
     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129201158862.png" alt="image-20240129201158862" style="zoom:67%;" />

  3. Parametric Leaky Integrate-and-Fire model   参数LIF模型
     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129201457796.png" alt="image-20240129201457796" style="zoom:67%;" />

     ![image-20240129202329651](../../../AppData/Roaming/Typora/typora-user-images/image-20240129202329651.png)

     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129201844376.png" alt="image-20240129201844376" style="zoom:67%;" />

     ![image-20240129202219317](../../../AppData/Roaming/Typora/typora-user-images/image-20240129202219317.png)

     这个公式会导致数值的不稳定，因为$τ$ 是分母， 只有当公式1中的$dt$ 小于 $τ$ 时才是一个有效的离散近似，即$τ$ 大于1，为了避免不稳定问题，下面给了公式6———————— ==没有明白这里的近似是啥意思。==

     ![image-20240129202929919](../../../AppData/Roaming/Typora/typora-user-images/image-20240129202929919.png)

     ![image-20240129211648493](../../../AppData/Roaming/Typora/typora-user-images/image-20240129211648493.png)

     

  4. RNN-like Expression of LIF and PLIF    LIF 和 PLIF 的 RNN式表达  从RNN的角度解释LIF和PLIF
     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129202043885.png" alt="image-20240129202043885" style="zoom:67%;" />

     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129203420923.png" alt="image-20240129203420923" style="zoom:67%;" />

     ![image-20240129211922912](../../../AppData/Roaming/Typora/typora-user-images/image-20240129211922912.png)

     

  5. Network Formulation    网络的公式化
     我们在本文中提出了构建 SNN 的一般公式，如图 4 所示。SNN 包括脉冲编码器网络和分类器网络。脉冲编码器网络由 N个下采样模块组成，每个模块包含 N个 重复的 {Conv2dSpiking Neurons 和一个池化层。脉冲编码器可以从输入中提取特征，并将它们转换为不同时间步长的放电脉冲。分类器网络由 N个重复的 {FC-Spiking Neurons} 组成。此处 Conv2d 表示二维卷积层，FC 表示全连接层。许多以前的作品 [12, 37, 60, 75, 8, 21] 使用==泊松编码器==将图像转换为脉冲作为输入，而 [56] 表明这种编码会给网络的放电引入可变性并损害其性能。类似于 [56, 64, 53]，==输入直接馈送到我们的网络，而没有首先转换为脉冲。==在这种情况下，图像脉冲编码由第一个 {Conv2d-Spiking Neurons} 模块完成，该模块可以看作是可学习的编码器。请注意，突触连接（包括卷积层和全连接层）是无状态的，而脉冲神经元层在时间域中具有自连接，如图 4 所示的展开网络公式。所有参数在所有时间步长上都是共享的。

     ![image-20240129212055894](../../../AppData/Roaming/Typora/typora-user-images/image-20240129212055894.png)

     

  6. Spike Max-Pooling    脉冲最大池化层

     先前研究都是用平均池化，认为最大池化会丢失信息。在这篇论文证明了不会丢失信息，且可以减小计算成本。与平均池化窗口中将所有神经元等量地传输信息给下一层的情况不同，在最大池化窗口中只有发射脉冲的神经元能够将信息传输到下一层。因此，最大池化层引入了胜者通吃机制，允许发射脉冲的神经元与下一层通信，并忽略池化窗口中的其他神经元。最大池化层的另一个吸引人的特点是动态调节连接。
     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129205141254.png" alt="image-20240129205141254" style="zoom:67%;" />

     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129212846299.png" alt="image-20240129212846299" style="zoom:67%;" />

  7. Training Framework  训练框架
     损失函数 MSE ,  其余一些求导过程不太能看懂。

     

* **试验：** 
  使用PLIF神经元和脉冲最大池化进行分类任务的SNN性能，包括传统的静态MNIST、Fashion-MNIST、CIFAR-10数据集以及神经形态学N-MNIST、CIFAR10-DVS、DVS128手势数据集。

  1. Network Structure 网络框架
     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129214107639.png" alt="image-20240129214107639" style="zoom:67%;" />

     投票层是通过设置kernel大小为M、stride为M的平均池化来实现的。我们对所有数据集设置了M=10。我们使用平均池化来实现民主投票，使得少数服从多数。使用最大池化进行投票可能导致专制主义，因为少数将不参与计算图（见图5），而使用M个神经元来代表一个类将退化为使用一个神经元。

     

  2. Comparison with the State-of-the-Art     与现有技术的比较
     准确率提高，且推理速度加快了许多。
     <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129215831702.png" alt="image-20240129215831702" style="zoom:67%;" />

* **讨论与总结：**
  在这项工作中，我们提出了参数泄露整合放电（PLIF）神经元，将可学习的膜时间参数纳入SNN中。我们展示了使用PLIF神经元的SNN在静态和神经形态数据集上都优于现有的比较方法。此外，我们还展示了使用PLIF神经元构建的SNN对初始值更加鲁棒，并且学习速度比由LIF神经元组成的SNN更快。我们还重新评估了SNN中最大池化和平均池化的性能，并发现先前的工作低估了最大池化的性能。我们建议在SNN中使用最大池化，因为它具有较低的计算成本、更高的时间拟合能力，并且具有接收脉冲和输出脉冲而非浮点值的特性，而前者则适用于平均池化。
  <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240129220430797.png" alt="image-20240129220430797" style="zoom:67%;" />


2. # ==测试==

* **期刊：**
* **作者及单位：**
* **摘要：**
* **介绍：**
* **方法：**
* **试验：**
* **讨论与总结：**



# ==Surrogate gradient==  替代梯度

1. ## ==Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks==

   通过反向传播进行深度脉冲神经网络的时序(脉冲序列)学习
   [参考链接](https://zhuanlan.zhihu.com/p/667158856)

* **期刊：** NIPS 2020 会议论文 

* **作者及单位：** 加州大学  zhang wenrui 

* **摘要：** 脉冲神经网络适合时空学习，且能效高。现有的SNN error BP 性能低下，需要较长的时间步才可以实现较好的性能，无法将深度进行扩展。作者提出了一种新的 时序脉冲学习 反向传播 算法  TSSL-BP 

  （temporal spike sequence learning backpropagation） , 该方法是将error BP 分为神经元间和神经元内两类依赖关系。  考虑脉冲发放活动的全有或全无特性，通过突触前的发放时间来捕获神经元间的依赖性； 通过处理每个神经元状态在时间上的内部演化，捕获神经元内的依赖性。    该方法大大缩短了时间窗口，有效的训练了深度SNN，同时提高了图像分类数据集的准确性。

* **1.介绍：**  该部分相当于摘要的扩展版。可以不用看

* **2.反向传播：** 

  * 2.1  现有的SNN反向传播算法
    最早的算法有 SpikeProp，但是局限于每个输出神经元只有一个脉冲，在实际任务中表现不好；ANN-SNN的转换会导致近似误差，且无法利用SNN的时序学习能力；在发放率编码损失函数下使用BP训练SNN可以实现较好的性能；通过时间反向传播来捕捉时间效应，使用替代梯度来近似脉冲过程来解决脉冲时间的不可分问题；一种基于门函数和阈值触发突触模型的递归SNN的BP方法；通过捕捉聚集在脉冲序列水平的神经元效应，提出了脉冲序列水平BP方法；添加神经元归一化和群体解码等优化技术；==作者提出了== TSSL-BP方法，适用于多个脉冲编码形式，可以捕捉时间依赖性，只需5个时间步即可完成训练，同时可获得极高的精度。
  * 2.2 脉冲神经元模型
    论文使用了LIF神经元模型和突触模型

* **3.方法：**  

  * 3.1 前向传播
    $a^{l-1}[t] = (\varepsilon * s^{l-1})[t]$   无权重的突触后模电压PSC
    $u^{l}[t] = (1-\frac{1}{\tau_m})u^{l}[t-1] + W^{(l)}a^{l-1}+(v*s^{(l)})[t]$   膜电压值 
    $s^{l}[t] = H(u^{l}[t] - V_{th})$  脉冲序列   ==上面三个公式统一定义为 公式(6)==   
    $V_{th}$ 是脉冲发放阈值，$H(·)$ 是Heaviside step function，x大于0，H(X)等于1，x小于0，H(X)等于0，x等于0时，H(X)等于1即可

    其中，$(\varepsilon * s^{l-1})[t] =  \sum_{i=0}^n \varepsilon[i]s^{l}[t-i]$ , 逐元素的卷积操作，形成的是一个矩阵向量的形式

    从上面公式中看的话, $u^{l}[t]$ 和 $s^{l}[t]$ 互相需要对方当前的值才可以得到。在==代码实现==上， $u^{l}[t]$ 只需要前两项，接着根据阈值生成脉冲，有脉冲就会进行归零，这和第三项的作用是一致的。

    对应的计算图如下，其中的$m^{l}[t]$ 暂时不清楚含义，推测应该是电压的记忆值

    <img src="D:/software/wechat_data/WeChat Files/wxid_v8vzx3rzcwp622/FileStorage/Temp/bc874316d8f5a97537e7312387232a3.png" alt="bc874316d8f5a97537e7312387232a3" style="zoom:50%;" />

    

    3.2 **损失函数的定义：**  
    $Loss = \sum_{k=0}^{N_t} E[t_k] = \sum_{k=0}^{N_t} \frac{1}{2} ((\varepsilon * d )[t_k] - (\varepsilon * s)[t_k])^2 $

    $E[t]$ 是时间t处的误差，d是脉冲输出序列，s是实际脉冲序列，两者的差距即为损失

    3.3 **TSSL-BP方法：**

    ![image-20240224111420389](../../../AppData/Roaming/Typora/typora-user-images/image-20240224111420389.png)

    上述公式的推导如下：
    <img src="D:/software/wechat_data/WeChat Files/wxid_v8vzx3rzcwp622/FileStorage/Temp/39ba5df1939b6c5eb6938f778080fa7.jpg" alt="39ba5df1939b6c5eb6938f778080fa7" style="zoom: 25%;" />

    ![c1a5f810f96ecc5dbda7fd613c7c335](D:/software/wechat_data/WeChat Files/wxid_v8vzx3rzcwp622/FileStorage/Temp/c1a5f810f96ecc5dbda7fd613c7c335.jpg)

    当 $l$ 是输出层时，公式的第一部分可以直接从损失函数中获得，第二部分需要计算
    ![image-20240225151509391](../../../AppData/Roaming/Typora/typora-user-images/image-20240225151509391.png)

    当 $l$ 是隐藏层时，通过反向链式法则进行构造，最后依然剩下这一项需要单独求解。

    $\frac{\partial \kern 2pt a^{l}[t_k]}{\partial \kern 2pt u^{l}[t_m]}$  

    

    * 3.3.1 SNN BP的核心挑战
      ![image-20240225093042153](../../../AppData/Roaming/Typora/typora-user-images/image-20240225093042153.png)

      将脉冲的累计过程近似成一条平滑的曲线，实际上是将一个实际的脉冲转换为多个虚构的脉冲(一个离散的值被扩展成多个连续值)，对突触后膜电位的产生造成影响，进而对脉冲发放时间产生影响。
      
    * 3.3.2 TSSL-BP 背后的思想
      先前在SNN中使用BP的两个主要局限性：缺乏对脉冲不连续的适当处理（导致时间精度的损失），以及 需要多个时间步才可以得到较高的准确率（多个时间步带来了较高的延迟）。
    
      
    
      <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240225101226685.png" alt="image-20240225101226685" style="zoom:67%;" />
    
      ![image-20240225101318009](../../../AppData/Roaming/Typora/typora-user-images/image-20240225101318009.png)
    
    * 3.3.3  神经元间的反向传播
      ![image-20240225103358510](../../../AppData/Roaming/Typora/typora-user-images/image-20240225103358510.png)
    
      突触前膜电位的变化引起了突触后波形的变化。
    
      <img src="../../../AppData/Roaming/Typora/typora-user-images/image-20240225104212278.png" alt="image-20240225104212278" style="zoom: 67%;" />
    
      当$t_k < t_m$时，或者在$t_m$处没有产生脉冲，那此时inter-neuron dependencies  $\phi_i^{(l)<1>}(t_k,t_m) = 0 $ ，否则，他们的关系就可以写为（11）
      ![image-20240225104726503](../../../AppData/Roaming/Typora/typora-user-images/image-20240225104726503.png)
    
      ![image-20240225104813523](../../../AppData/Roaming/Typora/typora-user-images/image-20240225104813523.png)
    
      结合(2) ，可以将(11)中的部分变换为 (12)
      ![image-20240225104910354](../../../AppData/Roaming/Typora/typora-user-images/image-20240225104910354.png)
    
      ![image-20240225105028142](../../../AppData/Roaming/Typora/typora-user-images/image-20240225105028142.png)
    
      结合(4)，可以得出（12）的解。
    
      ![image-20240225105302890](../../../AppData/Roaming/Typora/typora-user-images/image-20240225105302890.png)
    
    * 3.3.4 Intra-Neuron Backpropagation 神经元内的反向传播
      在$t_m$ 处的膜电位不仅对突触后电流有贡献，还会影响$t_p$处的膜电位 $u_i^{(l)}[t_p]$，$PSC a_i^{(l)}[t_k]$ 对 $u_i^{(l)}[t_p]$ 存在神经元间依赖性（上面说的），而 $u_i^{(l)}[t_p]$ 由于在$t_m$处的突触前电位重置而进一步受到$t_m$的影响，进而也就形成了一个间接影响。  
    
      概括为 ： $u_i^{(l)}[t_m]$  进行重置影响了 $u_i^{(l)}[t_p]$，而$u_i^{(l)}[t_p]$  会对后续脉冲的PSC产生影响。
      ![image-20240225114045108](../../../AppData/Roaming/Typora/typora-user-images/image-20240225114045108.png)
    
      结合公式(4)可以得到(13)中的变体。 $t_p$处的膜电位是$t_m$处的脉冲序列进行重置后再$t_p$时间点的值。
      最后的一个等式结果可以分为三项，反向传播过程中，在m处计算导数时，该项已经求出；第二项和第三项依据（12）中的求解过程进行即可。
    
      ![image-20240225142819775](../../../AppData/Roaming/Typora/typora-user-images/image-20240225142819775.png)
    
      从上往下分析，第一个等于0是显然的，因为脉冲均为0
      第二项当 $t_p$ 处的脉冲为0，该项简化为（11）
      当两者均不为0，该项变为 （11） + （13）的两个误差，也就是文中一直强调的 神经元内和神经元间的误差。
      ==按照作者所说的，膜电位对突触后电位的影响在计算误差时最多使用相邻三个脉冲来进行计算，是否存在弊端？==
  
* **实验部分：**

* **讨论与总结：**



2. # ==测试==

* **期刊：**
* **作者及单位：**
* **摘要：**
* **介绍：**
* **方法：**
* **试验：**
* **讨论与总结：**







# 十二

# ==1==



* **摘要：**  
* **介绍：**
* **方法：**
* **试验：**
* **讨论与总结：**
