# 关于文献阅读

阅读完文献后，至少要回答以下问题:

1. 这篇文章到底在解决什么问题？
2. 这个问题为什么在这个领域重要？
3. 这些作者是如何解决这个问题的？
4. 这个问题的解决有什么亮点，局限，有什么应用？

看5-10遍，能够回忆起研究目的，研究方法，研究过程，研究成果，研究结论就可以了，至于公式的推导，确实需要花费大量时间的，不重要的可以不去推导。

文献可以分为写的好的和写的差的，写的差的没必要看懂，写的很好的看不懂就很正常了。

不要只看不写，早点动手写论文。

前期是大量的泛读，然后总结。通过标题摘要大概了解这个领域多少人用了什么类型的方法，有全局的思维，基础的了解。之后有选择的精读文献。 精度不是一次就读懂，需要慢慢来。对好的文章多读。

对自己领域的文章进行精读是确保不要让自己和别人想法一样或者自己想法已经被证明是错误的。



[深度学习知识蒸馏的研究综述](https://mp.weixin.qq.com/s/OKX2vdkzSyOfAn7YEA6Sag)
[知识蒸馏详解](https://mp.weixin.qq.com/s/zBVm20uUONMwsKV4YrjQIg)

## 1. LaSNN: Layer-wise ANN-to-SNN Distillation for Effective and Efficient Training in Deep Spiking Neural Networks

Di Hong, Jiangrong Shen, Yu Qi∗, Yueming Wang∗    浙江大学

逐层人工神经网络到脉冲神经网络的蒸馏：有效且高效的深度脉冲神经网络训练

* **摘要：**  

  layer-wise ANN-to-SNN knowledge distillation(LaSNN).

  ​	==背景：== 脉冲神经网络在生物真实感和低功耗计算方面具有良好的前景，主要得益于其事件驱动机制。然而，SNN的训练在各种任务中通常会遭遇准确性损失，相较于人工神经网络的表现不尽人意。为了解决这一问题，提出了一种转换方案，旨在通过将训练好的ANN参数映射到相同结构的SNN中来获得竞争力的准确性。然而，这种转换的SNN在推理时需要大量的时间步，进而失去了能效优势。（该段说明了ANN to SNN 的缺点）

  ​	==本文工作：== 为充分发挥ANN的准确性优势和SNN的计算效率，我们提出了一个新颖的SNN训练框架—逐层ANN到SNN的知识蒸馏（LaSNN）。

  ​	==具体工作：== 为了实现竞争力的准确性和降低推理延迟，LaSNN通过知识蒸馏将学习从经过良好训练的ANN迁移到小型SNN，而不是直接转换ANN的参数。引入注意力机制，弥补了异构ANN和SNN之间的信息差距，使得ANN中的知识得以有效压缩并通过我们逐层的蒸馏范式高效转移。

  ​	==实验：==  验证LaSNN在三个基准数据集（CIFAR-10、CIFAR-100和Tiny ImageNet）上的有效性、高效性和可扩展性。与ANN相比，我们在top-1准确率上表现出竞争力，并实现了相较于转换后SNN快20倍的推理速度，同时保持相似的性能。更重要的是，LaSNN灵活性强且可扩展，能够轻松适应不同架构/深度以及输入编码方式的SNN，为其潜在的 发展提供了有力支持。

* **介绍：** 

  ​	那么，接踵而来的问题便是：是否存在一种最佳方式，既能借助经过良好训练的ANNs的指导（如转换方案），又能同时保持基于脉冲计算的效率（如替代方案）？为此，我们通过逐层知识蒸馏的方法，开发了一个新颖的框架，称为LaSNN。与转换方法类似，它利用经过良好训练的ANN来指导SNN的训练过程。然而，我们并不是直接转换网络参数，而是提出从ANN向目标SNN进行知识蒸馏，以实现低功耗和快速推理。

  ​	实现这一目标的关键在于如何有效地将知识从ANN蒸馏到SNN中。知识蒸馏通常是在强大的教师模型（性能较高）与较小的学生模型之间传递学习。尽管知识蒸馏在提升分类准确性和模型压缩方面具有良好的前景，但其应用主要受到同质模型的约束（例如，从ANN到ANN）。由于SNN的脉冲表示和计算方式与使用模拟值的ANN存在显著差异，以往的蒸馏方法和训练技术无法直接应用于异构模型。

  ​	==本文贡献如下：==

  1) 我们提出了一种ANN到SNN的知识传递方法，而非简单地将网络参数直接转换。这一方法利用注意力机制作为共享信息表示，旨在弥补ANN与SNN之间的信息差距。

  2) 我们提出了一种逐层ANN到SNN的蒸馏框架，能够有效地将知识从ANN转移到SNN。LaSNN的训练流程分为三个阶段：首先，训练一款复杂的ANN作为教师模型；其次，利用ANN到SNN的转换方法初始化学生SNN的参数；最后，基于逐层蒸馏方案对初始SNN进行训练，使其能够模仿教师ANN的推理过程。该框架使得SNN能够压缩来自大型ANN的知识，从而实现准确和高效的计算。

  3) 我们在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上进行详细实验，以评估我们的方法。实验结果表明，LaSNN在Top-1准确率上与ANN们相当，并且推理速度比转换后的SNN快20倍，同时保持相似的准确性。更重要的是，LaSNN适用于不同的架构、深度和编码方法。因此，我们的研究强烈表明，LaSNN在训练深度SNN方面具有显著优势。

* **相关工作：** 

  ​	训练SNN的算法可以分为三类：突触可塑性的学习规则，替代梯度训练，ANN to SNN.

  * ==突出可塑性的学习规则：==  依据神经元发放脉冲的时间间隔更新连接到权重值，缺乏全局信息，所以只能用于简单的任务和神经形态图片的处理。文章中就不做介绍了。

  * A. ==Training of SNNs== 

    1. Surrogate-gradient Algorithms

       使用近似函数替代阶跃函数的导数
       缺点：需要计算每个时间步的梯度和反向传播误差，故计算量大且速度慢，且近似函数带	   来的误差会累加，对于深层SNN的训练不友好。

    2. ANN-to-SNN Conversion

       之前的转换需要较多的时间步(2000-2500个时间步)来匹配ANN的激活值；
       最近的研究中，作者专注于逐层校准SNN的参数，以使转换后的激活相匹配。然而，这种方法需要复杂的架构，并且对于小型SNN并不具备可扩展性。
       [参考文献-A free lunch from
       ann: Towards efficient, accurate spiking neural networks calibration](https://proceedings.mlr.press/v139/li21d/li21d.pdf)

    3. Hybrid SNN Training  

       结合了反向传播和ANN to SNN，一方面克服BPTT的高计算需求，同时保持推理的低延迟（100-250个时间步）。
       具体来说，SNN是从预训练的ANN转换而来的，随后使用代理梯度或BPTT进行微调。
       然而，该方法仍然依赖内在信息来优化SNN的权重，缺乏额外的指导。
       因此，我们需要进一步优化准确性与延迟之间的平衡。
       
       [参考文献](https://openreview.net/pdf?id=B1xSperKvH)

  * B. Knowledge Distillation

    ​	知识蒸馏已被证明是一个有效的模型压缩方法，通过将复杂模型（教师模型）的知识转移到小型模型（学生模型）中，实现了这一目标。通常，使用的软标签包含比单热编码标签更多的信息，这些软标签在交叉熵损失函数中被用来规范化学生模型的学习过程。以往的研究主要集中在人工神经网络（ANN）之间的蒸馏，通过重新构建监督信号(上文提到的软标签)来实现更有效的知识转移，例如注意力转移等方法。

    ![image-20240916192655444](./assets/image-20240916192655444.png)

    ![image-20240916192751026](./assets/image-20240916192751026.png)

  * C. SNN Training with Knowledge Distillation

    ​	之前的工作都是基于标签信息的蒸馏，这篇文章要做一个基于注意力的蒸馏来进一步提高SNN的性能。

    ​	在本文中，我们首先介绍了一种基于注意力的蒸馏方法，旨在弥合人工神经网络与脉冲神经网络之间的信息表示和传递差距。基于这一注意力机制，我们提出了一种分层蒸馏框架，称为LaSNN，该框架能够有效且高效地转移模型的学习过程。此外，我们采用了替代梯度方法，以优化知识蒸馏过程中SNN的参数。

* **The LASNN Framework：**

  <img src="./assets/image-20240916193833972.png" alt="image-20240916193833972" style="zoom: 50%;" />

  1. SNN model

     * Encoding Method

       ​	我们采用时间编码方案，将输入图像（模拟值）编码为时空模式（脉冲序列），因为在脉冲神经网络中，信息的表示和传递通过脉冲进行。输入图像的像素值被归一化到[-1, 1]的范围内。当将这些归一化值输入到泊松编码器时，其输出的泊松脉冲序列的速率(平均脉冲发放率)与相应像素的强度成正比。具体而言，在输入图像后，泊松编码器将在每个时间步产生每个像素的随机数，然后将这些随机数与归一化值进行比较。当随机数小于归一化值时，就会输出脉冲。因此，从长时间来看，这些泊松分布的脉冲序列与像素值是相等的。
       （理解：当时间步非常多时，我们可以认为每个像素编码成的脉冲序列值是一致的，这样的话，这样一来，序列中的数值是否小于当前像素值也是确定的，故相同像素值发放脉冲的数量是一致的，且像素值越大，发放的脉冲数量越高。）

     * Spiking Neuron

       <img src="./assets/image-20240916195940271.png" alt="image-20240916195940271" style="zoom: 33%;" />

       使用的还是LIF神经元，其中阈值θ在同一层内的值不变，而泄露因子λ为全局固定值。

       <img src="./assets/image-20240916200438516.png" alt="image-20240916200438516" style="zoom: 33%;" />

       也就是最终使用的是膜电位来进行分类预测。输出层中神经元的数量和分类目标的数量是一致的。最终输出的是概率

     * Network Architectures

       ​	架构与传统ANN中的VGG，Residual结构类似。但是有一些细节需要被修改来最小化转换过程中的损失。
       ​	首先，由于采用偏置项会增加阈值平衡的难度及转换损失的概率，因此我们不使用偏置项。
       ​	由于去除了偏置项，Batch Normalization也未被采用（BN层里面会涉及到偏置项，当然也可以将BN层的偏置项置为0），这使得每一层的输入偏置都变为零。
       ​	我们在ANN和SNN中都使用了Dropout作为一种可选择的正则化方法。
       ​	其次，我们采用平均池化操作来减少特征图的大小。引入最大池化操作可能会导致显著的信息损失，因为SNN中的激活是二值的。（最大池化选择最大值的操作，可能会造成数值的剧烈变化，导致脉冲发放不平稳）
       ​	我们将残差架构中的原始大卷积核（7x7，步幅2）替换为一个由三个小卷积层（3x3，步幅1）和两个Dropout层组成的替代模块。

  2. ANN-to-SNN Distillation
         教师模型是一个经过充分训练的复杂ANN，包含丰富而准确的注意力信息，而学生模型则是一个具有相似ANN架构的小型SNN。我们将阐述两种表示注意力信息的方法：
     1）基于激活的方案
     2）基于梯度的方案
     这两者共同缩小了真实值ANN与离散信号SNN之间的知识差距。

     * Distilling Activation-based Attention 
       	假设隐藏神经元相对于特定输入的重要性由其绝对值表示，定义$A∈R^{C*H*W}$,表示人工神经网络卷积层的激活张量，定义下面的映射函数：

       <img src="./assets/image-20240916205600716.png" alt="image-20240916205600716" style="zoom:33%;" />

       其中对A进行了平方，是为了增加区分度。这样的话教师模型和学生模型之间的损失就多了一项，如下所示：

       <img src="./assets/image-20240916205736045.png" alt="image-20240916205736045" style="zoom:33%;" />

       Ta为教师模型，St为学生模型，$Z$为网络的层数，使用了$L2$​ 范数，将各层的损失进行平方累加求和并开根号。

       <img src="./assets/image-20240916210117564.png" alt="image-20240916210117564" style="zoom:33%;" />

       总的损失就得到了，其中a是一个超参数，$p_i$ 是教师模型输出的概率分布，$y_i$ 是学生模型的输出。

     * Distilling Gradient-based Attention

       ​	假设一个像素的细微变化会对模型的预测产生显著影响，那么模型就会对该像素给予特别关注。在这种情况下，我们定义了一种基于梯度的注意力，这被视为网络所学习的输入敏感性知识。换句话说，模型对输入特定空间位置的关注反映了其输出预测对该位置变化的敏感度。因此，教师模型关于输入的损失梯度定义如下：

       <img src="./assets/image-20240916211420587.png" alt="image-20240916211420587" style="zoom:33%;" />

       ​	受到脉冲激活图（Visual explanations from spiking neural networks using inter-spike intervals，SAM）的启发，我们利用前向传播中的脉冲活动来定义输入敏感性函数。

       <img src="./assets/image-20240916211913988.png" alt="image-20240916211913988" style="zoom:33%;" />

       ​	$t^{’}$ 表示上一次脉冲发放时间，集合 $O_{h,m}$ 位于(h,m)处的神经元上一次发放脉冲的时间，这里的(h,m)就是每一层中图片的长和宽。

       <img src="./assets/image-20240916212633459.png" alt="image-20240916212633459" style="zoom:33%;" />

       这里是假定了相同层时，学生模型和教师模型的特征图相同，如果不同的话，就采用插值法或者下采样法进行形状的匹配。

  3. Layer-wise Supervision Strategy

     ​	以往的研究表明，模型每一层的不同部分包含着不同的注意力信息。在低层，神经元表现出强烈的激活；在中层，神经元的激活则倾向于集中在可识别的区域，如脚或眼睛；而在高层，神经元的激活则反映整个物体的特征。

     <img src="./assets/image-20240916213334201.png" alt="image-20240916213334201" style="zoom:33%;" />

     ​	为了实现最小损失，我们将学习到的注意力知识分为三个层次，从低到高，以充分蒸馏来自教师ANN的注意力信息，并通过设计的损失函数（方程7或12）来监督迁移损失的过程。

  4. Three-stage Training Process of LaSNN

     <img src="./assets/image-20240916213610131.png" alt="image-20240916213610131" style="zoom:33%;" />

     ​	首先，训练一个带有偏置项和批归一化的教师（大型）ANN。
     ​	其次，将一个较小的单一ANN（中间ANN）转换为学生SNN，并采用脉冲神经元。在转换过程中，我们采用阈值平衡的方法，保持权重不变，然后由最大预激活值进行归一化。
     ​	第三，在教师模型训练完成及学生模型转换后，采用逐层方案将教师ANN的知识转移至学生SNN，包括蒸馏策略和损失函数的设计。
     ​	此外，学生SNN通过反向传播进行优化，利用线性替代梯度来近似不连续梯度。梯度函数如下：

     <img src="./assets/image-20240916214144132.png" alt="image-20240916214144132" style="zoom:33%;" />

     ​	为了增加稳定性，引入了一个衰减项λ：

     <img src="./assets/image-20240916214317114.png" alt="image-20240916214317114" style="zoom:33%;" />

     ​	最终损失关于权重的导数如下：

     <img src="./assets/image-20240916214501642.png" alt="image-20240916214501642" style="zoom:33%;" />

     <img src="./assets/image-20240916214823621.png" alt="image-20240916214823621" style="zoom:33%;" />

  5. 训练过程的伪算法：

     <img src="./assets/image-20240917092214474.png" alt="image-20240917092214474" style="zoom: 50%;" />

     

* **试验：**

  1. 数据集

     cifar-10,cifar-100,tiny-ImageNet(200个类别，每个类别500张训练，50张验证和50张测试，64*64，RGB)

  2. 训练细节

     首先训练一个大的ANN和小的ANN，其中大的ANN在300次迭代以后收敛，小的ANN在200次迭代以后收敛，接着将小的ANN转为SNN，使用的时间步为2500，然后利用大的ANN进行知识蒸馏给SNN，迭代次数为100，batch_size为16，时间步设置为100，利用上文提到的逐层蒸馏和基于标签的蒸馏。 
     3090，24GB

  3. 结果
     <img src="./assets/image-20240917093554027.png" alt="image-20240917093554027" style="zoom:50%;" />

     <img src="./assets/image-20240917093754433.png" alt="image-20240917093754433" style="zoom:50%;" />

     ​	从结果上看，这篇论文提到的蒸馏方式提高了精度，但是观察网络模型不难发现，这篇论文连ResNet32或者ResNet50都没有用，有两种可能：网络更深时，训练时间以及转换时间太长，作者没有做这个实验，另外一种可能就是，网络更深时，文中所提到的方法效果并不好甚至精度可能很低，所以作者并没有说。

  4. 还有多个实验比较，这里就不列出来了。另外这篇文章并没有给出代码。

     

* **讨论与总结：**



# 2.Constructing Deep Spiking Neural Networks from Artificial Neural Networks with Knowledge Distillation

从人工神经网络构建深度脉冲神经网络的知识蒸馏方法

CVPR2023，大连理工大学(Dalin University of Technology)，浙江大学的教授是通讯

* **摘要：**  

  ==背景：== 当前SNN性能受网络架构和训练方法的限制。SNN不可导的问题，无法直接使用梯度下降。
  knowledge distillation（KD）

  ==工作：== 使用知识蒸馏的方式，ANN作为教师模型，SNN为学生模型，避免了从头开始训练SNN.

  ==意义：== 本文方法可以合理地构建更高效的深度脉冲结构，而且与直接训练或ANN to SNN的方法相比，用更少的时间步数训练整个模型。更重要的是，该方法在面对各种人工噪声和自然信号时展现出卓越的抗噪能力。所提出的新方法为通过构建更深的结构以高吞吐量的方式提高SNN的性能提供了有效途径，具有在实际场景中应用于轻量化和高效的大脑启发计算的潜力。

* **介绍：** 

  1. 使用替代梯度的方法直接训练SNN需要花费很多的计算资源。
  2. 混合模型的训练比较难
  3. 直接将训练后的ANN转换为iSNN,利用的是激活值和脉冲发放率相等。[文章-Optimal ANN-SNN conversion for fast and accurate inference in deep spiking neural networks](https://arxiv.org/pdf/2105.11654)
  4. 和传统ANN to SNN 框架必须的方式不一样，知识蒸馏中的ANN 和 SNN 框架可以不一致。这种方法可以加速训练时间并节约内存。
  5. 使用了三个数据集：MNIST, CIFAR10, and CIFAR100
  6. 主要是测试了模型抗噪能力，证明SNN的知识蒸馏可应用于某些场景。

* **相关工作和动机：**  Related Work and Motivation
  ==总结当前训练SNN的方式：STDP训练，ANN to SNN, Surrogate gradient，ANN distill SNN，Hybrid SNN Training（先训练ANN,接着转为SNN，接着训练SNN），Using ANN to Enhance SNN==

  1. ANN-to-SNN Conversion Methods

     ANN的训练和SNN的转换都非常消耗资源，丧失了实时更新时空信息的能力。

  2. Surrogate Gradient Training Methods

     尽管在某种程度上，替代梯度可以缓解神经脉冲的不可导性，但从本质上来看，这仍然是一种梯度训练方法，生物学上并不太具备可信性。

  3. Motivation

     ​	基于上述的问题，提出知识蒸馏的SNN，结合ANN-to-SNN 和 surrogate gradient，在本文中，我们提出了两种ANN-SNN联合损失函数，以更好地实现从SNN到ANN的知识蒸馏。第一种方法分别利用ANN和SNN的一个输出层，而第二种方法则在多个中间层与输出层之间构建了联合损失函数。

     ​	通过所提出的知识蒸馏训练过程，构建的学生SNN模型能够从教师ANN那里学习到丰富的特征信息，从而使ANN和SNN的结构得以异构。该方法不仅采用ANN到SNN的转换，尽可能保持ANN和SNN的输出接近，还利用了替代梯度方法，将不可微分的函数替换为连续函数，并在梯度计算过程中应用，从而高效地训练深层SNN。

* **方法：**

  <img src="./assets/image-20240917111423272.png" alt="image-20240917111423272" style="zoom: 33%;" />

  1. Spiking Neuron Model

     使用IF神经元作为SNN模型

  2. Joint ANN-to-SNN Knowledge Distillation Method
     我们的方法分为两种：基于响应的知识蒸馏和基于特征的知识蒸馏。第一种方法仅从教师ANN模型最后一层的输出中提取知识。第二种方法则从教师ANN模型的多个中间层中提取知识。

     * KDSNN with response-based knowledge distillation.
       <img src="./assets/image-20240917112952211.png" alt="image-20240917112952211" style="zoom: 33%;" />

       $Z_i$ 代表的是教师模型的输出，T是为了使得输出值更加平滑。

       首先，对教师ANN模型进行预训练，并在训练学生SNN模型时固定教师ANN模型的权重。随后，学生SNN模型通过公式(4)从教师ANN模型的输出中学习隐含知识。当然SNN也要学习真实标签。

       <img src="./assets/image-20240917113835415.png" alt="image-20240917113835415" style="zoom:33%;" />

       $T>=1$，α是用来控制那一部分更重要的，a>=0 且a<=1

       

     * KDSNN with feature-based knowledge distillation.

       基于响应的知识蒸馏只能学习到最后几层的知识，无法获取整个ANN的知识，所以才有了这个基于特征的知识蒸馏方式，让SNN学习ANN中间层的特征知识。

       学生SNN模型的特征是通过发放率进行编码的。

       对于resnet，就是在每个block之后进行特征的学习，当特征图尺寸不匹配时，就使用1x1的卷积来操作。

       <img src="./assets/image-20240917115054383.png" alt="image-20240917115054383" style="zoom:33%;" />

       ==和上一篇文章一模一样，有没有，大同小异。==

       其中，$T_i$指的是经过1×1卷积层转换后的学生SNN模型的中间层特征。为了抑制负面信息的影响，教师ANN模型的特征需要经过边缘ReLU进行转换，而 $S_i$​ 则是经过转换后基于脉冲的特征。

       <img src="./assets/image-20240917115650864.png" alt="image-20240917115650864" style="zoom:33%;" />

       $L_{task}$​ 就是上文中提到的SNN与真实标签的交叉熵损失

       

     * Training of student SNN model.

       使用替代梯度的方法进行训练。

       文章伪算法：

       <img src="./assets/image-20240917120324129.png" alt="image-20240917120324129" style="zoom:50%;" />

     ​	在第一步中，我们选择具有更高准确性和更复杂模型的人工神经网络模型作为教师模型。教师模型是预训练的，并且在训练学生SNN时，教师ANN模型的权重参数是固定的。

     ​	在第二步中，我们选择一个SNN模型作为学生SNN模型。接着，我们利用教师ANN模型的隐含知识来指导学生SNN模型的训练。在前向传播的过程中，使用相同的数据集样本作为教师ANN模型和学生SNN模型的输入。学生SNN模型将输出转换为脉冲频率作为其特征。预训练的教师ANN模型计算教师模型的输出或从教师模型的中间层提取特征。随后，我们可以得到带有隐含知识的总损失函数，即公式（5）或公式（7）。在误差反向传播的过程中，采用代理梯度方法计算总损失函数的导数，从而更新学生SNN模型的突触权重。

* **试验：**

  ![image-20240917142941996](./assets/image-20240917142941996.png)

  ![image-20240917143303963](./assets/image-20240917143303963.png)

  ![image-20240917143334886](./assets/image-20240917143334886.png)

  ![image-20240917143559857](./assets/image-20240917143559857.png)

  

# 3. Joint A-SNN: Joint training of artificial and spiking neural networks via self-Distillation and weight factorization

Joint A-SNN：通过自蒸馏和权重分解对 ANN 和 SNN 进行联合训练

中国航天科工集团智能科学与技术研究院
2022年11月1日提交的文章，2023年3月18修订的文章，2023年4月27在线发表。

* **摘要：**  

  ==背景：== 这也导致了训练SNN从头开始需要重新定义发射函数以计算梯度的内在障碍。与之相比，ANN是完全可微的，可以通过梯度下降进行训练。

  ==工作：== 在本文中，我们提出了一种ANN和SNN的联合训练框架，其中ANN可以指导SNN的优化。该联合框架包含两个部分：首先，利用网络中的多个分支，将ANN中的知识蒸馏到SNN。其次，我们对ANN和SNN的参数进行了限制，使它们共享部分参数并学习不同的独立权重。

  ==实验：== 在多个广泛使用的网络结构上的大量实验表明，我们的方法始终优于许多其他最先进的训练方法。例如，在CIFAR100分类任务上，通过我们的方法训练的脉冲ResNet-18模型只需4个时间步长即可达到77.39%的Top-1准确率。

* **介绍：** 

  SNN的时序处理能力使得其学习时空信息和ANN不同。
  STDP算法缺乏关于误差的全局信息。
  替代梯度使得SNN的训练不稳定，收敛慢，相比于ANN。
  ANN to SNN  需要很长的时间步去推理，且这样的话SNN没有自己学习的特征

  ​	在这项工作中，我们提出采用ANN和SNN的联合训练框架。在训练期间，ANN和SNN的部分参数是共享的。
  ​	具体而言，我们使用奇异值分解（SVD）进行权重矩阵分解，并让ANN和SNN优化各自的奇异值权重，但保持奇异向量相同。---**奇异值不同，但是奇异值向量相同**。
  ​	其次，我们在网络中添加多个分支，从而实现知识从ANN到SNN的蒸馏，这被称为自我蒸馏。	
  ​	我们的方法可看作是ANN to SNN 和 替代梯度 训练的结合。
  ​	我们不改变SNN中的替代梯度，但在SNN的训练过程中提供指导。
  ​	不像ANNtoSNN，这两者共享相同的参数，我们增加了自由度 (例如参数中的不同奇异值)，但限制它们完全不同。

  ==扩展知识：== 权重矩阵分解（Weight Matrix Decomposition）是一种通过将权重矩阵分解成几个较小矩阵的乘积来优化模型的技术。这种方法在深度学习中具有减少模型参数、降低计算复杂度和提升模型性能等多种好处。
  	==方法：== 奇异值分解，低秩分解，张量分解，分块矩阵分解。
  	==应用领域：== 卷积神经网络（将3D卷积分解为多个1D卷积），全连接层（权重矩阵的分解），模型压缩与加速，知识蒸馏。
  ![image-20240917160140372](./assets/image-20240917160140372.png)

* **相关工作：**

  1. SNN（ANNtoSNN, Surrogate gradient）——之前的工作都没有很好的解决脉冲发放函数以及梯度消失/爆炸的问题。
  2. Knowledge distillation

* **Preliminary：**

  1. Leaky integrate-and-Fire model

     考虑到直接输出脉冲的数量来计算噶概率会丢失掉太多的信息（ANN输出有正负，但是SNN的输出只有正值），本文选择整合网络的输出。

     <img src="./assets/image-20240917165936327.png" alt="image-20240917165936327" style="zoom:33%;" />

     <img src="./assets/image-20240917170016342.png" alt="image-20240917170016342" style="zoom:33%;" />

     <img src="./assets/image-20240917165902665.png" alt="image-20240917165902665" style="zoom:33%;" />

     之后就可以根绝真实标签和Softmax（$y_{net}$）去计算损失了。

* **Method:** 

  首先是阐述了本工作的动机，例如梯度计算问题，其次介绍了自蒸馏算法，最后是参数的权重分解

  1. Motivation

     <img src="./assets/image-20240917173130423.png" alt="image-20240917173130423" style="zoom:33%;" />

     <img src="./assets/image-20240917173621079.png" alt="image-20240917173621079" style="zoom:33%;" />

     它们都有一个超参数来控制替代梯度的宽度和锐度。如果$V_{th}=0.5 且a=1$​​，矩形函数将变成Straight-Through Estimator(反向传播时梯度为恒定值。

     SNN的输出是0或者1，存在梯度消失或者爆炸的问题，即使有了残差块，仍然存在.(可以参考之前讲解的SEW_Resnet网络)

     ==扩展知识：== 替代梯度的宽度和锐度    the width and the sharpness
     宽度指的是梯度的覆盖程度，宽度越大，梯度更加平滑，但可能导致梯度不精确。
     锐度指的是梯度的陡峭程度，锐度越高，梯度更加陡峭，但可能导致梯度消失问题。

  2. Self distillation

     仿照残差块，在每个block之后，添加分支，用于知识蒸馏，共添加了四个分支。蒸馏框架的整体损失如下：

     * 中间分支都要和真实标签做损失，$z$为真实标签

       <img src="./assets/image-20240917180706775.png" alt="image-20240917180706775" style="zoom:33%;" />

     * ANN 和 SNN 输出之间的 KL散度损失。这一损失函数基于softmax（即类别概率）进行计算，确保SNN的输出应该模仿ANN的输出。其公式可以表示为：

       <img src="./assets/image-20240917181123269.png" alt="image-20240917181123269" style="zoom: 50%;" />

       此时ANN的输出在计算时是梯度分离的，否则会导致准确率下降。

     * 由于SNN架构中的时间维度和二进制激活，ANN中的特征和SNN中的特征可能具有不同的量纲。因此，我们在ANN和SNN的输出特征之间施加L2范数损失，其公式为：

       <img src="./assets/image-20240917181739983.png" alt="image-20240917181739983" style="zoom: 33%;" />

       ​	我们将这三个损失函数同时优化，以将知识从ANN转移到SNN。值得注意的是，==来自第一阶段和第二阶段输出的损失函数为浅层提供了指导信息==，从而缓解了梯度爆炸/消失的问题。总体损失函数可以写作:

       ![image-20240917182007667](./assets/image-20240917182007667.png)

       $λ_1和λ_2是超参数，用于控制蒸馏的损失。$

  3. Weight-Factorization training（WFT） **partial weight-sharing**

     ​	对权重参数应用奇异值分解（SVD），并共享ANN和SNN的特征向量，同时分别优化ANN和SNN的特征值
     ​	对一个全连接层权重值进行奇异值分解： $W ∈ R^{c_{in} ×c_{out}}$

     <img src="./assets/image-20240917183935908.png" alt="image-20240917183935908" style="zoom:33%;" />

     σ是奇异值，r是奇异值的数量，SVD可以把权重分解成：

     <img src="./assets/image-20240918145320243.png" alt="image-20240918145320243" style="zoom:33%;" />

     在本文的权重因子化训练中，SNN和ANN具有相同的特征向量，但是具有不同的特征值：

     <img src="./assets/image-20240918145647398.png" alt="image-20240918145647398" style="zoom:33%;" />

     在梯度下降中，这几个矩阵 $U,V,∑_{ANN},∑_{SNN}$​  是同步更新的。

     <img src="./assets/image-20240918150207027.png" alt="image-20240918150207027" style="zoom: 33%;" />

**结果:** 

<img src="./assets/image-20240918153908672.png" alt="image-20240918153908672" style="zoom: 50%;" />

<img src="./assets/image-20240918154056038.png" alt="image-20240918154056038" style="zoom: 33%;" />

**Computational Energy Cost： ** ANN中 multiply-accumulate (MAC)的操作 消耗 4.6pJ。 SNN中accumulation的操作消耗0.9pJ，
 ==参考文献1:== Diet-snn: direct input encoding with leakage and threshold optimization in deep spiking neural networks  
 ==参考文献2:==  1.1 computing’s energy problem (and what we can do about it),

**Data Movement Energy Cost：** 

==总结:==  该方法在多个广泛使用的网络结构上，较原始基线和现有最先进技术都带来了显著的提升。由于ANN在空间特征处理方面表现卓越，但在时间特征处理上相对较弱，因此依赖ANN的指导将在一定程度上限制SNN的时空处理能力。未来，我们将进一步研究ANN与SNN结合的方式，以优化基础代码，并保留SNN的时空处理能力。（有点扯）

奇异值分解：[参考链接](https://mp.weixin.qq.com/s?__biz=MjM5NzEyMzg4MA==&mid=2649469857&idx=7&sn=516b3ae2bee08db0a450c46c826d9c45&chksm=bec1d1e689b658f044cf9896ae3fb5ce5472f00cfd6ad61611239dfcc1e44fecf08838d3d770&scene=27)



# 4. SpikingBERT: Distilling BERT to Train Spiking Language Models Using Implicit
Differentiation

通过隐式微分对BERT进行蒸馏以训练脉冲语言模型

[代码地址](https://github.com/NeuroCompLab-psu/SpikingBERT)

* **摘要：**   

  ​	==背景：== 尽管大型语言模型（LLMs）日益强大，但其神经元和突触的数量仍远远少于人脑。然而，它们的运行所需的功耗却显著更高。

  ​	==工作：== 在本研究中，我们提出了一种新的仿生脉冲语言模型，旨在通过借鉴大脑中突触信息流的原理，降低传统语言模型的计算成本。本文展示了一种框架，利用神经元在**平衡状态下的平均脉冲频率**，通过隐式微分技术训练神经形态脉冲语言模型，从而克服SNN的不可微问题，而无需使用任何替代梯度。

  ​	==细节：== 脉冲神经元的稳态收敛特性使我们能够设计一种脉冲注意力机制，这对于实现可扩展的脉冲语言模型至关重要。此外，利用神经元在平衡状态下的平均脉冲频率的收敛性，我们开发了一种基于ANN-SNN知识蒸馏的新技术，其中使用经过预训练的BERT模型作为“教师”，来训练我们的“学生”脉冲架构。尽管本文中提出的主要架构受BERT的启发，但该技术也可延伸至不同类型的大型语言模型。

  ​	==实验：== 我们的工作首次展示了一个可操作的脉冲语言模型架构在多个不同任务上的性能，基于GLUE基准测试。

  

* **介绍：**

  大语言模型比较耗能，而使用SNN具有节能的优势，预期可以获得更好的能耗比。
  任务的复杂性，加上所需模型体系结构不断增长的深度，使得BPTT实际应用不可行。
  并没有使用BPTT，而是利用前向阶段到平衡状态的平均脉冲发放率（在历经所有时间步之后）

  ==average spiking rate (ASR)  平均脉冲率==
  在收敛后，可以从基础模型中推导出固定点方程，并随后对得到的稳态进行隐式微分，从而有效地训练模型参数。

  隐式微分训练主要应用于==深度平衡模型（deep equilibrium models，DEQ）==（Bai, Kolter, 和 Koltun 2019，**我下载了这篇论文，然后这些作者在2020年还更新了这个算法--MDEQ，另外这个DEQ第一次被使用，且已经发表的论文我也下载了---Xiao et al. 2021**）。最近，这一方法也被用于基于卷积的脉冲神经网络架构，以应对视觉相关任务。这种方法在训练过程中展现出卓越的内存效率，相较于需要大量内存来存储庞大计算图的反向传播通过时间（BPTT）方法，它更加高效。此外，==通过隐式计算梯度，这种方法消除了对代理梯度方法的需求，进而规避了脉冲模型的非可微性问题。==在特定约束下，这种学习形式与生物合理和基于能量的训练方法如平衡传播相似，从而进一步支持了神经形态学习的视角。

  在本论文中讨论的基于Transformer的语言模型中，注意力机制是一个至关重要的组成部分。然而，传统的注意力机制在本质上并不具备脉冲特性，因为它依赖于一系列实值向量来表示查询、键和值。我们在此提出了一种脉冲注意力机制，利用脉冲输入，并在模型收敛所需的时间步数（$T_{conv}$​）上进行运算。在神经元达成平衡状态时的脉冲发放率（ASR）收敛，使我们能够在脉冲注意力层的ASR与传统注意力之间建立密切的等价关系。

  ==本文贡献：== 

  1. SpikingBERT with Spiking Attention
     提出了脉冲注意力，在平衡状态下近似于普通的脉冲注意力。
  2. Spiking LM Training
     在理论上和实践上证明了所提的训练方法能够收敛，且不使用替代梯度。具有可扩展性。
  3. ANN-SNN KD using Equilibrium States
     在脉冲网络达到平衡状态时，使用知识蒸馏框架去更有效的训练SNN，使用大型ANN可以得到小型SNN。对于SNN来说，参数不用增加。

* **相关工作：**  

  1. Spiking Architectures  脉冲网络的架构：

     * 基本都是在图像分类上，很少又在NLP任务上的。大部分模型都是没有注意力的浅层模型。

     * GPT是一个基于解码器的架构，而BERT是一个基于编码器的架构，可以捕捉双向上下文信息，更适合文本分类问题。在很多GLUE benchmark的任务上较好。
     * ==之前也有关于知识蒸馏的SNN，但是还是基于BPTT和替代梯度的。提到了上面介绍的其中两篇文章。==

  2. Efficient LMs  有效的语言模型

     * 先前关于语言模型的蒸馏也做了很多的工作，包括将BERT精简化，BERT蒸馏，甚至将BERT蒸馏到LSTM上。
     * 由于目前尚无合适的神经形态基准，我们将所提议的模型与现有的标准自然语言处理模型和高效语言模型进行了比较。   ==standard NLP models and efficient LMs==

* **方法：** 

  ​	在本节中，我们将首先探讨我们方法的基本原则。随后，我们将深入网络结构的细节，以提供全面的理解。我们研究了==在使用隐式微分法训练的脉冲神经网络中， 平均脉冲率(ASR)收敛的理论和实证基础==。此外，我们提出了一种创新的方法，以利用这一收敛现象设计脉冲注意力机制，并引入了一种==新颖的知识蒸馏机制==，采用预训练的BERT模型作为“教师”，从而提升学习过程。我们还详细阐述了使用隐式微分技术训练我们脉冲架构的框架。

  <img src="./assets/image-20240918205521336.png" alt="image-20240918205521336" style="zoom:50%;" />

  1. Spiking Neural Networks
     使用的是LIF神经元

     <img src="./assets/image-20240921190810611.png" alt="image-20240921190810611" style="zoom:33%;" />

  2. Implicit Modeling    隐式建模

     ​	隐式建模采取了一种不同的方法，不是==明确地定义==模型从输入到输出的具体计算过程，而是通过对模型==施加特定的约束==，确保这些约束得以满足，以实现预期的结果。
     ​	$首先是一个简单的例子，x ∈ X,z ∈ Z, z = h(x)$，为了隐式的表达这个函数，定义一个新的函数 $g：X*Z，且 g(x,z)=h(x)-z,$  $那么我们的目标就变为了求 g(x,z) = 0 的解$，虽然这个例子展示的是代数方程，但这些方法可以扩展到不动点方程，从而发展DEQ(deep equilibrium models)算法。

     ​	一个固定点方程，设 $z = f_θ(z)，θ是一个参数,z依赖θ$，设这个方程在 $T_{conv}$ 个时间步后收敛，此时  $z_{T_{conv}} = z_{T_{conv}+1}$ , 相应的，可以得到对应的等式 $g_θ(z) = f_θ(z)-z$，定义的损失函数将使用平衡时z值，$z_{T_{conv}} = z^*$ ,使用DEQ算法中得到的结果，可以得到：

     <img src="./assets/image-20240919082839142.png" alt="image-20240919082839142" style="zoom:33%;" />

     ==这里自己推导一下该过程：== 

      $ \frac{\partial L(z^*)}{\partial θ}  = \frac{\partial L(z^*)}{\partial z^*} \times \frac{d z^*}{d θ}$， 接下来就是求 $\frac{d z^*}{d θ}$​

     平衡状态下，我们可以将 $z^*$ 视作是 $θ$ 的函数，对该式 $z^*=f_θ(z^*)$ 进行求导 ：

     $\frac {dz^*}{dθ} = \frac {\partial f}{\partial z^*} \times \frac{dz^*}{dθ} +  \frac {\partial f}{\partial θ}$

     变换形式可以得到：

     $\frac{dz^*}{dθ} = (I-f^{\prime}(z^*))^{-1}  \times \frac {\partial f}{\partial θ}$      **表达1**

     当$ z = z^*$, $g_θ(z^*) = f_θ(z^*)-z^*$, 此时$z^*$ 为常数，

     方程两边对$z^*$求导，可以得到 $\frac{\partial g}{\partial z^*} = \frac{\partial f}{\partial z^*} - 1$，等式两边同时加个负号，就可以替换**表达1**

     所以理论上g和f可以互换，后续还需要看原论文的所有推导方式。==DEQ==

     ---------------------

     ----------------------------------

     # ==知识补充==

     ### 固定点方程

     固定点方程描述了系统在某一特定状态下保持不变的点。
     固定点方程通过将差分方程中的自变量设为常数，并令方程左右两边相等而得到的。
     求解固定点方程可以得到一个或多个平衡点，这些平衡点对于理解系统的动态行为和稳定性至关重要。
     
     在差分方程的稳定性分析中，平衡点的稳定性是一个核心问题。
     判断平衡点稳定性的主要方法是利用线性化技术，即在平衡点附近对差分方程进行线性化处理，然后分析线性化方程的特征根。==根据特征根的性质，可以判断平衡点的稳定性：如果特征根的模小于1，则平衡点是稳定的；如果特征根的模大于1，则平衡点是不稳定的；如果存在特征根的模等于1，则平衡点处于临界稳定状态==。
     
     ### 深度平衡模型DEQ

     深度平衡模型（Deep Equilibrium Models，简称DEQ）是一种创新的深度学习架构，它引入了一种新颖的==隐式深度架构==，通过直接求解和反向传播无限深网络的固定点平衡状态，彻底改变了我们对深度学习的理解。
     DEQ模型的核心在于其隐式深度特性，它利用固定点求解器（如Anderson加速和Broyden方法）来迭代求解网络的平衡状态，这种设计不仅简化了网络结构，还通过雅可比正则化等技术增强了模型的稳定性。
     
     DEQ模型具有以下几个显著特点：

     1. **无尽深度**：DEQ模型以等价于无限层网络的平衡态进行计算，无需实际堆叠大量层，从而实现了理论上的无限深度。
  2. **高效内存管理**：DEQ模型在保持高性能的同时，仅需O(1)的内存，显著减少了对硬件资源的需求。
     3. **兼容现代结构化层**：DEQ模型支持Transformer等现代结构化层，使得其在自然语言处理（NLP）和计算机视觉（CV）等任务中都能达到最先进的性能。
     4. **稳定性工具**：DEQ模型提供了雅可比正则化等工具，帮助稳定隐式模型的训练过程。
     5. **易于扩展**：DEQ模型的项目结构清晰，代码简化，便于用户根据自己的需求进行扩展和定制。
     
     -----------------------------------------------------------------

     --------------------------------------------------------

  3. Architecture

     从生物学的角度来看，反馈连接存在于人类大脑中，并且在某些情况下即使是具有递归连接的浅层网络，其性能也与深层架构相当或更佳。

     整体架构和BERT类似，且添加了反馈机制，论文中说和视觉任务添加了反馈连接表现不一样，在语言任务上，反馈连接并没有显著提高性能，但仍然是一个可选项，并希望在未来任务中继续使用。

     <img src="./assets/image-20240918220304678.png" alt="image-20240918220304678" style="zoom: 33%;" />

     ​	$W_0$ 是嵌入层对输入进行的变换，最终输出向量$y ∈R^{N_s×D_{emb}}$。 长度x维度
     F是最后一个编码层返回的结果，作为权重反馈连接, b1是偏差，$s_{(N,out)}[t]$  是第N个脉冲编码层在前一个时间步中产生的脉冲，其他就是原始的LIF神经元。
  
     ​	接下来是其它层膜电位的变化，此时就没有反馈连接了：

     <img src="./assets/image-20240919102905535.png" alt="image-20240919102905535" style="zoom:33%;" />

     ​	第 $i$​​ 层的平均脉冲发放率为，$γ$ 是前面提到的泄露因子：

     ​								 $a_i[t] =  {\sum_{τ=1}^{t} γ^{t-τ} s_i[τ] \over \sum_{τ=1}^{t} γ^{t-τ}}$

     ​	从公式可以看出，前面的时间步由于$γ$ 的指数项比较大，所以比重比较小，而后面的时间步，比重比较大，相当于是对第t步的平均脉冲发放率应该是与它最近的几个时间步 连接权重比较大。

     <img src="./assets/image-20240919103948102.png" alt="image-20240919103948102" style="zoom: 33%;" />

     ==探寻一下这个公式的形成过程：==

     <img src="./assets/19939123f9e6c920f32cd0793ebf5f6-1726987305448-3.jpg" alt="19939123f9e6c920f32cd0793ebf5f6"  />

     $x[t](平均输入) → x^∗， a_i[t] → a_i^∗$， 在平衡状态下，平均发放率的关系如下：

     <img src="./assets/image-20240919104539128.png" alt="image-20240919104539128" style="zoom:33%;" />

     $σ函数就是将控制输出在[0,1]$  上面很容易，因为平衡状态下，膜电位的值，脉冲发放都成了固定值。

     

     <img src="./assets/image-20240919105842092.png" alt="image-20240919105842092" style="zoom:50%;" />

     对于脉冲注意力层，在平衡状态下的替代稳态等式为：

     <img src="./assets/image-20240922150250796.png" alt="image-20240922150250796" style="zoom:33%;" />

     第一层稳态下加了反馈连接的输出：

     <img src="./assets/image-20240922152409168.png" alt="image-20240922152409168" style="zoom:33%;" />

     ==整个求导过程都使用的是ASR，也就是没有在时间步上进行反向传播，这就大大减小了反向传播时的计算机内存和功耗，故即使此处使用的时间步很大，也不会增加多少资源消耗。==

     

  4. Spiking Attention Mechanism

     本文提出的脉冲自注意力机制，这里的 $x 等同于 原始自注意力机制的Q$：

     <img src="./assets/image-20240919111817014.png" alt="image-20240919111817014" style="zoom: 33%;" />

     qkv都是输入的值与权重相乘得到的，$π$是softmax函数,s是缩放因子。
     参考系统框架图，里面显示的只有Q矩阵是float值，其余矩阵都是脉冲值，而在原始的Attention中都是float值，自然计算复杂度就降低了很多。原始的的复杂度为$O(n^3)$, 且为乘法和加法的操作，在这个脉冲注意力中，复杂度也为$O(n^3)$,但只有加法操作。
  
  5. ANN-SNN KD using Equilibrium States

     这篇文章的蒸馏框架：   使用学生SNN中间层的稳态ASR去逼近ANN中间层的激活值。

     <img src="./assets/image-20240919113656581.png" alt="image-20240919113656581" style="zoom:33%;" />

     蒸馏过程中使用到的损失函数如下：

     <img src="./assets/image-20240919114002195.png" alt="image-20240919114002195" style="zoom:33%;" />

     $W_{T_d}$​ 是一个将学生模型的维度映射为和教师模型一致的变换。

     教师模型的编码层数量是比学生模型的数量多的，就需要进行选择性的学习，将学生模型的编码成对应到教师层的某些层上。

     <img src="./assets/image-20240919114602631.png" alt="image-20240919114602631" style="zoom:33%;" />

     这儿训练的时候，同样使用的是公式3，把里面的 $z^*替换为a^*$。

     <img src="./assets/image-20240919082839142.png" alt="image-20240919082839142" style="zoom:33%;" />

     中间层蒸馏完之后，当然对预测层也需要蒸馏，$c是一个线性映射，t'就是一个调控参数$。

     <img src="./assets/image-20240919115130339.png" alt="image-20240919115130339" style="zoom: 33%;" />

     ==自己推断：以此类推，其实也可以增加对网络第一层出来之后的蒸馏，就是数据编码部分==
     
     蒸馏分两部分进行，第一部分是对ANN大模型使用通用领域的知识而不是专门的任务进行预训练，第二部分是对ANN具体任务上进行微调，这个过程中SNN跟着一起训练。通过这个过程，大大减小SNN尺寸。
     ==参考文献： Tinybert: Distilling  bert for natural language understanding==
  
* **试验：** 6个分类任务，1个回归任务。    ==internal layer KD（IKD）==

* <img src="./assets/image-20240922162119249.png" alt="image-20240922162119249" style="zoom:33%;" />

  ​	中间提到了一个SpinkingGPT，这个网络在参数为45M的时候在SST-2数据集上实现了80.39的准确率，而在参数量为216M的情况下，实现了88.76%的准确率，但是本文设计的网络参数量为50M，实现了88.19%的准确率，其实查看下面这个结果，不难发现，这篇文章提出的网络之所以能够所有数据集上都有不错甚至最有的效果，主要原因应该是使用了==通用数据的预训练==，所以之后如果考虑这个方向，可以看看==如何改进现有的比较统一的知识蒸馏的方法==。  知识蒸馏的作用应该主要是进一步提高准确率吧

  ![image-20240919115434145](./assets/image-20240919115434145.png)

* **Analysis of Power & Energy Efficiency：**  电力与能源效率分析

  计算操作数使用到了这个公式：

  $Norm\#OPS = {\sum_{i} IFR_i \times Layer\#OPS_{i+1} \over \sum{Layer\#OPS}}$
  $IFR_i是第i层所有神经元在多个时间步上的脉冲数的平均数。$ 
  $对于ANN来说，所有神经元都会被激活，所以Norm\#OPS=1$

  $根据先前的理论，一个ACC(SNN)操作消耗能量是0.9pj，一个MAC(ANN)的操作消耗能量是4.6pj，
  是ACC的5.1倍。$

  $可以得到ANN/SNN的能量效率比为 ： e = {1 \over {{1 \over 5.1} \times Norm\#OPS}},也就是 e = ({{1 \over 5.1} \times Norm\#OPS})^{-1}$

  <img src="./assets/image-20240922165857544.png" alt="image-20240922165857544" style="zoom: 50%;" />

  ​	图b显示了增阈值增加，可以减小脉冲平均发放率，但是更难收敛，所以会带来时间步的增加，这也就是最终选择阈值为1，时间步为16。当然最有的是80个时间步。文中还提到，减小阈值，可以大大降低瞬时功耗，因此适用于边缘计算。

* **讨论与总结：**

​	借鉴人脑的惊人复杂性，这种复杂性超越了任何现有的语言模型，我们有机会利用这些见解，构建出不仅能够模拟生物学上合理的行为，还能通过最低的能量消耗提供高效解决方案的模型。在本文中，我们提出了一种脉冲语言模型（spiking LM），并在GLUE基准测试中进行了多任务评估。我们引入了脉冲注意机制，利用稳态收敛，提出了一种新颖的基于人工神经网络（ANN）和脉冲神经网络（SNN）的知识蒸馏方法，以实现更快和更高效的学习，同时通过隐式微分来探索脉冲语言模型的训练，从而克服影响SNN模型训练的多种问题。在洛希2（Loihi 2）等神经形态硬件上实施我们的模型进行推理，将有助于开发出一种低功耗的解决方案，潜在地可在边缘设备上应用。未来，我们还可以进一步扩展这种方法，以设计其他脉冲语言模型，如GPT等。不过，目前所提出的脉冲语言模型与基于BERT的微调模型之间仍存在一定的性能差距。我们可以通过深入研究多样的脉冲神经元模型、考察时间编码方案以及引入渐变脉冲等策略，努力缩小这一差距。



# 5. Self-Distillation Learning Based on Temporal-Spatial Consistency for Spiking Neural Networks

基于时空一致性的自蒸馏学习SNN

电子科技大学信息与软件工程学院  [左琳教授](https://sise.uestc.edu.cn/info/1035/5649.htm)   ,主要从事人工智能、神经网络等方面的研究。

* **摘要：**  

  ==背景：==Spiking neural networks (SNNs) 因其事件驱动、低功耗特性和高度生物可解释性而备受关注。受到知识蒸馏（knowledge distillation， KD）启发，近期研究通过使用预训练的教师模型提升了SNN模型的性能。然而，**额外的教师模型需要大量计算资源，并且手动定义合适的教师网络架构非常繁琐。**

  ==方法：== **本文探索了SNN的具有成本效益的自蒸馏学习，以规避这些问题。在没有明确定义的教师模型的情况下，SNN在训练过程中生成伪标签并学习一致性**。一方面，我们在训练过程中延长SNN的时间步长，以创建一个隐式的时间“教师”，引导原始“学生”的学习，即**时间自蒸馏**。另一方面，我们通过SNN的最终输出引导弱分类器在中间阶段的输出，即**空间自蒸馏**。我们的时间-空间自蒸馏（TSSD）学习方法不会引入任何推断开销，并且具有出色的泛化能力。

  ==结果：== 在静态图像数据集CIFAR10/100和ImageNet以及神经形态数据集CIFAR10-DVS和DVS-Gesture上进行的广泛实验验证了TSSD方法的优越性能。本文提出了一种将SNN与KD融合的新方式，为高性能SNN学习方法提供了新的见解。

* **介绍：**

  <img src="./assets/image-20241111101947984.png" alt="image-20241111101947984" style="zoom:50%;" />

  ​	脉冲神经网络（SNNs）模拟生物神经系统的信息传递机制，通过离散的脉冲传递信息，相较于人工神经网络（ANNs），其功耗极低[1]。此外，脉冲神经元固有的时间特性使得SNNs具备卓越的时间特征提取能力，得到了研究界的广泛关注[2; 3]。尽管SNNs显示出巨大的潜力，但其训练一直受到脉冲活动不可微分性的困扰。为了获得高性能的SNNs，一些研究预先训练一个ANN，然后将其转换为具有相同结构的SNN[4; 5]。然而，这一做法损害了SNNs的时间特征提取能力，导致显著的推理延迟[6; 7]。另一种可行的训练方法是替代梯度学习。在反向传播过程中，将不可微分脉冲活动的梯度替换为平滑的替代梯度，从而允许使用梯度下降法对SNNs进行训练[8; 9]。这种做法即使在低延迟下也能实现令人满意的性能，成为最常用的训练方法[10; 11; 12]。我们的研究遵循这一线索，进一步改善SNN的性能。

  ​	**最近，一些研究将知识蒸馏（KD）引入现有的SNN训练方法，通常使用一个表现更佳的大型模型来指导轻量模型的学习，以提升SNNs的性能。**如图1所示，这些蒸馏方法可分为两类：（1）需要额外的教师模型来指导学生SNN模型的训练[13; 14; 2; 15; 16; 17; 18]；（2）在没有显式教师模型的情况下，手动定义的教师或学生SNN模型自行生成指导信号[12; 19]。对于第一类，教师模型会带来额外的训练时间和内存开销。此外，为了获得令人满意的性能，教师模型通常是一个ANN[2; 17; 18]，这并未改善学生SNN的时间特征提取能力。因此，教师模型的架构必须根据任务和学生SNN手动定义。**与此相比，对于第二类，文献[19]使用预定义的固定教师信号，而文献[12]则利用正确时间步的输出指导其余时间步，显著减少了资源消耗。然而，文献[19]中的固定指导信号缺乏灵活性，而文献[12]在非常低的时间步（例如，1）时不可用。对于SNNs，仍需进一步探索更高效、有效的蒸馏学习策略**。

  ​	在本文中，从脉冲神经网络（SNN）的固有时空特性出发，**提出了一种针对SNN的时间-空间自蒸馏（TSSD）学习方法**。通过扩展训练时间步，TSSD将具有扩展时间步的SNN视为指导原始小时间步“学生”学习的“教师”。“教师”与“学生”共享相同的架构和参数，没有额外的内存或计算开销，并且在训练过程中持续优化，以为“学生”提供动态指导。**此外，这种时间蒸馏将训练和推理的时间步解耦，使得在非常低的时间步上仍能获得令人满意的推理性能**。另一方面，在训练期间，TSSD通过在SNN的中间阶段添加一个弱分类器来执行空间自蒸馏。弱分类器基于在中间阶段提取的特征进行预测，并由SNN的最终输出进行指导。这促使SNN的早期阶段提取与整个网络一致的特征，从而增强SNN的特征提取能力。训练完成后，弱分类器被丢弃，因此不会影响推理效率。**此外，我们的TSSD方法与现有的其他方法（如各种替代梯度、SNN架构和脉冲神经元模型）是正交的，具有优越的泛化能力。**为了评估TSSD方法的性能，我们在静态和类脑数据集上进行了广泛的实验。主要贡献可以总结如下：

  - 我们提出了TSSD学习方法，探索从SNN的固有时空角度进行高效自蒸馏，在不增加推理开销的情况下提升性能。
  - T**SSD方法与其他现有方法（如替代梯度、网络架构和脉冲神经元）是正交的，能够无缝集成，提供优越的泛化能力。**
  - 在静态和类脑数据集上的大规模实验验证了TSSD方法的性能和泛化能力。

* **相关工作：**

  1. Surrogate Gradient Learning in SNNs. 代理梯度基础的学习方法在反向传播过程中利用预定义的平滑代理梯度，从而避免了不可微分的脉冲活动问题 [8; 20]。基于代理梯度的深度脉冲神经网络（SNN）的训练已经有大量研究工作，包括高效训练方法 [21; 22; 23] 和归一化方法 [24; 25; 26]。一些研究工作设计了新的脉冲神经网络结构，并基于代理梯度进行了训练 [10; 27; 28]。基于代理梯度的研究还在探索更高效且生物学上更一致的脉冲神经元模型 [29; 30; 31; 32]。我们提出的TSSD方法基于代理梯度方法，且与特定网络结构、脉冲神经元模型和代理梯度函数解耦，具有优越的通用性。
  2. Knowledge Distillation. 知识蒸馏（KD）定义了一个繁琐的教师模型，并使用它来指导轻量级学生模型的训练 [33]。根据指导信息，知识蒸馏可以分为logit 蒸馏 [34; 35; 36; 37; 38] 和特征蒸馏 [39; 40; 41; 42]。logit蒸馏指导学生生成与教师相似的输出logit，而特征蒸馏则鼓励学生提取与教师相似的中间特征图。这两种蒸馏方法都需要额外的教师模型，而一些自蒸馏方法已在没有显式教师情况下取得了可比的结果 [36; 43; 44]。我们探索的自蒸馏学习在脉冲神经网络中提高性能的同时减少了蒸馏开销。
  3. Distillation Learning in SNNs. [13] 和 [2] 通过预训练一个人工神经网络（ANN）来指导SNN的训练，从而提高了学生SNN的性能。[14] 和 [45] 首先蒸馏一个ANN教师，然后将蒸馏后的ANN转换为SNN。[18] 和 [17] 跟随这一思路并与共享的ANN优化SNN。[16] 和 [15] 结合这一蒸馏机制来利用SNN进行自然语言处理，实现了出色的任务性能。然而，这些方法需要对教师模型进行预训练，这增加了额外的时间和内存开销。另一个显著的缺点是，ANN教师限制了SNN学生的时间特征提取能力，因此只能处理静态数据 [2; 18]。[19] 通过手动定义教师信号避免了额外的训练开销，但其 次优的性能 仍有改进的空间。[12] 根据生成预测的正确性将时间步划分为两部分，并用正确的输出指导不正确的预测，取得了显著的性能。然而，[12] 在极低的时间步（如1）下无法工作，此时只有单个预测。与这些方法相比，我们的方法不需要对教师进行预训练，仅需稍大的时间步和一个额外的弱分类器来训练SNN。此外，训练后的SNN具有优越的时空特征提取能力，能够在超低延迟下进行推断。

* **方法：**

  1. Spiking Neuron model 脉冲神经元模型  
     使用LIF神经元

  2. Temporal Self-Distillation  时间自蒸馏

     <img src="./assets/image-20241111105206707.png" alt="image-20241111105206707" style="zoom: 67%;" />

      	尖峰神经元在多个时间步T中迭代累积膜电位、发放尖峰并重置膜电位。**在一定范围内，时间步T越大，SNN的性能越好**。受到此启发，我们在训练过程中扩展SNN的时间维度，较大的时间步导致更好的性能，从而产生一个“教师”。在时间扩展之前的SNN作为“学生”，并在训练过程中通过“教师”进行时间自蒸馏（TSD）学习，如图2所示。接下来，我们将详细描述这种TSD学习方法。

     ​	设f(θ, T)表示一个SNN，其中θ是网络参数，T是仿真时间步。训练SNN使得f(x; θ, T) = y，其中x是输入数据，y是任务的目标（对于分类任务，y是类标签）。对于输入x，f(θ, T)在所有T时间步生成输出，最终决策$f(x; θ, T) =\frac{\sum_{t=1}^T f(x;θ)}{T} $ 被视为T时间步的平均输出。训练过程可以表述如下：

     <img src="./assets/image-20241111111158634.png" alt="image-20241111111158634" style="zoom:50%;" />

     D是训练集，使用的是交叉熵损失。

     ​	为了泛化，我们用 Ts < Tt 表示两个不同的时间步。因此，对于相同的参数 θ，我们得到两个逻辑上不同的脉冲神经网络（SNNs），它们具有不同的时间步：f(θ, Ts) 和 f(θ, Tt)。与 f(θ, Ts) 相比，f(θ, Tt) 使用更大的时间步，因此应该产生更好的性能。因此，我们将 f(θ, Tt) 作为“教师”来指导 f(θ, Ts) 这个“学生”模型进行时间步大小（TSD）学习。这促使“学生”模仿“教师”对相同输入产生的预测，从而做出更一致的决策并提高“学生”模型的整体性能。这种由 TSD 指导的学习过程可以表述为：

     <img src="./assets/image-20241111111953576.png" alt="image-20241111111953576" style="zoom:50%;" />

     使用L2损失函数：<img src="./assets/image-20241111112127437.png" alt="image-20241111112127437" style="zoom:50%;" />

     ​	值得注意的是，“教师”模型与“学生”模型共享相同的参数 θ。因此，不需训练额外的教师模型，只需在训练过程中将时间步从 Ts 扩展到 Tt，而 Ts 时间步仍用于推理。这使得训练和推理的时间步解耦，使得以超低延迟（例如，1 的时间步）进行推理成为可能。**Tt 和 Ts 的设置应考虑训练和推理期间的延迟与性能的权衡，我们将在第 4 节中进行探讨。**另一个显著的优势是，“教师”和“学生”都是脉冲神经网络（SNNs），因此“学生”模型的时间特征提取能力得到了增强，与 [2; 18] 中的人工神经网络（ANN）教师相比，后者导致脉冲神经网络学生的时间特征提取能力丧失。结合公式 5 和公式 6，基于时间步大小（TSD）的脉冲神经网络训练的总损失可以表述为：

     <img src="./assets/image-20241111112152596.png" alt="image-20241111112152596" style="zoom:50%;" />

     $α$ 是控制TSD权重的系数。

  3. Spatial Self-Distillation 空间自蒸馏

     ​	除了SNN特有的时间属性外，深度神经网络的一个共同特征——空间属性也被用来探索人工神经网络中的蒸馏学习 [44]。在这里，我们探索脉冲神经网络中的空间自蒸馏（SSD）学习，以实现更优的蒸馏性能。通常，一个神经网络由多个层或块压堆叠加在一起，例如VGG [46]和ResNet [47]架构。**前面的块提取主要特征，而后面的块提取用于决策的高级抽象特征。**考虑到这一点，我们在训练过程中将一个弱分类器插入到脉冲神经网络的中间阶段，以基于提取的主要特征进行预测。与完整的SNN相比，弱分类器被视为一个较小的子网络，因此弱分类器和完整的SNN可以看作是一个弱“学生”和一个强“教师”。我们的SSD方法鼓励弱分类器尽可能学习与完整SNN一致的输出，从而有助于前面阶段的特征学习能力。SSD学习如图2所示，具体阐述如下。在不失一般性的情况下，假设位于SNN中间的弱分类器 $C$ 将SNN分为两部分，f(θ) = f(θ1 ◦ θ2)。在Ts时间步内，弱分类器基于每个时间步的f(θ1)生成输出f(θ1 ◦ C)，这些输出被引导与完整的SNN平均输出对齐：

     <img src="./assets/image-20241111112422352.png" alt="image-20241111112422352" style="zoom:50%;" />

     ​	其中 ft (x; θ1 ◦ C) 表示弱分类器在时间步 t 的输出，而 f (x; θ, Ts) 表示完整 SNN 在 Ts 时间步上的平均输出。这类似于 [44] 中的做法，在人工神经网络中插入多个额外的瓶颈进行蒸馏，但我们进一步考虑了 SNN 的多时间步输出特性。这使得完整 SNN 的稳定平均输出能够指导弱分类器不稳定的逐时间步输出，从而进一步促进前面阶段的学习。仍然使用 L2 距离作为 SSD 的损失。这鼓励“学生”和“教师”的 logit 匹配，并消除了在常规 KL 散度损失 [48] 中繁琐的蒸馏温度(之前的ANN蒸馏都用到了温度因子这个参数，且这个参数需要手动调整。)调整需求。使用 SSD 训练 SNN 的总损失可以表述为：

     <img src="./assets/image-20241111113234828.png" alt="image-20241111113234828" style="zoom:50%;" />

     ​	在本文中，弱分类器由卷积、BN、LIF神经元和全连接层组成，与完整的SNN相比，只需占用微不足道的开销。需要注意的是，弱分类器在推理阶段不被使用，因此不会影响推理效率。或者，如果在推理过程中使用弱分类器识别简单样本，它可以实现空间提前退出 ，并缩短前向传播路径，从而进一步提高推理效率，我们在A.5中进行了探讨。

     ==附录A.5 Weak Classifier Prediction Accelerated Inference==

     ​	在不影响SNN推理效率的情况下，我们在训练后丢弃了弱分类器。然而，如果不丢弃弱分类器而是用于预测，则有可能进一步提高推理效率。为此，我们调查了弱分类器的分类准确率，希望通过仅使用弱分类器识别简单样本来加快推理过程。CIFAR10-DVS、DVS-Gesture和CIFAR100上的实验结果如表8所示。可以看出，弱分类器的识别性能与完整SNN相比略有下降，但仍能够保持令人满意的识别准确率（特别是对于DVS-Gesture，识别准确率仅降低了2.78%，但空间前向传播路径减半）。通过进一步优化弱分类器或在监督信号上进行训练，可以提高其识别能力。这使得在推理过程中仅使用弱分类器识别简单样本成为可能，避免将特征传递到最终分类层，从而显著减少推理延迟。
     <img src="./assets/image-20241111132720537.png" alt="image-20241111132720537" style="zoom:50%;" />

     从图2以及第四小节的算法流程图来看，这里的完整SNN就是教师模型，然后弱分类器就是前 $T_s$ 个时间步下每个时间步交叉熵结果的平均（作者没有细讲，我感觉就是把交叉熵损失的结果全部相加再求平均）

  4. Temporal-Spatial Self-Distillation Learning  时空自蒸馏学习

     <img src="./assets/image-20241111134021055.png" alt="image-20241111134021055" style="zoom:67%;" />

     

     <img src="./assets/image-20241111132820464.png" alt="image-20241111132820464" style="zoom: 67%;" />

  5. Theoretical Analysis 理论分析

     ​	根据[51]的研究，**对于判别预测器和非确定性标签**，贝叶斯蒸馏风险 $$\hat{R_*}(f; S) $$的方差远低于正常风险$$\hat{R}(f; S) $$。在我们的TSSD方法中，**教师在时间和空间维度上的输出可以被视为非确定性的软标签。**因此，我们的学生SNN模型学习这些非确定性的标签，能够获得较低的贝叶斯蒸馏风险，即更低的方差和更高的性能。从另一个角度来看，**教师的输出相对更稳定且方差低于学生，这有助于学生学习表示信息，这与[52]的观点一致。**

     [51] Aditya K Menon, Ankit Singh Rawat, Sashank Reddi, Seungyeon Kim, and Sanjiv Kumar. A statistical perspective on distillation. In Marina Meila and Tong Zhang, editors, Proceedings ofthe 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 7632–7642. PMLR, 18–24 Jul 2021.

     [52] XiaoWang, Haoqi Fan, Yuandong Tian, Daisuke Kihara, and Xinlei Chen. On the importance of asymmetry for siamese representation learning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16570–16579, June 2022.

     

* **试验：**

  1. Implementation Details

     ​	我们在静态CIFAR10/100 [53]、ImageNet [54]以及神经形态数据集CIFAR10-DVS [55]和DVS-Gesture [56]上评估了我们的方法。在实验中使用了两种不同类型的网络架构，VGG和ResNet。我们报告了三次实验的平均准确率和标准差。详细的实验设置可以在附录A.1中找到。

     ==附录A.1 Details of Experiments==

     * Dataset
       	==CIFAR10/100==：CIFAR10/100 [53]包含10种/100种不同类别的物体图像。该数据集包含60,000张大小为32 × 32的图像，其中50,000张用于训练，10,000张用于测试。对于CIFAR10/100，我们应用了**随机水平翻转和随机裁剪，而没有进行其他的数据增强**。
       	==ImageNet== [54]：ImageNet是一个具有挑战性的图像识别数据集，包含128万张训练图像和5万张测试图像，来自1000个物体类别。我们将其中的图像大小调整为224 × 224，并应用了**标准的数据增强处理**，如文献[62]所述。
       	[62] Man Yao, Jiakui Hu, Zhaokun Zhou, Li Yuan, Yonghong Tian, Bo Xu, and Guoqi Li. Spike-driven
       transformer. Advances in Neural Information Processing Systems, 36, 2023.
       	==CIFAR10-DVS==：CIFAR10-DVS [55]是CIFAR10的神经形态版本。CIFAR10-DVS中有10,000个事件流样本，空间大小为128 × 128，包含10个类别。每个事件流x ∈ [t, x, y, p]表示在时刻t相对于前一时刻位于[x, y]位置的像素值或亮度的变化。p表示极性，正极性表示像素值或亮度的增加，反之亦然。对于CIFAR10-DVS数据的预处理，我们采用了与文献[30]相同的方法。原始事件流被分成多个以**10毫秒**为增量的切片，每个切片被整合为一个帧，**并降采样到42 × 42**。CIFAR10-DVS按9:1的比例划分为训练集和测试集。
       	[30] Lang Feng, Qianhui Liu, Huajin Tang, De Ma, and Gang Pan. Multi-level firing with spiking ds-resnet:
       Enabling better and deeper directly-trained spiking neural networks. In Proceedings ofthe Thirty-First
       International Joint Conference on Artificial Intelligence, pages 2471–2477, 7 2022.
       	==DVS-Gesture==：DVS-Gesture [56]是一个用于手势识别的神经形态数据集。DVS-Gesture总共包含11个手势的事件流样本，其中1176个用于训练，288个用于测试，每个样本的空间大小为128 × 128。对于DVS-Gesture数据，我们将事件流按**30毫秒**的增量整合为帧，**并降采样到32 × 32。**

     * Implementation Details

       ​	实验是在PyTorch软件包下进行的。所有模型均在NVIDIA TITAN RTX（2018年发布的介于专业卡和游戏卡之间的显卡，相比游戏卡来说，显存容量较大，且为满血显存，实现各层级的割韭菜）上运行，训练了100个周期。初始学习率设置为0.1，并在每30个周期后衰减为前一个学习率的十分之一。批量大小设置为64。**使用带动量的随机梯度下降优化器，动量设置为0.9。静态CIFAR10/100和神经形态数据集的权重衰减分别设置为1e-4和1e-3。对于LIF神经元，膜电位时间常数τ设置为2.0，阈值ϑ设置为1.0。**

       ​	对于ImageNet数据集，我们遵循文献[62]的训练策略。使用Lamb优化器训练300个周期，初始学习率为0.0005，并使用余弦衰减学习率进行调整（为了减少训练时间，我们仅训练前150个周期）。批量大小设置为32。

     * Network Structures

       ​	在我们的实验中，我们使用了VGG-9和ResNet-18架构来验证我们的方法。这两个网络的具体结构和阶段划分的方式如表6所示。对于ResNet-18，原来在每个残差块中的加法操作后的脉冲神经元被移动到加法操作之前，这与文献[30]中的做法相同。**（我还以为他要说这与Sew-ResNet网络相似呢）**
       ​	[30] Lang Feng, Qianhui Liu, Huajin Tang, De Ma, and Gang Pan. Multi-level firing with spiking ds-resnet:
       Enabling better and deeper directly-trained spiking neural networks. In Proceedings ofthe Thirty-First
       International Joint Conference on Artificial Intelligence, pages 2471–2477, 7 2022.

       <img src="./assets/image-20241111154440471.png" alt="image-20241111154440471" style="zoom:67%;" />

     * Details of Reproduction of Existing Methods

       ​	为了重现现有的方法，网络结构、超参数和训练方法与我们的TSSD相同，除非另有说明。
       ​	**teacher default-KD：**我们使用与我们的TSSD相同的架构、参数和训练策略实现了teacher default-KD [19]。对于默认教师信号，我们将对应于目标的类别设置为0.95，其余类别均为相等值。----==不是太理解==
       ​	[19] Qi Xu, Yaxin Li, Xuanye Fang, Jiangrong Shen, Jian K. Liu, Huajin Tang, and Gang Pan. Biologically inspired structure learning with reverse knowledge distillation for spiking neural networks, 2023.

       ​	**Spike-driven Transforme：** 为了重现Spike-driven Transformer [62]，我们直接使用论文的官方代码。为了避免数据差异带来的负面影响，我们在30毫秒的间隔下整合了脉冲驱动Transformer的DVS-Gesture输入，这与我们TSSD方法的输入相同。
       ​	**ASGL：**我们使用原始论文[64]中的训练策略重现ASGL方法。为了公平比较，我们使用A.1.1中描述的数据处理方法处理了神经形态数据集，并评估ASGL的性能。在DVS-Gesture上的实验与CIFAR10-DVS具有相同的训练策略，我们直接使用ASGL提供的代码，仅修改数据集。

       ​	**PLIF：**我们将PLIF [29]中LIF神经元的膜电位时间常数τ设置为一个可学习的参数，每一层神经元具有相同的τ。训练开始时初始τ值为2.0。
       ​	**MLF： 我们用三个水平的MLF [30]神经元替换了VGG-9网络中的LIF神经元，脉冲阈值分别为0.6、1.6和2.6，其余参数和结构保持不变。**
       ​	**TEBN：** 我们用TEBN [25]替换了VGG-9网络中的批量归一化层，其余参数和结构保持不变。
       ​	**Spikformer：**在重现Spikformer [10]时，我们采用与原始论文相同的方法，即对CIFAR10-DVS进行数据增强，使用大小为128 × 128的数据，直接输入Spikformer而不进行下采样。使用原始论文中的损失函数、学习率调整策略和批量大小。对于DVS-Gesture，不使用数据增强。在神经形态数据集上使用Spikformer进行总共106个周期的训练，配置与原始论文在CIFAR10-DVS上的配置相同。

  2. Ablation Study

     * Comparison with the baseline SNN.

       ​	我们通过将损失系数α和β都设置为1.0来探讨TSD和SSD学习的有效性。实验在CIFAR10和CIFAR10-DVS上进行，设置为Tt = 2Ts。实验结果如表1所示。可以看出，TSD和SSD都能够改善基线的性能，而TSSD最大化了性能的提升。此外，在静态和神经形态数据集上这种持续的性能提升表明，TSSD方法增强了脉冲神经网络（SNN）对空间和时间特征的提取能力，而这在使用额外的人工神经网络（ANN）教师时是难以实现的。为进一步比较，准确率变化曲线和脉冲发射频率图也在附录A.2和附录A.3中提供。

       <img src="./assets/image-20241111141753442.png" alt="image-20241111141753442" style="zoom: 50%;" />

       ==附录A.2 Accuracy Change Curves==

       ​	为了更好地说明所提方法与基线之间的差异，图5显示了VGG网络在训练过程中的准确率变化曲线。所提方法的收敛准确率始终高于基线，确认了我们方法的优越性能，这一结论与表1一致。值得注意的是，TSSD的准确率曲线比基线更稳定，这表明TSSD具有更稳定的训练状态，尤其是在CIFAR10-DVS上。

       <img src="./assets/image-20241111155949898.png" alt="image-20241111155949898" style="zoom:67%;" />

       ==附录A.3 Spike Firing Rate Attention Map==

       ​	基于DVS-Gesture的脉冲发放率的注意力图如图6所示（为了确保可视化分辨率，我们绘制了VGG-9中的第一个卷积块）。手势识别中最重要的区域是位于左下方的挥动手。在左下方的手部区域之外，普通SNN的注意力区域还覆盖了头部区域（上中部）和右下方。这些区域分散了普通SNN的注意力，因此导致其性能有限。相比之下，我们的TSSD更关注手部区域，具有更集中的注意力，从而产生更高的识别准确率。此外，与普通SNN的发放率（0.0942）相比，我们的TSSD的发放率略低（0.0933），在神经形态硬件上部署时能够实现更低的能耗。

       <img src="./assets/image-20241111160144268.png" alt="image-20241111160144268" style="zoom:67%;" />

       ==附录A.4 Comparative Results on ImageNet==

       ​	在表7中展示了在ImageNet上的比较结果。为了公正起见，我们在相同架构下将我们的方法与其他方法进行了比较。在使用2个时间步的情况下，我们的ResNet-34达到了66.13%的准确率，优于代理模块学习（Surrogate Module Learning，65.77%）[66]和SRP（64.32%）[59]，且具备相同的延迟。尽管SEW-ResNet（67.04%）[67]和对比学习（Contrastive，66.78%）[68]的性能更高，但它们的延迟是我们方法的两倍，并且SEW-ResNet传递整数信息，这降低了其能量优势。值得注意的是，我们的方法仅用了150个训练周期，耗时少于这些比较方法。随着训练的继续，预计TSSD将提供更好的性能。
       ==这里就有意思了，明明作者自己说因为想要节省时间，所以只训练了150个时间步，但能想到的是即使接着训练，使用本文方法的网络应该也不会增加多少，因为时间步仅有2的情况下，自然会限制学习能力，但是时间步一长，本文所提方法的优越性又没了，这一点我觉得审稿人一定会问。==

       <img src="./assets/image-20241111160622472.png" alt="image-20241111160622472" style="zoom:67%;" />

     * Influence of $T_t$​

       <img src="./assets/image-20241111141832875.png" alt="image-20241111141832875" style="zoom:67%;" />

       ​	我们研究了训练时间步长对性能的影响，因为Tt对时间“教师”模型有直接影响。对于CIFAR10，Tt的范围为[2,6]，Ts = 1；对于CIFAR10-DVS，Tt的范围为[6,15]，Ts = 5。如图3所示，对于固定的Ts而言，较大的Tt能提供更好的性能，尽管在CIFAR10-DVS上的波动略有不同。对于任何Tt的值，TSSD始终优于基础模型。即使当Tt仅比Ts大1时，TSSD的准确率分别比基线高出0.36%和4.16%。值得注意的是，当Tt = 6和Ts = 1时，CIFAR10上的准确率可以达到93.80%，这表明我们的方法能以超低延迟实现令人满意的性能。

       ​	此外，我们将训练时间步长$T_t$​的传统脉冲神经网络（SNNs）与我们在Ts时间步长推理下的TSSD方法进行比较。如表2所示，**当推理时间步长Ts小于训练时间步长Tt时，传统SNN的表现远不及我们的TSSD方法，特别是在CIFAR10上**。原因在于Tt和Ts会影响信息的分布，而这种分布差异会影响传统SNN的推理，导致当时间步长差异较大时性能较差。与[60]中的研究相呼应的另一个性能下降原因是，训练期间较大的时间步长导致代理梯度和真实梯度之间的不匹配问题更加严重。相比之下，我们的TSSD方法在训练过程中同时整合了Ts和Tt时间步长的信息，充分利用了较大的训练时间步长，同时避免了这些性能下降的风险，从而提高了SNN的推理性能。

       ==这样的比较本来就不公平，当然本文是在宣传自己所提方法在推理时的时间步长显著降低，因而无可厚非。==

       <img src="./assets/image-20241111142309695.png" alt="image-20241111142309695" style="zoom:67%;" />

       

     * Influence of loss coefficients.

       我们探讨了α和β对CIFAR10-DVS中TSSD方法的影响。我们将其中一个固定为1.0，另一个在0.5到1.5之间变化，结果如图4所示。实验结果表明，我们的TSSD方法对α和β不敏感，并且对于任何α和β的值，准确率均明显高于基线模型，尽管略有波动。

       <img src="./assets/image-20241111143305350.png" alt="image-20241111143305350" style="zoom: 50%;" />

  3. Comparison with Other Methods

     我们将TSSD与其他现有方法在静态和神经形态数据集上进行了比较。对于自我实现的方法，实施细节见附录A.1.4。

     * Static datasets.

       ​	对于静态数据集，我们设置Tt = 4，CIFAR10/100的比较结果如表3所示。在相同或更低的时间步长下，我们的方法表现优于参与比较的方法。特别是，即使只有1个时间步，我们的VGG-9在CIFAR100上实现了73.33%的平均测试准确率，这甚至超越了RMP-Loss [57]在10个时间步下的表现。ImageNet的比较结果如表7(附录中)所示。我们的ResNet-34在两个时间步下达到了66.13%的准确率，优于同等延迟下的其他比较方法。即便与TET [21]、MPBN [26]和RMP-Loss [57]相比，我们的模型在更低延迟下实现了更高的准确率。这证实了我们的方法在大规模挑战性数据集上的有效性。

       <img src="./assets/image-20241111144406824.png" alt="image-20241111144406824" style="zoom:50%;" />

       <img src="./assets/image-20241111144608242.png" alt="image-20241111144608242" style="zoom: 67%;" />

     * Neuromorphic datasets.

       ​	对于神经形态数据集，我们设置Tt = 2Ts，实验结果如表4所示。在CIFAR10-DVS上，我们的VGG-9在8个时间步下实现了78.70%的平均测试准确率，甚至超过了DSR [61]和TET [21]在20和10个时间步下的准确率。即使只有5个时间步，我们的方法在相同条件下也达到了72.90%的平均测试准确率，优于MLF [30]、TEBN [25]、teacher default-KD [19]和Spikformer [10]，甚至在10个时间步下超越了Real Spike [58]。在DVS-Gesture上，TSSD在16个时间步和仅5个时间步的低延迟下均优于比较模型。特别是，我们的VGG-9在16个时间步下的准确率比MLF [30]在40个时间步下高出0.16%。在神经形态数据集上的实验强有力地证实了我们的方法在提高SNNs的时间特征提取能力方面的有效性。

       <img src="./assets/image-20241111145009911.png" alt="image-20241111145009911" style="zoom: 67%;" />

  4. Further Extension and Generalization

     ​	在这里，我们进一步探讨了TSSD的扩展以及与现有方法的兼容性。通过在Tt个时间步内用最终输出来指导中间阶段的弱分类器，使TSSD方法得到了自然的扩展。我们在表5中评估了这一扩展的性能提升。实验结果表明，CIFAR10-DVS和DVS-Gesture上的平均测试准确率分别提高了0.43%和1.39%。此外，我们的TSSD方法可以与各种替代梯度函数 [63; 65]、更细粒度的脉冲神经元 [29; 32; 30] 以及专门为SNN设计的BN [24; 25; 26] 集成，这可以显著提高SNN的性能。例如，表5中展示了针对ASGL [64]、MLF [30]和TEBN [25]的TSSD扩展实验。这些方法在神经形态物体识别中的准确率得到了极大提升，证明了TSSD的通用性。我们将更多的扩展视为未来的工作。

     <img src="./assets/image-20241111145146450.png" alt="image-20241111145146450" style="zoom:67%;" />

* **讨论与总结：**

  ​	本文探讨了在脉冲神经网络（SNN）中进行自蒸馏学习，以缓解传统知识蒸馏（KD）所带来的高开销和低效率挑战，同时提升SNN的性能。为此，我们提出了TSSD学习方法，该方法从时间和空间两个方面对SNN进行自蒸馏，并引导SNN学习一致性。在静态和神经形态数据集上的大量实验显示了持续的性能提升，证明了我们方法的有效性。TSSD方法的一个局限性是它引入了额外的训练开销，但与其他需要重型教师模型的蒸馏方法相比，这一开销是适度的。此外，TSSD与其他SNN架构、脉冲神经元和替代梯度方法兼容，为扩展留出了广泛的空间。我们期望这项工作能为高性能和高效的SNN做出贡献。

  





​	从上面几篇论文中，不难发现，都在强调替代梯度存在梯度爆炸或消失或梯度不可导的问题，这种情况下有两种解决办法：一种是优化网络结构（SEW_ResNet）并使用替代梯度的方式，一种是网络结构不用怎么变但是去优化梯度替代的方式（使用DEQ，深度平衡网络），==同时后续改进知识蒸馏可以是一个方式。==



# 十二

# ==1==



* **摘要：**  
* **介绍：**
* **方法：**
* **试验：**
* **讨论与总结：**