# 第一条思考

SNN所涉及到的时间维度，是否真的是需要该维度。因为在我们目前的考量中，大脑依靠的是记忆，对于时间的发生从来都是随机的。

SNN的用处貌似并不是在一个任务上要达到多么高的准确率，而是在多个任务上都达到还不错的准确率。

SNN的时间维度应该是用来记忆的，搜寻相关内容。

SNN中的时间常数有可能是一个非常重要的参数，应该设置成跟随网络的深度而逐渐变化？就像学习率一样。

运用CNN中的残差结构时，应考虑到卷积之后的数据形式与脉冲形式是不一致的，如果使用残差需要转换卷积之后的数据形式。

通常SNN的学习算法可以分为有监督，无监督，基于奖励的学习以及 ANN to SNN 的四个方法。



当前大多数研究，最终的结果依据都是脉冲数量的大小，大脑内是否是这样的机制运行？

低精度ANN与SNN之间的联系，SNN精度低的原因更多应该还是因为时间步的问题，增加了时间步，导致误差的增加？



在SNN中使用最大池化的方式，相比平均池化可以得到更加明显的效果提升，这也与之前自己的两篇小论文工作相呼应——对于时间维度的数据，最大池化具有更好的性能。



大脑是否有电位的重置机制，为何LIF神经元会有这种 重置，一个神经元在发射一次脉冲后，电位应该也是下降的趋势而不是直接变为某个值，接着下降吧。



## 2024.07.24

脉冲网络是否更有利于网络泛化性的学习？？



spiking-resnet 给的学习率较低的话，会极大地影响训练速度。
在mnist数据集上进行测试，学习率为 1e-4 时，学习的较慢，且学习的上限到了94.82% (使用1e-4时，第一步的准确率也是非常低的，达到了16.8%)， 学习率为 1e-2时，学习的较快，最终测试集准确率为99.28%，注意这不是最高的，因为我们只训练了20轮。

sew-resnet 在设置与残差相连接的方式为 ADD时（和上述spiking-resnet残差连接一样），学习率设置为1e-2时，最终测试集准确率为99.18%， 设置为1e-4，最终测试集准确率为94.9%，这样学习的加快， 而当使用AND时，学习率要设置的大点，如 1e-2，最终测试集准确率为：99.24%，当学习率为1e-4时，最终测试集准确率为：94.74%
![image-20240724211352526](C:\Users\ALiang\AppData\Roaming\Typora\typora-user-images\image-20240724211352526.png)

这两个网络的区别为： spiking-resnet网络进行残差相加时，如果包含了下采样，就是两个自然数序列的相加，如果没有包含下采样，就是脉冲序列和自然数的相加。   而sew-resnet网络进行残差相加时，永远是两个脉冲序列相加。

------综合对比下来，发现学习率需要设置较大时，最终测试集准确率较高，且高了将近4.5个%点



## 2024.07.25

创建脉冲神经元时，切记每次都要创建一个新的，而不能连续使用一个已经创建好的，否则会报错，因为数据维度在网络中会发生变化。



## 2024.07.28

关于torch.fx函数的使用



# 2024.08.05

我们使用Python Tonic库[32]来便于加载N-MNIST和DVSGesture数据集。



# 2024.09.20

使用低精度ANN对SNN进行蒸馏，或者说如何并联学习，是否能突破时间步的限制？只用两步即可？
主要是SNN的精度比低精度量化的ANN还要低。

1bit量化神经网络的激活函数不是常见的sigmoid或者ReLU之类的，而是Sign（阈值比较，大于阈值，输出1，小于阈值，输出0）
低精度量化神经网络QNN。 （Low precision quantization neural network）

ReLU与IF神经元的有效叠加？

并行与剪枝的叠加使用。

1bit的二值化神经网络BNN的输入和IF模型下的SNN最大的差别在没有时间信息。因为SNN是一串编码，信息不止是编码0和1，还存在于脉冲时间序列中。按理来说SNN的脉冲编码蕴含比BNN更多的信息，准确度会高一些，但是实际上并没有什么优势，感觉SNN的训练还是有点问题。（==或者说IF神经元还可以优化？ 如何基于激活值对神经元公式进行优化？==）

二值化神经网络不是简单把输入量化到0/1，而是量化为-1/+1，只不过用0表示-1，用1表示+1。然后算一下+1×+1，+1×-1，-1×+1，-1×-1，结果再换成对应的0/1表示。然后就可以发现输入的01计算和输出的01表示其实可以就是个1bit的XNOR运算。

# 2024.10.11     openai o1   [参考链接](https://mp.weixin.qq.com/s/sPYeM5LbfAwyHUxbQ78Vsg)

1. 关于训练一个挑毛病的模型    [Finding GPT-4’s mistakes with GPT-4](https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/)
   这篇博客介绍了 CriticGPT——OpenAI 基于 GPT-4 训练的一个专门==给 ChatGPT 挑毛病的新模型==。它通过精准地分析 ChatGPT 的回答并提出建设性的批评，帮助人类训练师更准确地评估模型生成的代码，并识别其中的错误或潜在问题。据介绍，在 CriticGPT 的辅助下，人们审查 ChatGPT 代码的准确率提高了 60%。研究人员还发现，CriticGPT 在很多情况下比人类专家更擅长发现错误，它们甚至能在一些被认为是「完美无缺」的任务中找出问题，尽管这些任务大多数并不是代码任务，对 CriticGPT 来说有点超纲。

2. 关于推理 scaling law   [OpenAI’s Strawberry, LM self-talk, inference scaling laws, and spending more on inference](https://www.interconnects.ai/p/openai-strawberry-and-inference-scaling-laws)
   博客概述：在文章中，作者讨论了 OpenAI 的新方法「Strawberry」及推理 scaling law，强调了推理计算的投入对 AI 能力提升的重要性。作者指出，扩大推理计算比单纯扩大模型规模更有效，类似 AlphaGo 的推理技术能够显著提升模型表现。文章呼吁未来 AI 开发要更多关注推理技术。

3. 系统讨论o1     [Reverse engineering OpenAI’s o1](https://www.interconnects.ai/p/reverse-engineering-openai-o1)
   博客概述：这篇博客系统讨论了 OpenAI o1。o1 通过训练新模型处理长推理链，并使用大量强化学习来实现。与自回归语言模型不同，o1 在线为用户搜索答案，展示了新的 scaling law—— 推理 scaling law。博客还讨论了 o1 的一些技术细节，包括其如何使用强化学习进行训练，以及它在推理时的高成本。此外，博客还探讨了 o1 对未来 AI 领域的影响，包括它如何改变 AI 产品的部署堆栈和期望，以及它如何作为一个模型，通过不同的生成策略来实现复杂的任务。最后，博客提出了一些关于 o1 结构和功能的问题，并讨论了在开源领域复制这种系统所面临的挑战。作者还对 AI 未来的发展方向表示了期待，认为 AI 的进步将继续奖励那些敢于想象不可能很快变为可能的人。

4. 验证器的训练  [论文地址](https://arxiv.org/abs/2110.14168)
   论文概述：这篇论文发布于 2021 年 10 月。论文指出，尽管最先进的语言模型在很多任务上表现优异，但在处理多步骤数学推理时仍有困难。为了解决这个问题，作者创建了 GSM8K 数据集，包含 8500 个多样化的小学数学问题。研究发现，即使是大型 Transformer 模型也难以在这些任务上取得好成绩。为了提高性能，作者建议训练验证器来评估模型答案的正确性。通过在测试时生成多个答案并选择验证器评分最高的答案，这种方法显著提升了模型在 GSM8K 上的表现，并证明了这种方法比传统的微调方法更有效。

5. [Generative Language Modeling for Automated Theorem Proving](https://arxiv.org/abs/2009.03393)

   论文概述：这篇论文发布于 2020 年 9 月，Ilya Sutskever 是作者之一。论文探讨了基于 Transformer 的语言模型在自动定理证明中的应用。研究的动机是，自动定理证明器与人类相比的一个主要限制 —— 生成原创的数学术语 —— 可能可以通过语言模型的生成来解决。作者介绍了一个名为 GPT-f 的自动证明器和证明助手，用于 Metamath 形式化语言，并分析了其性能。GPT-f 发现了被 Metamath 主要库接受的新短证明，据作者所知，这是基于深度学习系统首次为形式数学社区贡献并被采纳的证明。

6. [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050)
   论文概述：这篇论文发布于 2023 年 5 月。论文探讨了大型语言模型在复杂多步推理任务中的表现及其可靠性问题。作者比较了两种训练方法：结果监督（outcome supervision）和过程监督（process supervision），前者仅对最终结果提供反馈，后者则对每个推理步骤提供反馈。研究发现，过程监督在训练模型解决 MATH 数据集中的问题时，显著优于结果监督。具体来说，采用过程监督的模型在 MATH 测试集的一个代表性子集中解决问题的成功率为 78%。此外，论文还展示了主动学习（active learning）在提高过程监督效率方面的重要性。为了支持相关研究，作者还发布了 PRM800K 数据集，这是一个包含 800,000 个步骤级人类反馈标签的完整数据集，用于训练他们的最佳奖励模型。

   由于包括 Ilya 在内的多位 o1 核心贡献者都参与了这篇论文，有人猜测这是 o1 模型训练的方法论。感兴趣的读者可以重点阅读。也可参阅机器之心的报道《[OpenAI 要为 GPT-4 解决数学问题了：奖励模型指错，解题水平达到新高度](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650879157&idx=1&sn=eac1b887a62133667d5987ea995b49fe&chksm=84e4f0cbb39379dd724f6e21344eb101980458c3dbcca5ce87f5794e24810731c31883fe31de&scene=21#wechat_redirect)》。

7. [LLM Critics Help Catch LLM Bugs](https://arxiv.org/abs/2407.00215)
   论文概述：这篇论文发布于 2024 年 6 月。论文介绍了一种通过训练「批评者」模型（即前面提到的 CriticGPT ）来提高人类评估机器学习模型输出的方法。这些批评者模型是大型语言模型，它们被训练来提供自然语言反馈，指出代码中的问题。研究表明，这些模型在识别代码错误方面比人类更有效，甚至能够发现人类审查者未发现的错误。尽管存在局限性，如可能产生误导的幻觉错误，但结合人类和机器的团队可以减少这种误导，同时保持错误检测的效率。

**使用一种新的transformer，也许不一定用三个矩阵，又或者增加矩阵的个数？那如何与生物神经机制相关联？**





# 2024.11.06     

现有SNN结构都是默认float32+脉冲(0,1)的形式，那么能否改为  {float16，int8} + 脉冲的形式

在SNN中使用标签平滑是否可能有一些效果，因为本身脉冲形式的序列较为稀疏，更容易在标签没有平滑时产生标签依赖的问题。



# 2024.11.09     

对自注意力改进，能否只用Q和K，不适用value，或者进行其他改进，减少复杂度？ Qkformer已经被做了出来。

对SNN残差形式的一种更高级改进，参考convnext，甚至全用3x3卷积，分组数为32。
