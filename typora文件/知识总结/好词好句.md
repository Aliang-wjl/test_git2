subtle link  

delicate interplay

hones in on  专注于 

as would occur in 就像发生在，正如在，这就类似于

This is a nonexhaustive list, and these codes are not necessarily independent of each other.

这并不是一个详尽无遗的列表，这些编码也不一定彼此独立。

When bright light ==is incident upon（投射）== our photoreceptor cells, the retina triggers a spike train to the visual cortex.

As a rudimentary example     作为一个基本示例

An alternative method 另一种方法

Temporal codes ==are marginally more difficult to== implement.   略微复杂的实现

A subtle challenge with using spike times is that the default implementation assumes each neuron spikes at least once, ==which is not necessarily the case.==  这种情况不一定成立

That is often all that matters.  这往往是最重要的。

The difficulty of learning scales with the number of weights,   学习的难度与权重的数量成正比。

These have not been ported to SNNs at the time of writing ==though someone is bound to step up to the mantle soon==.   很快就会有人站出来去做这个事情。

By drawing on some key tricks from ANNs    通过借鉴ANN当中的一些关键技巧

==Although to some extent==, the surrogate gradient can mitigate the spiking non-derivable, it is essentially a gradient training method that is less biologically plausible.
尽管在某种程度上，替代梯度可以缓解神经脉冲的不可导性，但从本质上来看，这仍然是一种梯度训练方法，生物学上并不太具备可信性。

==Emerged as== a biology-inspired method，  作为

For example, some work shows that SNNs can ==save orders of magnitude energy== over Artificial Neural Networks.                                                                   节省多个数量级的能量 

In order to alleviate the operational complexity of LLMs, we aim to ==draw motivation from== the brain.   从......汲取动力

orders of magnitude energy efficiency  数量级的能源效率

additional overheads  额外的开销。

LLMs like GPT3 has shown ==additional characteristics== such as ==emergent abilities== which can only be realized once the model size/compute increases above a certain threshold.
像GPT-3这样的语言模型展示了一些额外的特性，例如涌现能力，这些能力只有在模型规模或运算能力超过某个临界值时才会显现出来。

**emergent capability**  涌现能力    **emergent property**  涌现特性

Integrating the mechanism and knowledge embodied in LLMs into brain-inspired neural models ==hold immense promise== for creating a bio-plausible and energy-efficient solution.
将大语言模型中蕴含的机制和知识融入脑启发的神经模型中，==蕴藏着巨大的潜力==，可以为我们创造出一种生物可信且能源高效的解决方案。

The convergence of ASR of the neurons at equilibrium allows us to ==draw a close equivalence== between the ASR of the spiking attention layer and vanilla attention.
收敛到平衡状态的基于平均脉冲率的神经元，使得我们能够将脉冲注意力层的 ASR 与传统注意力机制之间==建立紧密的等价关系==。

Moreover, the feasibility of model training during distillation ==is enabled by== the previously discussed training method.   此外，之前提到的训练方法为在蒸馏过程中进行模型训练的可行性==提供了保障==。

We ==delve into== the theoretical and empirical foundations.   我们==深入探讨==理论和实证基础。

The general formulations of steady-state ASR equations, ==developed subsequently==, can be seamlessly applied to models involving both feedback connections and those without any feedback.       
相继开发的，之后开发的。

feedback did not improve performance ==considerably.==   反馈并没有显著地提升性能。

The model converges over $T_{conv}$ time steps during the “forward” phase ==to settle to== an equilibrium state.   

settle有定居的意思，这里翻译为达到

Some ideas are well accepted and commonly used among the ==neuromorphic engineering community==, while others are presented or justified for the first time here.

在神经形态工程界，有些想法被广泛接受并普遍使用，而有些想法则是首次在这里被提出或得到证实。

However, for all the state-of-the-art models designed every day, a Kaggle contest for state-of-the-art energy efficiency ==would go to the brain==, every time.
然而，尽管每天都会设计出最先进的模型，但每次Kaggle竞赛中，关于最先进的能源效率的奖项都会颁发给大脑（即最聪明、最具创新性的解决方案）。这里的“the brain”是一个比喻，指的是那些能够模仿大脑提出最具创新性和效率的解决方案的人或团队。

make sense of   理解

==As such== SNNs take advantage of low-precision parameters and high spatial and temporal sparsity. ==因此==

These models are designed with power-efficient execution on specialized neuromorphic hardware ==in mind==.
这些模型的设计考虑了在专门的类脑硬件上高效执行。

right down to  直到 
Should we replicate the genetic makeup of a neuron right down to the molecular level ?

 This article hones in on the intricacies of training brain-inspired neuromorphic algorithms.
本文深入探讨了以大脑为灵感的神经形态算法的训练细节

However, the brain-inspired nature of these emerging sensors, neuron models, and training methods is different enough to warrant a deep dive into biologically inspired neural networks.
然而，这些新兴传感器、神经元模型及训练方法的生物启发性特质足够不同，因此值得我们深入研究生物启发型神经网络。

In biosignal monitoring, nerve implants for the brain-machine or biosignal interfaces have to preprocess information ==locally== at minimum power and lack the bandwidths to transmit data for cloud computation.
在生物信号监测中，神经植入物用于大脑-机器/生物信号接口必须在最低功耗下本地预处理信息，且并无足够的带宽传输数据进行云端计算。

There are several persistent themes across these theories, ==which can be distilled down to== “the three S’s”.
==能够被归结为：==

==What actually matters== is the timing of the spike.  真正重要的是发放脉冲的时间。

A single action potential can be represented by ==a sparsely populated vector==.  一个稀疏分布的向量

invariance不变性    The spatiotemporal receptive fields of neurons promote excitable responses ==to== regions of spatial contrast (or edges) ==over== regions of spatial invariance.

without breaking a sweat 不费吹灰之力

Typical values of τ ==fall on the order of== 1–100 ms.     $τ$ 的典型值通常在1到100毫秒之间。

Relaxing the ==physically viable assumptions== made thus far, the coefficient of input current in (3),
放宽到迄今为止==物理学上所做的可行的假设==， 

Rather than drawing upon neuroscience, it is just as possible to start with primitives from deep learning and apply spiking thresholds.
与其依赖神经科学，我们也可以从深度学习的基本元素出发，运用脉冲阈值的方法。

The main takeaway is given as follows：use the neuron model that suits your task.  主要观点总结如下：

==As a rudimentary example==, a bright pixel is encoded into a highfrequency firing rate, whereas a dark pixel would result in low-frequency firing.   一个简单的例子。

Encoding input data into spikes can be thought of as how the sensory periphery transmits signals to the brain.
将输入数据编码成脉冲，可以看作是感官外周向大脑传递信号的一种方式。

We know that the reaction time of a human ==is roughly in the ballpark of 250 ms.==  大约在250ms左右。

This is one of those instances where a deep learning practitioner might be less concerned with what the brain does and prefers to focus on what is most useful.  

we still ==have much left to== understand about gradient backpropagation.  ==有很多事情要去做==

as per usual 像往常一样。

There is a common misconception that backprop and STDP-like learning rules ==are at odds with one other==.  
两者是矛盾的。

On the one hand, it is thought that STDP deserves more attention as it ==scales with== less complexity than backprop. 具有

==reverse-mode autodifferentiation==  反向自微分

to signal that 去通知，去表达   
Look for ways ==to signal that== you enjoy a person's company.  寻找方法表达你喜欢和某人在一起。

Hardware, especially GPUs, has been rapidly evolving in recent years. As a result, ==the optimal choices for many performance related trade-offs== have changed. 
硬件，尤其是GPU，近年来发展迅速。因此，==许多性能相关权衡的最佳选择==已经改变。

convex problems  凸问题

Multiple works have proposed ==heuristics== to solve this issue.  许多工作提出了==启发式算法==去解决这个问题
heuristic /hjuˈrɪstɪk/ algorithm   启发式算法

While in this paper we limit ourselves to methods that are sufficient for ==single machine training==, in which case a batch size no more than 2K often leads to good system efficiency. 
在本文中，我们将讨论适用于单机训练的方法，这种情况下，批量大小不超过2000通常能够达到良好的系统效率。

non-negligible   不可忽视

shoot two birds with one stone  一箭双雕
we can shoot two birds with one stone via a novel temporal reversible architecture for SNNs.

Without loss of generality  不失一般性
